{
  
    
        "post0": {
            "title": "[MachineLearning] Reinforcement learning",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Definition . Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20analysis/2022/03/22/ml-reinforcement-learning.html",
            "relUrl": "/reinforcement%20analysis/2022/03/22/ml-reinforcement-learning.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[MachineLearning] Reinforcement learning - Genetic Algorithm",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . Genetic Algorithm&#51060;&#46976;? . &quot;유전자 알고리즘&quot;을 뜻함 | 자연계의 진화 연산을 컴퓨팅의 최적화 분야에 적용한 것 | . Step . 1. &#51665;&#45800; &#52488;&#44592;&#54868; . 문제를 정의하고, 문제를 염색체 형태로 표현한 후 N개의 집단으로 이루어진 초기 염색체 집단을 생성 . 2. &#51201;&#54633;&#46020; &#44228;&#49328; . 염색체의 적합도를 측정하는 적합도 함수를 정의하고 계산 . 3. &#51333;&#47308; &#51312;&#44148; &#54869;&#51064; . 도출된 적합도가 종료 조건을 만족하면 알고리즘을 종료하고, 만족하지 못하면 다음 단계로 넘어감 . 4. &#49440;&#53469; . 현재의 해당 집단에서 부모 염색체를 한 쌍 선택함. 단, 적합도가 높은 염색체가 선택될 확률이 높아야 함 . 5. &#44368;&#52264; . 부모 염색체의 일부를 교차시켜서 자식 염색체를 한 쌍 생성함 . 6. &#46028;&#50672;&#48320;&#51060; . 만들어진 자식 염색체의 일부를 랜덤하게 선택하여 변경함 부모 염색체와 동일한 수의 자식 염색체가 생성되었으면 이것으로 모두 부모 염색체를 교체하고 다시 2번으로 돌아감 . . &#50672;&#49328;&#51088; . &#49440;&#53469; &#50672;&#49328;&#51088; (select) . 선택 연산자란 좋은 적합도 점수를 가진 염색체에게 우선 순위를 부여하고 좋은 유전자를 다음 세대에 전달할 수 있도록 하는 연산자 보통 룰렛 휠 선택 알고리즘(roulette wheel selection)이 많이 쓰이며 염색체 후보들이 차지하는 룰렛의 비율이 적합도 함수 값에 비례하도록 한 알고리즘임 . &#44368;&#52264; &#50672;&#49328;&#51088; (crossover) . 교차 연산자는 염색체 간의 교배를 나타내며, 선택 사용자를 사용하여 두 개의 염색체를 선택하고 교차 위치를 임의로 선택함. 그 후 교차 지점을 중심으로 유전자를 서로 교환하여 새로운 자식을 생성 . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328;&#51088; (mutate) . Local minimum을 피하고 개체군의 다양성을 유지하기 위한 연산자로써, 자손에 무작위 유전자를 삽입(or 변경)하는 연산자임. . Pseudo Code . Genetic Algorithm(population, FitnessFunc) { repeat new_population = [] for i = 1 to size(population) do father = select(population, FitnessFunc) mother = select(population, FitnessFunc) child = crossover(father, mother) if (난수 &lt; 변이 확률) then child = mutate(child) new_population.append(child) population = new_population until 종료 조건 만족할 때 까지 return 가장 적합한 개체 } . . 2.Example . 고전적인 예제인 0 ~ 31 범위 안에서 x^2의 값을 최대화 하는 x 값을 유전자 알고리즘을 이용해 찾아내보자 | . import random POPULATION_SIZE = 4 MUTATION_RATE = 0.1 SIZE = 5 . &#50684;&#49353;&#52404; &#53364;&#47000;&#49828; &#44396;&#54788; . class Chromosome: def __init__(self, g=[]): self.genes = g.copy() # 유전자는 리스트로 구현 self.fitness = 0 # 적합도 if self.genes.__len__()==0: # 염색체가 초기 상태이면 초기화 i = 0 while i&lt;SIZE: if random.random() &gt;= 0.5: self.genes.append(1) else: self.genes.append(0) i += 1 def cal_fitness(self): # 적합도를 계산 self.fitness = 0 value = 0 for i in range(SIZE): value += self.genes[i] * pow(2, SIZE-1-i) self.fitness = value return self.fitness def __str__(self): return self.genes.__str__() . &#50684;&#49353;&#52404;&#50752; &#51201;&#54633;&#46020; &#52636;&#47141; &#54632;&#49688; . def print_p(pop): i = 0 for x in pop: print(&quot;염색체 #&quot;, i, &quot;=&quot;, x, &quot;적합도=&quot;, x.cal_fitness()) i += 1 print(&quot;&quot;) . &#49440;&#53469; &#50672;&#49328; . def select(pop): max_value = sum([c.cal_fitness() for c in population]) pick = random.uniform(0, max_value) current = 0 for c in pop: current += c.cal_fitness() if current &gt; pick: return c . &#44368;&#52264; &#50672;&#49328; . def crossover(pop): father = select(pop) mother = select(pop) index = random.randint(1, SIZE - 2) child1 = father.genes[:index] + mother.genes[index:] child2 = mother.genes[:index] + father.genes[index:] return (child1, child2) . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328; . def mutate(c): for i in range(SIZE): if random.random() &lt; MUTATION_RATE: if random.random() &lt; 0.5: c.genes[i] = 1 else: c.genes[i] = 0 . &#47700;&#51064; &#54532;&#47196;&#44536;&#47016; . population = [] i=0 # 초기 염색체를 생성하여 객체 집단에 추가 while i&lt;POPULATION_SIZE: population.append(Chromosome()) i += 1 count=0 population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;세대 번호=&quot;, count) print_p(population) count=1 while population[0].cal_fitness() &lt; 31: new_pop = [] # 선택과 교차 연산 for _ in range(POPULATION_SIZE//2): c1, c2 = crossover(population); new_pop.append(Chromosome(c1)); new_pop.append(Chromosome(c2)); # 자식 세대가 부모 세대를 대체 # 깊은 복사를 수행 population = new_pop.copy(); # 돌연변이 연산 for c in population: mutate(c) # 출력을 위한 정렬 population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;세대 번호=&quot;, count) print_p(population) count += 1 if count &gt; 100 : break; . 세대 번호= 0 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [0, 1, 1, 0, 1] 적합도= 13 염색체 # 2 = [0, 0, 1, 0, 0] 적합도= 4 염색체 # 3 = [0, 0, 0, 1, 1] 적합도= 3 세대 번호= 1 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 0, 1] 적합도= 17 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 2 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 0, 1] 적합도= 17 염색체 # 3 = [0, 0, 0, 0, 1] 적합도= 1 세대 번호= 3 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 1] 적합도= 17 세대 번호= 4 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 5 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 6 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 7 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 8 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 9 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 10 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 11 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 12 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 13 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 14 염색체 # 0 = [1, 1, 0, 1, 1] 적합도= 27 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 15 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 3 = [0, 1, 0, 1, 0] 적합도= 10 세대 번호= 16 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 17 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 1, 1, 0] 적합도= 22 세대 번호= 18 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 19 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 20 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 21 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 0, 1, 0, 0] 적합도= 20 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 22 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 1, 0, 1] 적합도= 21 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 23 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 1, 0, 1] 적합도= 21 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 24 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 25 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 26 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 1] 적합도= 19 세대 번호= 27 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 28 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 2 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 29 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 0, 0] 적합도= 8 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 30 염색체 # 0 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 31 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 32 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [0, 1, 0, 1, 0] 적합도= 10 세대 번호= 33 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [1, 1, 0, 1, 0] 적합도= 26 세대 번호= 34 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 35 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 36 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [1, 1, 1, 1, 0] 적합도= 30 세대 번호= 37 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 1, 1, 0, 0] 적합도= 12 세대 번호= 38 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 39 염색체 # 0 = [1, 1, 1, 1, 1] 적합도= 31 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 1] 적합도= 15 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "relUrl": "/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[MachineLearning] Regression Analysis",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Definition . Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the &#39;outcome&#39; or &#39;response&#39; variable) and one or more independent variables (often called &#39;predictors&#39;, &#39;covariates&#39;, &#39;explanatory variables&#39; or &#39;features&#39;). . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[MachineLearning] Regression Analysis - Ridge/Lasso Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Ridge &amp; Lasso Regression . Ridge, Lasso Regression은 다중공선성 문제와 over-fitting 과적합 문제 방지를 위해 정규화 방식이 적용 . | Ridge Regression은 L2 정규화 . | Lasso Regression은 L1 정규화 | . &#45796;&#51473;&#44277;&#49440;&#49457;(Multicollinearity) . 두개 이상의 에측 변수 x들간의 강한 상관관계가 나타나, 독립변수들이 독립적이지 않는 문제가 발생하게 되는 현상을 말함. . | 이 경우 coefficient 추정치가 부정확해지고 standard error 값이 높아지게 된다 . | . Ridge Regression . Ridge 릿지 회귀는 선형 회귀모델의 Cost Function에 페널티를 적용한 것 . | 여기서 페널티는 Lambda * 계수 coefficient 제곱의 합이다 . | 이때 Lambda 값이 0에 가까워지면 Ridge는 본래 선형 회귀모델의 Cost Function에 가까워지게 된다 . | 반면에 Lambda의 값이 어느 정도 크다면, coefficient의 크기가 줄어서(0에 가까워져서) 모델의 복잡도가 줄어들고 multicollinearity 문제의 영향을 줄어든다. . | 왜냐면 서로 영향을 미치는 독립변수들의 weight가 줄어드는 것이기 때문이다. | . Lasso Regression . Ridge Regression은 Linear Regression 선형 회귀모델의 Cost Function 비용함수에 페널티를 적용한 것이다 . | Ridge와 수식은 비슷하지만 한 가지 차이점은, 페널티의 계산이 Lambda coefficient 제곱합이 아니라 Lambda coefficient 절대값의 합이라는 것이다. . | 이 계산은 L1 정규화 방식이고, zero coefficient 를 만드는 것이 가능해진다 . | 즉, 어떤 독립변수의 경우 아예 사라지게 될 수도 있게 되면서 feature selection, 상대적으로 더 중요한 독립변수를 선택할 수 있다 . | Lasso는 특정 독립변수의 coefficient 값을 0으로 만들 수 있고 Ridge는 0에 가까워지게 하지만 Lasso처럼 0으로 만들 수는 없다는 차이점이 있다 . Reference:https://nurilee.com/2020/01/26/data-science-model-summary-linear-ridge-lasso-elasticnet/&gt; https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/ . | . 2. Example . import matplotlib.pyplot as plt import numpy as np import pandas as pd import matplotlib matplotlib.rcParams.update({&#39;font.size&#39;: 12}) from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge . &#45936;&#51060;&#53552; &#51456;&#48708; . boston=load_boston() boston_df=pd.DataFrame(boston.data,columns=boston.feature_names) # add another column that contains the house prices which in scikit learn datasets are considered as target boston_df[&#39;Price&#39;]=boston.target newX=boston_df.drop(&#39;Price&#39;,axis=1) newX.head() newY=boston_df[&#39;Price&#39;] #print type(newY)# pandas core frame X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3) . Ridge Regression . lr = LinearRegression() lr.fit(X_train, y_train) rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization, # in this case linear and ridge regression resembles rr.fit(X_train, y_train) rr100 = Ridge(alpha=100) # comparison with alpha value rr100.fit(X_train, y_train) . Ridge(alpha=100) . train_score=lr.score(X_train, y_train) test_score=lr.score(X_test, y_test) Ridge_train_score = rr.score(X_train,y_train) Ridge_test_score = rr.score(X_test, y_test) Ridge_train_score100 = rr100.score(X_train,y_train) Ridge_test_score100 = rr100.score(X_test, y_test) . plt.plot(rr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Ridge; $ alpha = 0.01$&#39;,zorder=7) plt.plot(rr100.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Ridge; $ alpha = 100$&#39;) plt.plot(lr.coef_,alpha=0.4,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=7,color=&#39;green&#39;,label=&#39;Linear Regression&#39;) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.show() . Lasso Regression . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(X_train,y_train) train_score=lasso.score(X_train,y_train) test_score=lasso.score(X_test,y_test) coeff_used = np.sum(lasso.coef_!=0) print(&quot;training score:&quot;, train_score) print(&quot;test score: &quot;, test_score) print(&quot;number of features used: &quot;, coeff_used) . training score: 0.6832133784853487 test score: 0.6364462662362061 number of features used: 11 . lasso001 = Lasso(alpha=0.01, max_iter=10e5) lasso001.fit(X_train,y_train) train_score001=lasso001.score(X_train,y_train) test_score001=lasso001.score(X_test,y_test) coeff_used001 = np.sum(lasso001.coef_!=0) print(&quot;training score for alpha=0.01:&quot;, train_score001) print(&quot;test score for alpha =0.01: &quot;, test_score001) print(&quot;number of features used: for alpha =0.01:&quot;, coeff_used001) . training score for alpha=0.01: 0.7414845253242521 test score for alpha =0.01: 0.7096270988778384 number of features used: for alpha =0.01: 13 . lasso00001 = Lasso(alpha=0.0001, max_iter=10e5) lasso00001.fit(X_train,y_train) train_score00001=lasso00001.score(X_train,y_train) test_score00001=lasso00001.score(X_test,y_test) coeff_used00001 = np.sum(lasso00001.coef_!=0) print(&quot;training score for alpha=0.0001:&quot;, train_score00001) print(&quot;test score for alpha =0.0001: &quot;, test_score00001) print(&quot;number of features used: for alpha =0.0001:&quot;, coeff_used00001) . training score for alpha=0.0001: 0.7419034541315459 test score for alpha =0.0001: 0.7147428283500775 number of features used: for alpha =0.0001: 13 . lr = LinearRegression() lr.fit(X_train,y_train) lr_train_score=lr.score(X_train,y_train) lr_test_score=lr.score(X_test,y_test) print(&quot;LR training score:&quot;, lr_train_score) print(&quot;LR test score: &quot;, lr_test_score) . LR training score: 0.7419034960343789 LR test score: 0.7147895265576851 . plt.plot(lasso.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Lasso; $ alpha = 1$&#39;,zorder=7) # alpha here is for transparency plt.plot(lasso001.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Lasso; $ alpha = 0.01$&#39;) # alpha here is for transparency plt.plot(lasso00001.coef_,alpha=0.8,linestyle=&#39;none&#39;,marker=&#39;v&#39;,markersize=6,color=&#39;black&#39;,label=r&#39;Lasso; $ alpha = 0.00001$&#39;) # alpha here is for transparency plt.plot(lr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=5,color=&#39;green&#39;,label=&#39;Linear Regression&#39;,zorder=2) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.tight_layout() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[MachineLearning] Regression Analysis - Polynomial Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Polynomial Regression&#46976;? . 데이터의 분포가 선형이 아닌 곡선으로 나타나는 경우에 사용하는 회귀 . | 1차항이 아닌 2차, 3차항 등으로 확장하여 구성된 다항 회귀식 . | $ bar y = b_0 + {b_1}{x_i} + {b_2}{x_i^2} cdot cdot cdot b_p x_i^p$ . | . &#50508;&#44256;&#47532;&#51608; . 입력 데이터셋을 X라 할 때, . | 입력 데이터셋에 새로운 변수로 추가하고, 이 확장된 변수를 포함한 데이터셋을 선형모델로 훈련시키는 방법 . | 즉, X 를 통해 X^2, X^3 을 생성한 뒤, y = 1 + aX + bX^2 + cX^3 형식을 구성하고 . | a, b, c 를 선형 회귀 모델을 통해 학습하는 방법입니다. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np m = 100 X_train = 6 * np.random.rand(m, 1) - 3 y_train = 0.7 * X_train **2 + X_train + 2 + np.random.randn(m, 1) plt.scatter(X, y) plt.show() . &#54617;&#49845; . poly = PolynomialFeatures(degree=2, include_bias=True) X_train_poly = poly.fit_transform(X_train) . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X_train_poly, y_train) . LinearRegression() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#49373;&#49457; . m = 20 X_test = 6 * np.random.rand(m, 1) - 3 y_test = 0.7 * X_test **2 + X_test + 2 + np.random.randn(m, 1) . &#48320;&#54872; . X_test_poly = poly.transform(X_test) . &#50696;&#52769; . y_pred = lin_reg.predict(X_test_poly) . lin_reg.score(X_test_poly, y_test) . 0.7699902117540998 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[MachineLearning] Regression Analysis - Logistic Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Logistic Regression&#46976;? . Regression기법을 분류 문제로 확장한 것 . | 이항 형태의 범주형 데이터인 경우, 사용가능 ex) 양성/음성, 합격/불합격 . | 입력 값이 각 클래스에 속하는 확률값을 회귀분석으로 예측 . | . Odds . 임의의 사건 X가 발생하지 않을 확률 대비 일어날 확률의 비율 . | $Odds = {P(y=1|x) over {1-P(y=1|x)}} = exp(mx + b)$ . | . Logistic Function (Sigmoid Function) . 𝑥 ∈ (−∞,∞)를 −1,1 범위로 매핑하는 S자형 함수 | $f(x) = {1 over {1 + exp(-x)}}$ = $exp(x) over {1 + exp(x)}$ | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=30) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . classifier = LogisticRegression(random_state=0) classifier.fit(X_train, y_train) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . LogisticRegression(random_state=0) . &#50696;&#52769; . y_pred = classifier.predict(X_valid) y_pred . array([[6.16121922e-07, 2.15069658e-02, 9.78492418e-01], [2.24813541e-04, 1.44106469e-01, 8.55668717e-01], [9.86872166e-01, 1.31278254e-02, 8.45196341e-09], [7.42905537e-02, 9.15190175e-01, 1.05192710e-02], [6.17130899e-02, 9.34883255e-01, 3.40365509e-03], [9.85451789e-01, 1.45481989e-02, 1.22669080e-08], [9.83349822e-01, 1.66501582e-02, 2.00523225e-08], [9.74561843e-01, 2.54381352e-02, 2.13873436e-08], [1.05323138e-06, 2.92859163e-02, 9.70713030e-01], [4.60507545e-03, 8.28495273e-01, 1.66899652e-01], [2.90757993e-02, 9.57199786e-01, 1.37244147e-02], [5.07575743e-05, 5.38960827e-02, 9.46053160e-01], [2.38861710e-02, 9.59412646e-01, 1.67011834e-02], [3.74400885e-06, 1.20910117e-02, 9.87905244e-01], [9.76403956e-01, 2.35960160e-02, 2.74985641e-08], [9.76299518e-01, 2.37004423e-02, 3.95390327e-08], [9.52336822e-01, 4.76629417e-02, 2.36333757e-07], [5.73850109e-04, 4.81120782e-01, 5.18305368e-01], [9.81557938e-01, 1.84420425e-02, 1.96011616e-08], [1.01003329e-02, 7.51043736e-01, 2.38855931e-01], [1.67803358e-05, 1.42899971e-01, 8.57083248e-01], [9.73880934e-01, 2.61190258e-02, 3.98647797e-08], [8.45573758e-03, 9.35096103e-01, 5.64481595e-02], [6.91704414e-03, 8.60176111e-01, 1.32906844e-01], [3.84852525e-04, 4.50294871e-01, 5.49320276e-01], [1.11451716e-03, 8.01459697e-01, 1.97425786e-01], [6.74370685e-05, 4.30887311e-02, 9.56843832e-01], [6.18950666e-05, 1.89381057e-01, 8.10557048e-01], [9.78683149e-01, 2.13168316e-02, 1.91861458e-08], [4.82177753e-06, 5.19363140e-02, 9.48058864e-01]]) . classifier.score(X_valid, y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[MachineLearning] Regression Analysis - Linear Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Linear Regression&#46976;? . 가지고 있는 데이터를 가장 잘 설명할 수 있는 선(y = ax + b)을 찾는 방법을 선형 회귀(Linear Regression)이라 부른다. . | 예를 들어 키와 몸무게 데이터를 가장 잘 설명할 수 있는 선을 하나 잘 찾는다면, 새로운 사람의 키 정보만을 가지고 몸무게를 예측하는 것이 가능하다 . | . &#45936;&#51060;&#53552;&#47484; &#44032;&#51109; &#51096; &#49444;&#47749;&#54616;&#45716; &#49440;&#51060;&#46976; &#47924;&#50631;&#51064;&#44032;? . 우리가 예측한 선(ax + b)과 실제 가지고 있는 데이터 사이에는 차이가 존재한다. 우리는 이 차이를 오차라고 부른다. . | 우리가 찾고자 하는 데이터를 잘 설명하는 선이라는 것은 이 오차를 최소화화는 선을 찾는 것이다. . | 오차는 Mean Square Error 또는 Mean Absolute Error 등 다양하게 오차를 정의하여 사용할 수 있다. . | . Mean Square Error . 일반적으로 오차는 양수, 음수 모두로 표현이 되므로, 우리는 이 모든 오차를 양수, 음수 관계없이 동일하게 반영하도록 이 오차를 제곱하여 사용한다. . | 우리는 이 오차를 제곱하여 평균을 해준 것을 Mean Square Error(평균 제곱 오차)라고 부른다. . | $MSE = {1 over N} sum_{i=1}^N ({y - bar{y}})^2$ . | . &#50612;&#46523;&#44172; &#50724;&#52264;&#47484; &#51460;&#51068; &#44163;&#51064;&#44032;? . Gradient Descent(경사하강법)을 이용한다. | https://m.blog.naver.com/jevida/221855713144 | . &#54620;&#44228; . x와 y가 선형 관계가 아닌 경우, 에측이 잘 되지 않음 | 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . import numpy as np import matplotlib.pyplot as plt m = 100 X = 6 * np.random.rand(m,1) - 3 y = 3.331 * X + 23 + 4 * np.random.randn(m,1) # 약간의 노이즈 포함 X_train, X_test, y_train, y_test = X[20:], X[:20], y[20:], y[:20] plt.plot(X,y,&quot;b.&quot;) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#54617;&#49845; . import matplotlib.pyplot as plt import numpy as np from sklearn import linear_model from sklearn.metrics import mean_squared_error, r2_score # Create linear regression object regr = linear_model.LinearRegression() # Train the model using the training sets regr.fit(X_train, y_train) # Make predictions using the testing set y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, y_pred)) . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 . &#54617;&#49845;&#45936;&#51060;&#53552;&#50640; &#45824;&#54644;&#49436; &#50696;&#52769;&#54620; &#49440; &#54869;&#51064; . 오차가 최소한인 선이 구해진 것을 볼 수 있다. . diabetes_y_pred = regr.predict(X_train) # Plot outputs plt.scatter(X_train, y_train, color=&quot;r&quot;) plt.plot(X_train, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . diabetes_y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, diabetes_y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, diabetes_y_pred)) # Plot outputs plt.scatter(X_test, y_test, color=&quot;r&quot;) plt.plot(X_test, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[MachineLearning] Dimensionality Reduction",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Definition . Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[MachineLearning] Dimensionality Reduction - t-SNE",
            "content": "Content creators: 조동현 . Content reviewers: . 1.Overview . t-SNE&#46976;? . &quot;t - distributed stochastic neighbor embedding&quot; 의 약자로 &quot;t 분포 확률적 임베딩&quot;을 뜻함 | . t-SNE &#51032;&#51032; . 매니폴드 학습의 하나로 복잡한 데이터의 시각화가 목적, 높은 차원의 데이터를 2차원 또는 3차원으로 축소시켜 시각화를 함 | 높은 차원 공간에서 비슷한 데이터 구조는 낮은 차언 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 대응함 | . 2. Example . from sklearn.datasets import load_iris from sklearn.manifold import TSNE import matplotlib.pyplot as plt import pandas as pd import numpy as np . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() df = pd.DataFrame(data = iris.data, columns = iris.feature_names) df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . 2&#52264;&#50896; t-SNE &#51076;&#48288;&#46377; . tsne_np = TSNE(n_components=2).fit_transform(df) tsne_df = pd.DataFrame(tsne_np, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) tsne_df[&#39;target&#39;] = iris.target tsne_df . cp1 cp2 target . 0 -24.555080 | 7.379160 | 0 | . 1 -21.977486 | 6.074713 | 0 | . 2 -22.032751 | 7.286000 | 0 | . 3 -21.595722 | 6.841762 | 0 | . 4 -24.597988 | 7.200622 | 0 | . ... ... | ... | ... | . 145 17.367140 | -5.027658 | 2 | . 146 14.227304 | -5.973306 | 2 | . 147 16.485725 | -5.267701 | 2 | . 148 18.021391 | -4.299052 | 2 | . 149 12.856414 | -6.667024 | 2 | . 150 rows × 3 columns . plt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=iris.target) plt.xlabel(&quot;cp1&quot;) plt.ylabel(&quot;cp2&quot;) . Text(0, 0.5, &#39;cp2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[MachineLearning] Dimensionality Reduction - Singular Value Decomposition(SVD)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . SVD&#46976;? . Singular Value Decompositiond 의 약자로 &quot;특이값 분해&quot;를 뜻함 . | 수식 . | . $A=U&#931;V^T$ . $A$ : m x n 행렬$U$ : m x m 직교행렬$Σ$ : m x n 대각행렬$V$ : n x n 직교행렬SVD &#51032;&#51032; . 직교하는 벡터 집합에서, 선형 변환 후에 크기는 변하지만 여전히 직교할 수 있게 되는 직교 집합을 구하는 것, 그리고 선형 변환 후 결과를 구하는 것 | . SVD &#54596;&#50836;&#49457; . EVD(고유값 분해)의 경우 조건을 만족하는 정방행렬(n x n)에 대해서만 적용 가능함 | 하지만 행렬이 정방 행렬이든 아니든 관계 없이 모든 m x n 행렬에 대해 적용 가능하기 때문에 유용함 | . Truncated SVD . Σ의 대각 원소 중 상위 몇개만 추출하고 여기 대응하는 U와 V의 원소도 함께 줄이는 방법 | . . 2. Example . SVD &#50696;&#49884; . import numpy as np from numpy.linalg import svd . 4 by 4 &#47004;&#45924; &#54665;&#47148; &#49373;&#49457; . np.random.seed(10) a= np.random.randn(4, 4) a . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . SVD &#48516;&#54644; . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&quot;U matrix: n&quot;, np.round(U, 3)) print(&quot;Sigma Value: n&quot;, np.round(Sigma, 3)) print(&quot;V transpose: n&quot;, np.round(Vt, 3)) . (4, 4) (4,) (4, 4) U matrix: [[-0.909 0.292 -0.29 0.07 ] [-0.055 -0.568 -0.208 0.794] [ 0.241 -0.094 -0.921 -0.292] [ 0.336 0.763 -0.157 0.529]] Sigma Value: [2.284 1.659 1.255 0.146] V transpose: [[-0.687 -0.134 0.688 0.193] [-0.422 0.856 -0.282 0.098] [-0.293 -0.046 -0.033 -0.954] [ 0.514 0.498 0.668 -0.205]] . &#44160;&#51613; . Sigma_mt = np.diag(Sigma) verify_mt = np.dot(np.dot(U, Sigma_mt), Vt) verify_mt . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . . Truncated SVD &#50696;&#49884; . from scipy.sparse.linalg import svds from scipy.linalg import svd . &#53945;&#51060;&#44050; 3&#44060;&#47196; Truncated SVD &#49688;&#54665; . U_tr, Sigma_tr, Vt_tr = svds(a, k=3) print(U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print(&quot;U_tr matrix: n&quot;, np.round(U_tr, 3)) print(&quot;Sigma_tr Value: n&quot;, np.round(Sigma_tr, 3)) print(&quot;V_tr transpose: n&quot;, np.round(Vt_tr, 3)) . (4, 3) (3,) (3, 4) U_tr matrix: [[ 0.29 0.292 0.909] [ 0.208 -0.568 0.055] [ 0.921 -0.094 -0.241] [ 0.157 0.763 -0.336]] Sigma_tr Value: [1.255 1.659 2.284] V_tr transpose: [[ 0.293 0.046 0.033 0.954] [-0.422 0.856 -0.282 0.098] [ 0.687 0.134 -0.688 -0.193]] . &#48373;&#50896; . Sigma_tr_mt = np.diag(Sigma_tr) restore = np.dot(np.dot(U_tr, Sigma_tr_mt), Vt_tr) restore . array([[ 1.32634718, 0.71020432, -1.55220761, -0.00629071], [ 0.56177478, -0.77777473, 0.18812528, 0.13234356], [ 0.02616807, -0.15341116, 0.46144994, 1.19429753], [-1.00471538, 0.98987057, 0.1771143 , 0.46097789]]) . . SVD &#54876;&#50857; . from sklearn.decomposition import TruncatedSVD from sklearn.datasets import load_iris import matplotlib.pyplot as plt import pandas as pd . &#45936;&#51060;&#53552; &#47196;&#46300; . iris = load_iris() iris_data = iris.data . Truncated SVD &#51652;&#54665; . tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_data) iris_tsvd = tsvd.transform(iris_data) result = pd.DataFrame(data = iris_tsvd, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) . &#49884;&#44033;&#54868; . plt.scatter(x=result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&quot;TruncsatedSVD Component 1&quot;) plt.ylabel(&quot;TruncsatedSVD Component 1&quot;) . Text(0, 0.5, &#39;TruncsatedSVD Component 1&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[MachineLearning] Dimensionality Reduction - Principal Component Analysis(PCA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . PCA&#46976;? . Principal Component Analysis 의 약자로 &quot;주성분 분석&quot;을 뜻함 | . PCA &#51032;&#51032; . n차원 데이터를 정사영 시켜 k 차원으로 낮출 때, (n &gt; k) 어떤 벡터에 데이터를 정사영 시켜야 원래 데이터의 구조을 제일 잘 유지시킬 수 있는지 알아내는 것 | . PCA &#54596;&#50836;&#49457; . 실제 데이터들은 매우 많은 feature를 가지고 있음(= 차원이 높음). 따라서 머신러닝을 적용하여 문제를 해결하는데 있어서 아래와 어려움이 있음 전체 데이터의 양이 많아 학습 속도가 느려짐 | 의미 없는 faeture들에 의해 과적합되거나 원활한 학습이 되지 않음 | | 그래서 차원축소를 통해 전체적인 데이터 양을 줄일 필요가 있음 | . 2. Example . import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.datasets import load_iris . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() x = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) x.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . &#45936;&#51060;&#53552; scale . x = StandardScaler().fit_transform(x) pd.DataFrame(data=x, columns=iris[&#39;feature_names&#39;]).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 -0.900681 | 1.019004 | -1.340227 | -1.315444 | . 1 -1.143017 | -0.131979 | -1.340227 | -1.315444 | . 2 -1.385353 | 0.328414 | -1.397064 | -1.315444 | . 3 -1.506521 | 0.098217 | -1.283389 | -1.315444 | . 4 -1.021849 | 1.249201 | -1.340227 | -1.315444 | . PCA &#49892;&#54665; . pca = PCA(n_components=2) result = pca.fit_transform(x) result_df = pd.DataFrame(data=result, columns = [&#39;pc1&#39;, &#39;pc2&#39;]) result_df[&#39;species&#39;] = iris[&#39;target&#39;] result_df[&#39;species&#39;] = result_df[&#39;species&#39;].map({0: &#39;setosa&#39;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) result_df.head() . pc1 pc2 species . 0 -2.264703 | 0.480027 | setosa | . 1 -2.080961 | -0.674134 | setosa | . 2 -2.364229 | -0.341908 | setosa | . 3 -2.299384 | -0.597395 | setosa | . 4 -2.389842 | 0.646835 | setosa | . PCA &#44208;&#44284;&#44032; &#50896;&#48376; &#45936;&#51060;&#53552; &#48516;&#49328;&#51032; 96%&#47484; &#49444;&#47749;&#54620;&#45796;&#44256; &#48380; &#49688; &#51080;&#51020; . sum(pca.explained_variance_ratio_) . 0.9581320720000164 . &#49884;&#44033;&#54868; . figure = plt.figure(figsize = (8, 8)) axis = figure.add_subplot(1, 1, 1) axis.set_xlabel(&#39;Principal Component 1&#39;) axis.set_ylabel(&#39;Principal Component 2&#39;) axis.set_title(&#39;PCA result&#39;) targets = iris[&#39;target_names&#39;] colors = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;] for target, color in zip(targets, colors): indices = result_df[&#39;species&#39;] == target axis.scatter(result_df.loc[indices, &#39;pc1&#39;], result_df.loc[indices, &#39;pc2&#39;], c = color, s = 50) axis.legend(targets) axis.grid() .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[MachineLearning] Dimensionality Reduction - Linear Discriminant Analysis(LDA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . LDA&#46976;? . Linear Discriminant Analysis 의 약자로 &quot;선형 판별법&quot;을 뜻함 | . LDA &#51032;&#51032; . PCA와 매우 유사하게 dataset을 저차원 공간에 투영해 차원을 축소하는 기법이지만, 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소함 | 그렇기 때문에 Classification에 적용하기 전 데이터 전처리로 사용하기 적합함 | . LDA &#44396;&#54788; &#50896;&#47532; . 클래스간 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 차원 축소를 진행함 | . 2. Example . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; scale &#51652;&#54665; . iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) . LDA &#51652;&#54665; . lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) result = pd.DataFrame(data = iris_lda) . &#49884;&#44033;&#54868; . plt.scatter(x = result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&#39;LDA component 1&#39;) plt.ylabel(&#39;LDA component 2&#39;) . Text(0, 0.5, &#39;LDA component 2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[MachineLearning] Dimensionality Reduction - Latent Semantic Analysis(LCA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . LCA&#46976;? . Latent Semantic Analysis 의 약자로 &quot;잠재 의미 분석&quot;을 뜻함 | . LCA &#51032;&#51032; . 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘 | DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 떄문에 단어의 의미를 고려하지 못한다는 단점이 존재 이를 위한 대안으로 등장함 | . . 2. Example . import pandas as pd from sklearn.datasets import fetch_20newsgroups import nltk from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) documents = dataset.data print(&#39;샘플의 수 :&#39;,len(documents)) . 샘플의 수 : 11314 . documents[1] . &#34; n n n n n n nYeah, do you expect people to read the FAQ, etc. and actually accept hard natheism? No, you need a little leap of faith, Jimmy. Your logic runs out nof steam! n n n n n n n nJim, n nSorry I can&#39;t pity you, Jim. And I&#39;m sorry that you have these feelings of ndenial about the faith you need to get by. Oh well, just pretend that it will nall end happily ever after anyway. Maybe if you start a new newsgroup, nalt.atheist.hard, you won&#39;t be bummin&#39; so much? n n n n n n nBye-Bye, Big Jim. Don&#39;t forget your Flintstone&#39;s Chewables! :) n-- nBake Timmons, III&#34; . print(dataset.target_names) . [&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;] . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . news_df = pd.DataFrame({&#39;document&#39;:documents}) news_df[&#39;clean_doc&#39;] = news_df[&#39;document&#39;].str.replace(&quot;[^a-zA-Z]&quot;, &quot; &quot;, regex=True) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: &#39; &#39;.join([w for w in x.split() if len(w)&gt;3])) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: x.lower()) . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons&#39; . &#48520;&#50857;&#50612; &#51228;&#44144; . stop_words = stopwords.words(&#39;english&#39;) tokenized_doc = news_df[&#39;clean_doc&#39;].apply(lambda x: x.split()) # 토큰화 tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) . print(tokenized_doc[1]) . [&#39;yeah&#39;, &#39;expect&#39;, &#39;people&#39;, &#39;read&#39;, &#39;actually&#39;, &#39;accept&#39;, &#39;hard&#39;, &#39;atheism&#39;, &#39;need&#39;, &#39;little&#39;, &#39;leap&#39;, &#39;faith&#39;, &#39;jimmy&#39;, &#39;logic&#39;, &#39;runs&#39;, &#39;steam&#39;, &#39;sorry&#39;, &#39;pity&#39;, &#39;sorry&#39;, &#39;feelings&#39;, &#39;denial&#39;, &#39;faith&#39;, &#39;need&#39;, &#39;well&#39;, &#39;pretend&#39;, &#39;happily&#39;, &#39;ever&#39;, &#39;anyway&#39;, &#39;maybe&#39;, &#39;start&#39;, &#39;newsgroup&#39;, &#39;atheist&#39;, &#39;hard&#39;, &#39;bummin&#39;, &#39;much&#39;, &#39;forget&#39;, &#39;flintstone&#39;, &#39;chewables&#39;, &#39;bake&#39;, &#39;timmons&#39;] . TF-IDF &#54665;&#47148; &#47564;&#46308;&#44592; . detokenized_doc = [] for i in range(len(news_df)): t = &#39; &#39;.join(tokenized_doc[i]) detokenized_doc.append(t) news_df[&#39;clean_doc&#39;] = detokenized_doc . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons&#39; . vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_features= 1000, max_df = 0.5, smooth_idf=True) X = vectorizer.fit_transform(news_df[&#39;clean_doc&#39;]) print(&#39;TF-IDF 행렬의 크기 :&#39;,X.shape) . TF-IDF 행렬의 크기 : (11314, 1000) . &#53664;&#54589; &#47784;&#45944;&#47553; . svd_model = TruncatedSVD(n_components=20, algorithm=&#39;randomized&#39;, n_iter=100, random_state=122) svd_model.fit(X) len(svd_model.components_) . 20 . terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨. def get_topics(components, feature_names, n=5): for idx, topic in enumerate(components): print(&quot;Topic %d:&quot; % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]]) get_topics(svd_model.components_,terms) . Topic 1: [(&#39;like&#39;, 0.21386), (&#39;know&#39;, 0.20046), (&#39;people&#39;, 0.19293), (&#39;think&#39;, 0.17805), (&#39;good&#39;, 0.15128)] Topic 2: [(&#39;thanks&#39;, 0.32888), (&#39;windows&#39;, 0.29088), (&#39;card&#39;, 0.18069), (&#39;drive&#39;, 0.17455), (&#39;mail&#39;, 0.15111)] Topic 3: [(&#39;game&#39;, 0.37064), (&#39;team&#39;, 0.32443), (&#39;year&#39;, 0.28154), (&#39;games&#39;, 0.2537), (&#39;season&#39;, 0.18419)] Topic 4: [(&#39;drive&#39;, 0.53324), (&#39;scsi&#39;, 0.20165), (&#39;hard&#39;, 0.15628), (&#39;disk&#39;, 0.15578), (&#39;card&#39;, 0.13994)] Topic 5: [(&#39;windows&#39;, 0.40399), (&#39;file&#39;, 0.25436), (&#39;window&#39;, 0.18044), (&#39;files&#39;, 0.16078), (&#39;program&#39;, 0.13894)] Topic 6: [(&#39;chip&#39;, 0.16114), (&#39;government&#39;, 0.16009), (&#39;mail&#39;, 0.15625), (&#39;space&#39;, 0.1507), (&#39;information&#39;, 0.13562)] Topic 7: [(&#39;like&#39;, 0.67086), (&#39;bike&#39;, 0.14236), (&#39;chip&#39;, 0.11169), (&#39;know&#39;, 0.11139), (&#39;sounds&#39;, 0.10371)] Topic 8: [(&#39;card&#39;, 0.46633), (&#39;video&#39;, 0.22137), (&#39;sale&#39;, 0.21266), (&#39;monitor&#39;, 0.15463), (&#39;offer&#39;, 0.14643)] Topic 9: [(&#39;know&#39;, 0.46047), (&#39;card&#39;, 0.33605), (&#39;chip&#39;, 0.17558), (&#39;government&#39;, 0.1522), (&#39;video&#39;, 0.14356)] Topic 10: [(&#39;good&#39;, 0.42756), (&#39;know&#39;, 0.23039), (&#39;time&#39;, 0.1882), (&#39;bike&#39;, 0.11406), (&#39;jesus&#39;, 0.09027)] Topic 11: [(&#39;think&#39;, 0.78469), (&#39;chip&#39;, 0.10899), (&#39;good&#39;, 0.10635), (&#39;thanks&#39;, 0.09123), (&#39;clipper&#39;, 0.07946)] Topic 12: [(&#39;thanks&#39;, 0.36824), (&#39;good&#39;, 0.22729), (&#39;right&#39;, 0.21559), (&#39;bike&#39;, 0.21037), (&#39;problem&#39;, 0.20894)] Topic 13: [(&#39;good&#39;, 0.36212), (&#39;people&#39;, 0.33985), (&#39;windows&#39;, 0.28385), (&#39;know&#39;, 0.26232), (&#39;file&#39;, 0.18422)] Topic 14: [(&#39;space&#39;, 0.39946), (&#39;think&#39;, 0.23258), (&#39;know&#39;, 0.18074), (&#39;nasa&#39;, 0.15174), (&#39;problem&#39;, 0.12957)] Topic 15: [(&#39;space&#39;, 0.31613), (&#39;good&#39;, 0.3094), (&#39;card&#39;, 0.22603), (&#39;people&#39;, 0.17476), (&#39;time&#39;, 0.14496)] Topic 16: [(&#39;people&#39;, 0.48156), (&#39;problem&#39;, 0.19961), (&#39;window&#39;, 0.15281), (&#39;time&#39;, 0.14664), (&#39;game&#39;, 0.12871)] Topic 17: [(&#39;time&#39;, 0.34465), (&#39;bike&#39;, 0.27303), (&#39;right&#39;, 0.25557), (&#39;windows&#39;, 0.1997), (&#39;file&#39;, 0.19118)] Topic 18: [(&#39;time&#39;, 0.5973), (&#39;problem&#39;, 0.15504), (&#39;file&#39;, 0.14956), (&#39;think&#39;, 0.12847), (&#39;israel&#39;, 0.10903)] Topic 19: [(&#39;file&#39;, 0.44163), (&#39;need&#39;, 0.26633), (&#39;card&#39;, 0.18388), (&#39;files&#39;, 0.17453), (&#39;right&#39;, 0.15448)] Topic 20: [(&#39;problem&#39;, 0.33006), (&#39;file&#39;, 0.27651), (&#39;thanks&#39;, 0.23578), (&#39;used&#39;, 0.19206), (&#39;space&#39;, 0.13185)] .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[MachineLearning] Cluster Analysis",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Definition . Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters) . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "[MachineLearning] Cluster Analysis - Mean Shift",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Aims to discover blobs in a smooth density of samples | Works by updating candidates for centroids to be the mean of the points within a given region | Candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids | Parameters : Non-parametric(bandwidth) | . Algorithm . Determine initial centroids | While moved distance is smaller than threshold For each centroids, move points towards a region of the maximum increase in the density of points. | . | Eliminate near-duplicates to form the final set of centroids | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import MeanShift mean_shift = MeanShift(bandwidth=1.4).fit(X) pred = mean_shift.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "[MachineLearning] Cluster Analysis - K-Means",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Clusters data by trying to separate samples in n groups of equal variance | Highly dependent on the initialization of the centroids | Object : Minimize inertia(=within-cluster sum-of-squares) Inertia makes the assumption that clusters are convex and isotropic | Responds poorly to elongated clusters, or manifolds with irregular shapes | . | Parameters : number of clusters | . Algorithm . Choose n initial centroids | While the centroids do not move significantly assigns each sample to its nearest centroid | creates new centroids by taking the mean value of all of the samples assigned to each previous centroid | | Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=4, random_state=SEED).fit(X) pred = kmeans.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], label=&quot;centers&quot;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "[MachineLearning] Cluster Analysis - Fuzzy C Means",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . A form of clustering in which each data point can belong to more than one cluster | In fuzzy clustering, data points can potentially belong to multiple clusters. | The fuzzy c-menas algorithm is very similar to the k-means algorithm. | Parameters : number of clusters | . Algorithm . Choose a number of clusters. | Assign coefficients randomly to each data point for being in the clusters. | While the coefficients&#39; change between iterations is no more than the given threshold: Compute the centroid for each cluster. | For each data point, compute its coefficients of being in the clusters. | | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . import skfuzzy as fuzz # scikit-fuzzy c_means = fuzz.cmeans(X.T, c=4, m=2, error=1e-3, maxiter=300) centers, coefs = c_means[0], c_means[1] pred = np.argmax(coefs, axis=0) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(centers[:, 0], centers[:, 1], label=&#39;centers&#39;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "[MachineLearning] Cluster Analysis - Agglomerative Clustering",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Performs a hierarchical clustering using a bottom up approach | Each observation starts in its own cluster, and clusters are successively merged together Ward minimizes the sum of squared differences within all clusters. | Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters. | Average linkage minimizes the average of the distances between all observations of pairs of clusters. | Single linkage minimizes the distance between the closest observations of pairs of clusters. | . | . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import AgglomerativeClustering clustering = AgglomerativeClustering() pred = clustering.fit_predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "[MachineLearning] Cluster Analysis - DBSCAN",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Views clusters as areas of high density separated by areas of low density | clusters found by DBSCAN can be any shape | central component to the DBSCAN is the concept of core samples core samples are in areas of high density | . | Parameters : min_samples, eps . min_samples : controls how tolerant the algorithm is towards noise | eps : controls the local neighborhood of the points | . | Core sample : a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample . | Cluster : a set of core samples that can be built by recursively taking a core sample = finding all of its neighbors that are core samples | = finding all of their neighbors that are core samples | . | non-core samepls : neighbors of a core sample in the cluster but are not themselves core samples | Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm. | . Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) n_noise_ = list(labels).count(-1) unique_labels = set(labels) colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = [0, 0, 0, 1] class_member_mask = labels == k xy = X[class_member_mask &amp; core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=14, ) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=6, ) .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "[MachineLearning] Classification",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Definition . Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. Classification is the grouping of related facts into classes. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "[MachineLearning] Classification - Support Vector Machine",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . SVM(Support Vector Machine)&#46976;? . 결정 경계(Decision Boundary), 즉 분류를 위한 기준 선을 정의하는 모델 . | 새로운 데이터가 주어졌을 때, 어느쪽 결정경계에 포함하는지에 따라 분류 . | . &#51339;&#51008; &#44208;&#51221;&#44221;&#44228;&#46976;? . 데이터 군으로부터 멀리 떨어져있는 결정 경게 . | 서포트 벡터: 결정 경계와 가까이 있는 데이터들 . | Margin: 결정 경계와 서포트 벡터 사이의 거리 . | 최적의 결정경게는 Margin을 최대화 한다 . | n개의 속성을 가진 데이터에는 최소 n+1개의 서포트 벡터가 존재 . | . SVM &#51109;&#51216; . SVM에서 결정 경계는 서포트 벡터에 의해 정의되므로, 데이터 중에서 서포트 벡터만을 잘 선별하면 필요없는 데이터들을 무시할 수 있다. . | 이로인해 매우 빠르다는 장점이 있다. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.svm import SVC classifier = SVC(kernel = &#39;linear&#39;) classifier.fit(X_train, y_train) . SVC(kernel=&#39;linear&#39;) . &#50696;&#52769; . classifier.predict(X_valid) . array([2, 1, 2, 1, 0, 1, 2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 0, 0, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 2, 0, 0]) . classifier.score(X_valid, y_valid) . 0.9555555555555556 . classifier.score(X_valid, y_valid) . 0.9555555555555556 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "relUrl": "/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "[MachineLearning] Classification - Naive Bayes Classification",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Naive Bayes&#46976;? . 베이즈 정리에 기반한 통계적 분류 기법 . | 공통적으로 모든 특성들이 서로 독립임을 가정 . | 복잡한 반복 매개변수 추정 없어 매우 큰 데이터셋에 유용함 . | 정확성이 높음 . | 스팸 메일 필터, 텍스트 분류, 감정 분석, 추천 시스템 등에 활용 . | . &#50508;&#44256;&#47532;&#51608; . 베이즈 정리는 P(c), P(x) 및 P(x|c)로부터 후방 확률 P(c|x)를 계산하는 방법을 제공합니다. . | Naigive Bayes 분류자는 주어진 클래스(c)에 대한 예측 변수(x) 값의 효과가 다른 예측 변수의 값과 독립적이라고 가정합니다. . | . P(c|x)는 주어진 예측 변수(속성)에서 클래스(목표값)의 후방 확률 . | P(c)는 클래스의 사전 확률 . | P(x|c)는 주어진 클래스의 확률인 우도 . | P(x)는 예측 변수의 사전 확률 . | $$ Bayes Rule: {P(c|x) = }{{P(x|c)P(c)} over P(x)} $$ . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.naive_bayes import GaussianNB #Create a Gaussian Classifier classifier = GaussianNB() # Train the model using the training sets classifier.fit(X_train, y_train) #Predict Output predicted= classifier.predict(X_valid) # 0:Overcast, 2:Mild print(&quot;Predicted Value:&quot;, predicted) # 1: Yes . Predicted Value: [2 1 1 1 0 1 2 0 2 2 2 0 1 2 0 1 0 0 1 2 2 0 1 1 2 1 1 1 1 1 0 0 1 2 2 0 2 1 0 2 0 0 2 0 0] . &#50696;&#52769; . classifier.score(X_valid, y_valid) . 0.9333333333333333 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "[MachineLearning] Classification - K-Nearest Neighbor",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . K-Nearest Neighbor&#46976;? . 비모수 밀도추정 방법이다. (확률분포 모델을 미리 가정하지 않고 데이터 집합을 이용) . | 모든 학습 데이터를 저장하여 예측에 사용한다. . | 새로운 데이터가 주어졌을 때, 이웃한 K개의 학습 데이터를 찾는다. . | 찾아진 이웃 데이터들이 많이 속한 클래스에 할당한다. . | . &#54617;&#49845; &#50508;&#44256;&#47532;&#51608; . 주어진 데이터 x와 모든 학습 데이터 {x1,x2, ..., xN} 과의 거리를 계 산한다. . | 거리가 가장 가까운 것부터 순서대로 K개의 데이터를 찾아 후보 집 합 N(x)={x1,x2,..., xK}을 만든다. . | 후보 집합의 각 원소가 어떤 클래스에 속하는지 그 라벨값 c(x1), c(x2),...,c(xK)을 찾는다. . | 찾아진 라벨 값 중 가장 많은 빈도수를 차지하는 클래스를 찾아 x를 그 클래스에 할당한다. . | &#44256;&#47140;&#49324;&#54637; . K&#51032; &#44050; . K가 작다면, 몇개의 이웃한 데이터에만 의존하여 클래스가 결정됨 | 이는 노이즈에 민감, 오버피팅에 발생 | 데이터 특정에 따라 적절한 K를 선택해야함 | . &#44144;&#47532;&#54632;&#49688;&#47484; &#50612;&#46523;&#44172; &#49444;&#51221;&#54624; &#44163;&#51064;&#44032;? . 다양한 거리 함수가 존재함 | 거리함수에 따라, 예측 결과가 달라질 수 있음 | ex) 1차 노름, 2차 노름, 내적, cosine distance 등 .. | . &#51109;&#51216; . 복잡한 데이터에 대해서도 비교적 잘 작동함 | 학습에 시간이 걸리지 않음 | . &#45800;&#51216; . 새 데이터가 주어질 때, 모든 학습 데이터와의 거리를 구해주어야함 (계산 비용이 많이 듦) | 학습 데이터를 모두 저장하고 있어야함 (메모리 문제) | . 2. Example . from sklearn.datasets import load_iris import pandas as pd from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . iris = load_iris() . df = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) df[&#39;target&#39;] = iris[&#39;target&#39;] df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, :4], df[&#39;target&#39;], stratify=df[&#39;target&#39;], test_size=0.2, random_state=30) x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) knn.fit(x_train, y_train) . KNeighborsClassifier(n_jobs=-1) . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . prediction = knn.predict(x_valid) (prediction == y_valid).mean() knn.score(x_valid, y_valid) . 0.9333333333333333 . &#52572;&#51201;&#51032; K&#44050; &#52286;&#44592; . test_scores = [] train_scores = [] for i in range(1,10): knn = KNeighborsClassifier(i) knn.fit(x_train,y_train) train_scores.append(knn.score(x_train,y_train)) test_scores.append(knn.score(x_valid,y_valid)) . max_train_score = max(train_scores) train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score] print(&#39;Max train score {} % and k = {}&#39;.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind)))) . Max train score 100.0 % and k = [1] . max_test_score = max(test_scores) test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score] print(&#39;Max test score {} % and k = {}&#39;.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind)))) . Max test score 96.66666666666667 % and k = [1, 3, 4, 7, 8, 9] . plt.figure(figsize=(12,5)) p = sns.lineplot(range(1,10),train_scores,marker=&#39;*&#39;,label=&#39;Train Score&#39;) p = sns.lineplot(range(1,10),test_scores,marker=&#39;o&#39;,label=&#39;Test Score&#39;) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . knn = KNeighborsClassifier(7) knn.fit(x_train,y_train) knn.score(x_valid,y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "relUrl": "/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "date": " • Mar 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://knu-ai-researcher.github.io/reports/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://knu-ai-researcher.github.io/reports/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}