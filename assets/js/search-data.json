{
  
    
        "post0": {
            "title": "[MachineLearning] Reinforcement learning",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Definition . Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20analysis/2022/03/22/ml-reinforcement-learning.html",
            "relUrl": "/reinforcement%20analysis/2022/03/22/ml-reinforcement-learning.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[MachineLearning] Reinforcement learning - Genetic Algorithm",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Overview . Genetic Algorithm&#51060;&#46976;? . &quot;ìœ ì „ì ì•Œê³ ë¦¬ì¦˜&quot;ì„ ëœ»í•¨ | ìì—°ê³„ì˜ ì§„í™” ì—°ì‚°ì„ ì»´í“¨íŒ…ì˜ ìµœì í™” ë¶„ì•¼ì— ì ìš©í•œ ê²ƒ | . Step . 1. &#51665;&#45800; &#52488;&#44592;&#54868; . ë¬¸ì œë¥¼ ì •ì˜í•˜ê³ , ë¬¸ì œë¥¼ ì—¼ìƒ‰ì²´ í˜•íƒœë¡œ í‘œí˜„í•œ í›„ Nê°œì˜ ì§‘ë‹¨ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì´ˆê¸° ì—¼ìƒ‰ì²´ ì§‘ë‹¨ì„ ìƒì„± . 2. &#51201;&#54633;&#46020; &#44228;&#49328; . ì—¼ìƒ‰ì²´ì˜ ì í•©ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì í•©ë„ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ê³„ì‚° . 3. &#51333;&#47308; &#51312;&#44148; &#54869;&#51064; . ë„ì¶œëœ ì í•©ë„ê°€ ì¢…ë£Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì„ ì¢…ë£Œí•˜ê³ , ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê° . 4. &#49440;&#53469; . í˜„ì¬ì˜ í•´ë‹¹ ì§‘ë‹¨ì—ì„œ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë¥¼ í•œ ìŒ ì„ íƒí•¨. ë‹¨, ì í•©ë„ê°€ ë†’ì€ ì—¼ìƒ‰ì²´ê°€ ì„ íƒë  í™•ë¥ ì´ ë†’ì•„ì•¼ í•¨ . 5. &#44368;&#52264; . ë¶€ëª¨ ì—¼ìƒ‰ì²´ì˜ ì¼ë¶€ë¥¼ êµì°¨ì‹œì¼œì„œ ìì‹ ì—¼ìƒ‰ì²´ë¥¼ í•œ ìŒ ìƒì„±í•¨ . 6. &#46028;&#50672;&#48320;&#51060; . ë§Œë“¤ì–´ì§„ ìì‹ ì—¼ìƒ‰ì²´ì˜ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ë³€ê²½í•¨ ë¶€ëª¨ ì—¼ìƒ‰ì²´ì™€ ë™ì¼í•œ ìˆ˜ì˜ ìì‹ ì—¼ìƒ‰ì²´ê°€ ìƒì„±ë˜ì—ˆìœ¼ë©´ ì´ê²ƒìœ¼ë¡œ ëª¨ë‘ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë¥¼ êµì²´í•˜ê³  ë‹¤ì‹œ 2ë²ˆìœ¼ë¡œ ëŒì•„ê° . . &#50672;&#49328;&#51088; . &#49440;&#53469; &#50672;&#49328;&#51088; (select) . ì„ íƒ ì—°ì‚°ìë€ ì¢‹ì€ ì í•©ë„ ì ìˆ˜ë¥¼ ê°€ì§„ ì—¼ìƒ‰ì²´ì—ê²Œ ìš°ì„  ìˆœìœ„ë¥¼ ë¶€ì—¬í•˜ê³  ì¢‹ì€ ìœ ì „ìë¥¼ ë‹¤ìŒ ì„¸ëŒ€ì— ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì—°ì‚°ì ë³´í†µ ë£°ë › íœ  ì„ íƒ ì•Œê³ ë¦¬ì¦˜(roulette wheel selection)ì´ ë§ì´ ì“°ì´ë©° ì—¼ìƒ‰ì²´ í›„ë³´ë“¤ì´ ì°¨ì§€í•˜ëŠ” ë£°ë ›ì˜ ë¹„ìœ¨ì´ ì í•©ë„ í•¨ìˆ˜ ê°’ì— ë¹„ë¡€í•˜ë„ë¡ í•œ ì•Œê³ ë¦¬ì¦˜ì„ . &#44368;&#52264; &#50672;&#49328;&#51088; (crossover) . êµì°¨ ì—°ì‚°ìëŠ” ì—¼ìƒ‰ì²´ ê°„ì˜ êµë°°ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì„ íƒ ì‚¬ìš©ìë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ê³  êµì°¨ ìœ„ì¹˜ë¥¼ ì„ì˜ë¡œ ì„ íƒí•¨. ê·¸ í›„ êµì°¨ ì§€ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìœ ì „ìë¥¼ ì„œë¡œ êµí™˜í•˜ì—¬ ìƒˆë¡œìš´ ìì‹ì„ ìƒì„± . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328;&#51088; (mutate) . Local minimumì„ í”¼í•˜ê³  ê°œì²´êµ°ì˜ ë‹¤ì–‘ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì—°ì‚°ìë¡œì¨, ìì†ì— ë¬´ì‘ìœ„ ìœ ì „ìë¥¼ ì‚½ì…(or ë³€ê²½)í•˜ëŠ” ì—°ì‚°ìì„. . Pseudo Code . Genetic Algorithm(population, FitnessFunc) { repeat new_population = [] for i = 1 to size(population) do father = select(population, FitnessFunc) mother = select(population, FitnessFunc) child = crossover(father, mother) if (ë‚œìˆ˜ &lt; ë³€ì´ í™•ë¥ ) then child = mutate(child) new_population.append(child) population = new_population until ì¢…ë£Œ ì¡°ê±´ ë§Œì¡±í•  ë•Œ ê¹Œì§€ return ê°€ì¥ ì í•©í•œ ê°œì²´ } . . 2.Example . ê³ ì „ì ì¸ ì˜ˆì œì¸ 0 ~ 31 ë²”ìœ„ ì•ˆì—ì„œ x^2ì˜ ê°’ì„ ìµœëŒ€í™” í•˜ëŠ” x ê°’ì„ ìœ ì „ì ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•´ ì°¾ì•„ë‚´ë³´ì | . import random POPULATION_SIZE = 4 MUTATION_RATE = 0.1 SIZE = 5 . &#50684;&#49353;&#52404; &#53364;&#47000;&#49828; &#44396;&#54788; . class Chromosome: def __init__(self, g=[]): self.genes = g.copy() # ìœ ì „ìëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬í˜„ self.fitness = 0 # ì í•©ë„ if self.genes.__len__()==0: # ì—¼ìƒ‰ì²´ê°€ ì´ˆê¸° ìƒíƒœì´ë©´ ì´ˆê¸°í™” i = 0 while i&lt;SIZE: if random.random() &gt;= 0.5: self.genes.append(1) else: self.genes.append(0) i += 1 def cal_fitness(self): # ì í•©ë„ë¥¼ ê³„ì‚° self.fitness = 0 value = 0 for i in range(SIZE): value += self.genes[i] * pow(2, SIZE-1-i) self.fitness = value return self.fitness def __str__(self): return self.genes.__str__() . &#50684;&#49353;&#52404;&#50752; &#51201;&#54633;&#46020; &#52636;&#47141; &#54632;&#49688; . def print_p(pop): i = 0 for x in pop: print(&quot;ì—¼ìƒ‰ì²´ #&quot;, i, &quot;=&quot;, x, &quot;ì í•©ë„=&quot;, x.cal_fitness()) i += 1 print(&quot;&quot;) . &#49440;&#53469; &#50672;&#49328; . def select(pop): max_value = sum([c.cal_fitness() for c in population]) pick = random.uniform(0, max_value) current = 0 for c in pop: current += c.cal_fitness() if current &gt; pick: return c . &#44368;&#52264; &#50672;&#49328; . def crossover(pop): father = select(pop) mother = select(pop) index = random.randint(1, SIZE - 2) child1 = father.genes[:index] + mother.genes[index:] child2 = mother.genes[:index] + father.genes[index:] return (child1, child2) . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328; . def mutate(c): for i in range(SIZE): if random.random() &lt; MUTATION_RATE: if random.random() &lt; 0.5: c.genes[i] = 1 else: c.genes[i] = 0 . &#47700;&#51064; &#54532;&#47196;&#44536;&#47016; . population = [] i=0 # ì´ˆê¸° ì—¼ìƒ‰ì²´ë¥¼ ìƒì„±í•˜ì—¬ ê°ì²´ ì§‘ë‹¨ì— ì¶”ê°€ while i&lt;POPULATION_SIZE: population.append(Chromosome()) i += 1 count=0 population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;ì„¸ëŒ€ ë²ˆí˜¸=&quot;, count) print_p(population) count=1 while population[0].cal_fitness() &lt; 31: new_pop = [] # ì„ íƒê³¼ êµì°¨ ì—°ì‚° for _ in range(POPULATION_SIZE//2): c1, c2 = crossover(population); new_pop.append(Chromosome(c1)); new_pop.append(Chromosome(c2)); # ìì‹ ì„¸ëŒ€ê°€ ë¶€ëª¨ ì„¸ëŒ€ë¥¼ ëŒ€ì²´ # ê¹Šì€ ë³µì‚¬ë¥¼ ìˆ˜í–‰ population = new_pop.copy(); # ëŒì—°ë³€ì´ ì—°ì‚° for c in population: mutate(c) # ì¶œë ¥ì„ ìœ„í•œ ì •ë ¬ population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;ì„¸ëŒ€ ë²ˆí˜¸=&quot;, count) print_p(population) count += 1 if count &gt; 100 : break; . ì„¸ëŒ€ ë²ˆí˜¸= 0 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [0, 1, 1, 0, 1] ì í•©ë„= 13 ì—¼ìƒ‰ì²´ # 2 = [0, 0, 1, 0, 0] ì í•©ë„= 4 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 1] ì í•©ë„= 3 ì„¸ëŒ€ ë²ˆí˜¸= 1 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 0, 1] ì í•©ë„= 29 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 0, 1] ì í•©ë„= 17 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 0] ì í•©ë„= 2 ì„¸ëŒ€ ë²ˆí˜¸= 2 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 0, 1] ì í•©ë„= 29 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 0, 1] ì í•©ë„= 17 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 0, 1] ì í•©ë„= 1 ì„¸ëŒ€ ë²ˆí˜¸= 3 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 0, 1] ì í•©ë„= 29 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 0, 1] ì í•©ë„= 29 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 0, 1] ì í•©ë„= 17 ì„¸ëŒ€ ë²ˆí˜¸= 4 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 0, 1] ì í•©ë„= 29 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 5 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 6 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 7 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 8 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 9 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 0] ì í•©ë„= 2 ì„¸ëŒ€ ë²ˆí˜¸= 10 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 11 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 12 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 13 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 0] ì í•©ë„= 2 ì„¸ëŒ€ ë²ˆí˜¸= 14 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 1] ì í•©ë„= 27 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 15 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì„¸ëŒ€ ë²ˆí˜¸= 16 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 0] ì í•©ë„= 2 ì„¸ëŒ€ ë²ˆí˜¸= 17 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì„¸ëŒ€ ë²ˆí˜¸= 18 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 19 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 20 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 21 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 1, 0, 0] ì í•©ë„= 20 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 22 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 1, 0, 1] ì í•©ë„= 21 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [0, 0, 0, 1, 0] ì í•©ë„= 2 ì„¸ëŒ€ ë²ˆí˜¸= 23 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 1, 0, 1] ì í•©ë„= 21 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì„¸ëŒ€ ë²ˆí˜¸= 24 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì„¸ëŒ€ ë²ˆí˜¸= 25 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì„¸ëŒ€ ë²ˆí˜¸= 26 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 1, 1, 0] ì í•©ë„= 22 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 1, 1] ì í•©ë„= 19 ì„¸ëŒ€ ë²ˆí˜¸= 27 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 3 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì„¸ëŒ€ ë²ˆí˜¸= 28 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì—¼ìƒ‰ì²´ # 2 = [1, 0, 0, 0, 0] ì í•©ë„= 16 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 0, 0] ì í•©ë„= 8 ì„¸ëŒ€ ë²ˆí˜¸= 29 ì—¼ìƒ‰ì²´ # 0 = [1, 0, 0, 1, 0] ì í•©ë„= 18 ì—¼ìƒ‰ì²´ # 1 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 0, 0, 0] ì í•©ë„= 8 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 0, 0] ì í•©ë„= 8 ì„¸ëŒ€ ë²ˆí˜¸= 30 ì—¼ìƒ‰ì²´ # 0 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 1 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 0, 0] ì í•©ë„= 8 ì„¸ëŒ€ ë²ˆí˜¸= 31 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 1 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 0, 0] ì í•©ë„= 8 ì„¸ëŒ€ ë²ˆí˜¸= 32 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 0, 1, 0] ì í•©ë„= 10 ì„¸ëŒ€ ë²ˆí˜¸= 33 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 3 = [1, 1, 0, 1, 0] ì í•©ë„= 26 ì„¸ëŒ€ ë²ˆí˜¸= 34 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 0, 0, 0] ì í•©ë„= 24 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì„¸ëŒ€ ë²ˆí˜¸= 35 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì„¸ëŒ€ ë²ˆí˜¸= 36 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 3 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì„¸ëŒ€ ë²ˆí˜¸= 37 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 1, 0, 0] ì í•©ë„= 12 ì„¸ëŒ€ ë²ˆí˜¸= 38 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 1, 1, 0] ì í•©ë„= 14 ì„¸ëŒ€ ë²ˆí˜¸= 39 ì—¼ìƒ‰ì²´ # 0 = [1, 1, 1, 1, 1] ì í•©ë„= 31 ì—¼ìƒ‰ì²´ # 1 = [1, 1, 1, 1, 0] ì í•©ë„= 30 ì—¼ìƒ‰ì²´ # 2 = [0, 1, 1, 1, 1] ì í•©ë„= 15 ì—¼ìƒ‰ì²´ # 3 = [0, 1, 1, 1, 0] ì í•©ë„= 14 .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "relUrl": "/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[MachineLearning] Regression Analysis",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Definition . Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the &#39;outcome&#39; or &#39;response&#39; variable) and one or more independent variables (often called &#39;predictors&#39;, &#39;covariates&#39;, &#39;explanatory variables&#39; or &#39;features&#39;). . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[MachineLearning] Regression Analysis - Ridge/Lasso Regression",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . Ridge &amp; Lasso Regression . Ridge, Lasso Regressionì€ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œì™€ over-fitting ê³¼ì í•© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì •ê·œí™” ë°©ì‹ì´ ì ìš© . | Ridge Regressionì€ L2 ì •ê·œí™” . | Lasso Regressionì€ L1 ì •ê·œí™” | . &#45796;&#51473;&#44277;&#49440;&#49457;(Multicollinearity) . ë‘ê°œ ì´ìƒì˜ ì—ì¸¡ ë³€ìˆ˜ xë“¤ê°„ì˜ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ë‚˜íƒ€ë‚˜, ë…ë¦½ë³€ìˆ˜ë“¤ì´ ë…ë¦½ì ì´ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ë˜ëŠ” í˜„ìƒì„ ë§í•¨. . | ì´ ê²½ìš° coefficient ì¶”ì •ì¹˜ê°€ ë¶€ì •í™•í•´ì§€ê³  standard error ê°’ì´ ë†’ì•„ì§€ê²Œ ëœë‹¤ . | . Ridge Regression . Ridge ë¦¿ì§€ íšŒê·€ëŠ” ì„ í˜• íšŒê·€ëª¨ë¸ì˜ Cost Functionì— í˜ë„í‹°ë¥¼ ì ìš©í•œ ê²ƒ . | ì—¬ê¸°ì„œ í˜ë„í‹°ëŠ” Lambda * ê³„ìˆ˜ coefficient ì œê³±ì˜ í•©ì´ë‹¤ . | ì´ë•Œ Lambda ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ë©´ RidgeëŠ” ë³¸ë˜ ì„ í˜• íšŒê·€ëª¨ë¸ì˜ Cost Functionì— ê°€ê¹Œì›Œì§€ê²Œ ëœë‹¤ . | ë°˜ë©´ì— Lambdaì˜ ê°’ì´ ì–´ëŠ ì •ë„ í¬ë‹¤ë©´, coefficientì˜ í¬ê¸°ê°€ ì¤„ì–´ì„œ(0ì— ê°€ê¹Œì›Œì ¸ì„œ) ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì¤„ì–´ë“¤ê³  multicollinearity ë¬¸ì œì˜ ì˜í–¥ì„ ì¤„ì–´ë“ ë‹¤. . | ì™œëƒë©´ ì„œë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë…ë¦½ë³€ìˆ˜ë“¤ì˜ weightê°€ ì¤„ì–´ë“œëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. | . Lasso Regression . Ridge Regressionì€ Linear Regression ì„ í˜• íšŒê·€ëª¨ë¸ì˜ Cost Function ë¹„ìš©í•¨ìˆ˜ì— í˜ë„í‹°ë¥¼ ì ìš©í•œ ê²ƒì´ë‹¤ . | Ridgeì™€ ìˆ˜ì‹ì€ ë¹„ìŠ·í•˜ì§€ë§Œ í•œ ê°€ì§€ ì°¨ì´ì ì€, í˜ë„í‹°ì˜ ê³„ì‚°ì´ Lambda coefficient ì œê³±í•©ì´ ì•„ë‹ˆë¼ Lambda coefficient ì ˆëŒ€ê°’ì˜ í•©ì´ë¼ëŠ” ê²ƒì´ë‹¤. . | ì´ ê³„ì‚°ì€ L1 ì •ê·œí™” ë°©ì‹ì´ê³ , zero coefficient ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ë‹¤ . | ì¦‰, ì–´ë–¤ ë…ë¦½ë³€ìˆ˜ì˜ ê²½ìš° ì•„ì˜ˆ ì‚¬ë¼ì§€ê²Œ ë  ìˆ˜ë„ ìˆê²Œ ë˜ë©´ì„œ feature selection, ìƒëŒ€ì ìœ¼ë¡œ ë” ì¤‘ìš”í•œ ë…ë¦½ë³€ìˆ˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤ . | LassoëŠ” íŠ¹ì • ë…ë¦½ë³€ìˆ˜ì˜ coefficient ê°’ì„ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆê³  RidgeëŠ” 0ì— ê°€ê¹Œì›Œì§€ê²Œ í•˜ì§€ë§Œ Lassoì²˜ëŸ¼ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ëŠ” ì—†ë‹¤ëŠ” ì°¨ì´ì ì´ ìˆë‹¤ . Reference:https://nurilee.com/2020/01/26/data-science-model-summary-linear-ridge-lasso-elasticnet/&gt; https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/ . | . 2. Example . import matplotlib.pyplot as plt import numpy as np import pandas as pd import matplotlib matplotlib.rcParams.update({&#39;font.size&#39;: 12}) from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge . &#45936;&#51060;&#53552; &#51456;&#48708; . boston=load_boston() boston_df=pd.DataFrame(boston.data,columns=boston.feature_names) # add another column that contains the house prices which in scikit learn datasets are considered as target boston_df[&#39;Price&#39;]=boston.target newX=boston_df.drop(&#39;Price&#39;,axis=1) newX.head() newY=boston_df[&#39;Price&#39;] #print type(newY)# pandas core frame X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3) . Ridge Regression . lr = LinearRegression() lr.fit(X_train, y_train) rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization, # in this case linear and ridge regression resembles rr.fit(X_train, y_train) rr100 = Ridge(alpha=100) # comparison with alpha value rr100.fit(X_train, y_train) . Ridge(alpha=100) . train_score=lr.score(X_train, y_train) test_score=lr.score(X_test, y_test) Ridge_train_score = rr.score(X_train,y_train) Ridge_test_score = rr.score(X_test, y_test) Ridge_train_score100 = rr100.score(X_train,y_train) Ridge_test_score100 = rr100.score(X_test, y_test) . plt.plot(rr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Ridge; $ alpha = 0.01$&#39;,zorder=7) plt.plot(rr100.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Ridge; $ alpha = 100$&#39;) plt.plot(lr.coef_,alpha=0.4,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=7,color=&#39;green&#39;,label=&#39;Linear Regression&#39;) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.show() . Lasso Regression . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(X_train,y_train) train_score=lasso.score(X_train,y_train) test_score=lasso.score(X_test,y_test) coeff_used = np.sum(lasso.coef_!=0) print(&quot;training score:&quot;, train_score) print(&quot;test score: &quot;, test_score) print(&quot;number of features used: &quot;, coeff_used) . training score: 0.6832133784853487 test score: 0.6364462662362061 number of features used: 11 . lasso001 = Lasso(alpha=0.01, max_iter=10e5) lasso001.fit(X_train,y_train) train_score001=lasso001.score(X_train,y_train) test_score001=lasso001.score(X_test,y_test) coeff_used001 = np.sum(lasso001.coef_!=0) print(&quot;training score for alpha=0.01:&quot;, train_score001) print(&quot;test score for alpha =0.01: &quot;, test_score001) print(&quot;number of features used: for alpha =0.01:&quot;, coeff_used001) . training score for alpha=0.01: 0.7414845253242521 test score for alpha =0.01: 0.7096270988778384 number of features used: for alpha =0.01: 13 . lasso00001 = Lasso(alpha=0.0001, max_iter=10e5) lasso00001.fit(X_train,y_train) train_score00001=lasso00001.score(X_train,y_train) test_score00001=lasso00001.score(X_test,y_test) coeff_used00001 = np.sum(lasso00001.coef_!=0) print(&quot;training score for alpha=0.0001:&quot;, train_score00001) print(&quot;test score for alpha =0.0001: &quot;, test_score00001) print(&quot;number of features used: for alpha =0.0001:&quot;, coeff_used00001) . training score for alpha=0.0001: 0.7419034541315459 test score for alpha =0.0001: 0.7147428283500775 number of features used: for alpha =0.0001: 13 . lr = LinearRegression() lr.fit(X_train,y_train) lr_train_score=lr.score(X_train,y_train) lr_test_score=lr.score(X_test,y_test) print(&quot;LR training score:&quot;, lr_train_score) print(&quot;LR test score: &quot;, lr_test_score) . LR training score: 0.7419034960343789 LR test score: 0.7147895265576851 . plt.plot(lasso.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Lasso; $ alpha = 1$&#39;,zorder=7) # alpha here is for transparency plt.plot(lasso001.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Lasso; $ alpha = 0.01$&#39;) # alpha here is for transparency plt.plot(lasso00001.coef_,alpha=0.8,linestyle=&#39;none&#39;,marker=&#39;v&#39;,markersize=6,color=&#39;black&#39;,label=r&#39;Lasso; $ alpha = 0.00001$&#39;) # alpha here is for transparency plt.plot(lr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=5,color=&#39;green&#39;,label=&#39;Linear Regression&#39;,zorder=2) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.tight_layout() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[MachineLearning] Regression Analysis - Polynomial Regression",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . Polynomial Regression&#46976;? . ë°ì´í„°ì˜ ë¶„í¬ê°€ ì„ í˜•ì´ ì•„ë‹Œ ê³¡ì„ ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” íšŒê·€ . | 1ì°¨í•­ì´ ì•„ë‹Œ 2ì°¨, 3ì°¨í•­ ë“±ìœ¼ë¡œ í™•ì¥í•˜ì—¬ êµ¬ì„±ëœ ë‹¤í•­ íšŒê·€ì‹ . | $ bar y = b_0 + {b_1}{x_i} + {b_2}{x_i^2} cdot cdot cdot b_p x_i^p$ . | . &#50508;&#44256;&#47532;&#51608; . ì…ë ¥ ë°ì´í„°ì…‹ì„ Xë¼ í•  ë•Œ, . | ì…ë ¥ ë°ì´í„°ì…‹ì— ìƒˆë¡œìš´ ë³€ìˆ˜ë¡œ ì¶”ê°€í•˜ê³ , ì´ í™•ì¥ëœ ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ë°ì´í„°ì…‹ì„ ì„ í˜•ëª¨ë¸ë¡œ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²• . | ì¦‰, X ë¥¼ í†µí•´ X^2, X^3 ì„ ìƒì„±í•œ ë’¤, y = 1 + aX + bX^2 + cX^3 í˜•ì‹ì„ êµ¬ì„±í•˜ê³  . | a, b, c ë¥¼ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np m = 100 X_train = 6 * np.random.rand(m, 1) - 3 y_train = 0.7 * X_train **2 + X_train + 2 + np.random.randn(m, 1) plt.scatter(X, y) plt.show() . &#54617;&#49845; . poly = PolynomialFeatures(degree=2, include_bias=True) X_train_poly = poly.fit_transform(X_train) . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X_train_poly, y_train) . LinearRegression() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#49373;&#49457; . m = 20 X_test = 6 * np.random.rand(m, 1) - 3 y_test = 0.7 * X_test **2 + X_test + 2 + np.random.randn(m, 1) . &#48320;&#54872; . X_test_poly = poly.transform(X_test) . &#50696;&#52769; . y_pred = lin_reg.predict(X_test_poly) . lin_reg.score(X_test_poly, y_test) . 0.7699902117540998 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[MachineLearning] Regression Analysis - Logistic Regression",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . Logistic Regression&#46976;? . Regressionê¸°ë²•ì„ ë¶„ë¥˜ ë¬¸ì œë¡œ í™•ì¥í•œ ê²ƒ . | ì´í•­ í˜•íƒœì˜ ë²”ì£¼í˜• ë°ì´í„°ì¸ ê²½ìš°, ì‚¬ìš©ê°€ëŠ¥ ex) ì–‘ì„±/ìŒì„±, í•©ê²©/ë¶ˆí•©ê²© . | ì…ë ¥ ê°’ì´ ê° í´ë˜ìŠ¤ì— ì†í•˜ëŠ” í™•ë¥ ê°’ì„ íšŒê·€ë¶„ì„ìœ¼ë¡œ ì˜ˆì¸¡ . | . Odds . ì„ì˜ì˜ ì‚¬ê±´ Xê°€ ë°œìƒí•˜ì§€ ì•Šì„ í™•ë¥  ëŒ€ë¹„ ì¼ì–´ë‚  í™•ë¥ ì˜ ë¹„ìœ¨ . | $Odds = {P(y=1|x) over {1-P(y=1|x)}} = exp(mx + b)$ . | . Logistic Function (Sigmoid Function) . ğ‘¥ âˆˆ (âˆ’âˆ,âˆ)ë¥¼ âˆ’1,1 ë²”ìœ„ë¡œ ë§¤í•‘í•˜ëŠ” Sìí˜• í•¨ìˆ˜ | $f(x) = {1 over {1 + exp(-x)}}$ = $exp(x) over {1 + exp(x)}$ | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=30) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . classifier = LogisticRegression(random_state=0) classifier.fit(X_train, y_train) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . LogisticRegression(random_state=0) . &#50696;&#52769; . y_pred = classifier.predict(X_valid) y_pred . array([[6.16121922e-07, 2.15069658e-02, 9.78492418e-01], [2.24813541e-04, 1.44106469e-01, 8.55668717e-01], [9.86872166e-01, 1.31278254e-02, 8.45196341e-09], [7.42905537e-02, 9.15190175e-01, 1.05192710e-02], [6.17130899e-02, 9.34883255e-01, 3.40365509e-03], [9.85451789e-01, 1.45481989e-02, 1.22669080e-08], [9.83349822e-01, 1.66501582e-02, 2.00523225e-08], [9.74561843e-01, 2.54381352e-02, 2.13873436e-08], [1.05323138e-06, 2.92859163e-02, 9.70713030e-01], [4.60507545e-03, 8.28495273e-01, 1.66899652e-01], [2.90757993e-02, 9.57199786e-01, 1.37244147e-02], [5.07575743e-05, 5.38960827e-02, 9.46053160e-01], [2.38861710e-02, 9.59412646e-01, 1.67011834e-02], [3.74400885e-06, 1.20910117e-02, 9.87905244e-01], [9.76403956e-01, 2.35960160e-02, 2.74985641e-08], [9.76299518e-01, 2.37004423e-02, 3.95390327e-08], [9.52336822e-01, 4.76629417e-02, 2.36333757e-07], [5.73850109e-04, 4.81120782e-01, 5.18305368e-01], [9.81557938e-01, 1.84420425e-02, 1.96011616e-08], [1.01003329e-02, 7.51043736e-01, 2.38855931e-01], [1.67803358e-05, 1.42899971e-01, 8.57083248e-01], [9.73880934e-01, 2.61190258e-02, 3.98647797e-08], [8.45573758e-03, 9.35096103e-01, 5.64481595e-02], [6.91704414e-03, 8.60176111e-01, 1.32906844e-01], [3.84852525e-04, 4.50294871e-01, 5.49320276e-01], [1.11451716e-03, 8.01459697e-01, 1.97425786e-01], [6.74370685e-05, 4.30887311e-02, 9.56843832e-01], [6.18950666e-05, 1.89381057e-01, 8.10557048e-01], [9.78683149e-01, 2.13168316e-02, 1.91861458e-08], [4.82177753e-06, 5.19363140e-02, 9.48058864e-01]]) . classifier.score(X_valid, y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[MachineLearning] Regression Analysis - Linear Regression",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . Linear Regression&#46976;? . ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„ (y = ax + b)ì„ ì°¾ëŠ” ë°©ë²•ì„ ì„ í˜• íšŒê·€(Linear Regression)ì´ë¼ ë¶€ë¥¸ë‹¤. . | ì˜ˆë¥¼ ë“¤ì–´ í‚¤ì™€ ëª¸ë¬´ê²Œ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„ ì„ í•˜ë‚˜ ì˜ ì°¾ëŠ”ë‹¤ë©´, ìƒˆë¡œìš´ ì‚¬ëŒì˜ í‚¤ ì •ë³´ë§Œì„ ê°€ì§€ê³  ëª¸ë¬´ê²Œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ . | . &#45936;&#51060;&#53552;&#47484; &#44032;&#51109; &#51096; &#49444;&#47749;&#54616;&#45716; &#49440;&#51060;&#46976; &#47924;&#50631;&#51064;&#44032;? . ìš°ë¦¬ê°€ ì˜ˆì¸¡í•œ ì„ (ax + b)ê³¼ ì‹¤ì œ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„° ì‚¬ì´ì—ëŠ” ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤. ìš°ë¦¬ëŠ” ì´ ì°¨ì´ë¥¼ ì˜¤ì°¨ë¼ê³  ë¶€ë¥¸ë‹¤. . | ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ì„ ì´ë¼ëŠ” ê²ƒì€ ì´ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í™”ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. . | ì˜¤ì°¨ëŠ” Mean Square Error ë˜ëŠ” Mean Absolute Error ë“± ë‹¤ì–‘í•˜ê²Œ ì˜¤ì°¨ë¥¼ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. . | . Mean Square Error . ì¼ë°˜ì ìœ¼ë¡œ ì˜¤ì°¨ëŠ” ì–‘ìˆ˜, ìŒìˆ˜ ëª¨ë‘ë¡œ í‘œí˜„ì´ ë˜ë¯€ë¡œ, ìš°ë¦¬ëŠ” ì´ ëª¨ë“  ì˜¤ì°¨ë¥¼ ì–‘ìˆ˜, ìŒìˆ˜ ê´€ê³„ì—†ì´ ë™ì¼í•˜ê²Œ ë°˜ì˜í•˜ë„ë¡ ì´ ì˜¤ì°¨ë¥¼ ì œê³±í•˜ì—¬ ì‚¬ìš©í•œë‹¤. . | ìš°ë¦¬ëŠ” ì´ ì˜¤ì°¨ë¥¼ ì œê³±í•˜ì—¬ í‰ê· ì„ í•´ì¤€ ê²ƒì„ Mean Square Error(í‰ê·  ì œê³± ì˜¤ì°¨)ë¼ê³  ë¶€ë¥¸ë‹¤. . | $MSE = {1 over N} sum_{i=1}^N ({y - bar{y}})^2$ . | . &#50612;&#46523;&#44172; &#50724;&#52264;&#47484; &#51460;&#51068; &#44163;&#51064;&#44032;? . Gradient Descent(ê²½ì‚¬í•˜ê°•ë²•)ì„ ì´ìš©í•œë‹¤. | https://m.blog.naver.com/jevida/221855713144 | . &#54620;&#44228; . xì™€ yê°€ ì„ í˜• ê´€ê³„ê°€ ì•„ë‹Œ ê²½ìš°, ì—ì¸¡ì´ ì˜ ë˜ì§€ ì•ŠìŒ | 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . import numpy as np import matplotlib.pyplot as plt m = 100 X = 6 * np.random.rand(m,1) - 3 y = 3.331 * X + 23 + 4 * np.random.randn(m,1) # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ í¬í•¨ X_train, X_test, y_train, y_test = X[20:], X[:20], y[20:], y[:20] plt.plot(X,y,&quot;b.&quot;) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#54617;&#49845; . import matplotlib.pyplot as plt import numpy as np from sklearn import linear_model from sklearn.metrics import mean_squared_error, r2_score # Create linear regression object regr = linear_model.LinearRegression() # Train the model using the training sets regr.fit(X_train, y_train) # Make predictions using the testing set y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, y_pred)) . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 . &#54617;&#49845;&#45936;&#51060;&#53552;&#50640; &#45824;&#54644;&#49436; &#50696;&#52769;&#54620; &#49440; &#54869;&#51064; . ì˜¤ì°¨ê°€ ìµœì†Œí•œì¸ ì„ ì´ êµ¬í•´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. . diabetes_y_pred = regr.predict(X_train) # Plot outputs plt.scatter(X_train, y_train, color=&quot;r&quot;) plt.plot(X_train, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . diabetes_y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, diabetes_y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, diabetes_y_pred)) # Plot outputs plt.scatter(X_test, y_test, color=&quot;r&quot;) plt.plot(X_test, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[MachineLearning] Dimensionality Reduction",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Definition . Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[MachineLearning] Dimensionality Reduction - t-SNE",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1.Overview . t-SNE&#46976;? . &quot;t - distributed stochastic neighbor embedding&quot; ì˜ ì•½ìë¡œ &quot;t ë¶„í¬ í™•ë¥ ì  ì„ë² ë”©&quot;ì„ ëœ»í•¨ | . t-SNE &#51032;&#51032; . ë§¤ë‹ˆí´ë“œ í•™ìŠµì˜ í•˜ë‚˜ë¡œ ë³µì¡í•œ ë°ì´í„°ì˜ ì‹œê°í™”ê°€ ëª©ì , ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œì‹œì¼œ ì‹œê°í™”ë¥¼ í•¨ | ë†’ì€ ì°¨ì› ê³µê°„ì—ì„œ ë¹„ìŠ·í•œ ë°ì´í„° êµ¬ì¡°ëŠ” ë‚®ì€ ì°¨ì–¸ ê³µê°„ì—ì„œ ê°€ê¹ê²Œ ëŒ€ì‘í•˜ë©°, ë¹„ìŠ·í•˜ì§€ ì•Šì€ ë°ì´í„° êµ¬ì¡°ëŠ” ë©€ë¦¬ ë–¨ì–´ì ¸ ëŒ€ì‘í•¨ | . 2. Example . from sklearn.datasets import load_iris from sklearn.manifold import TSNE import matplotlib.pyplot as plt import pandas as pd import numpy as np . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() df = pd.DataFrame(data = iris.data, columns = iris.feature_names) df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . 2&#52264;&#50896; t-SNE &#51076;&#48288;&#46377; . tsne_np = TSNE(n_components=2).fit_transform(df) tsne_df = pd.DataFrame(tsne_np, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) tsne_df[&#39;target&#39;] = iris.target tsne_df . cp1 cp2 target . 0 -24.555080 | 7.379160 | 0 | . 1 -21.977486 | 6.074713 | 0 | . 2 -22.032751 | 7.286000 | 0 | . 3 -21.595722 | 6.841762 | 0 | . 4 -24.597988 | 7.200622 | 0 | . ... ... | ... | ... | . 145 17.367140 | -5.027658 | 2 | . 146 14.227304 | -5.973306 | 2 | . 147 16.485725 | -5.267701 | 2 | . 148 18.021391 | -4.299052 | 2 | . 149 12.856414 | -6.667024 | 2 | . 150 rows Ã— 3 columns . plt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=iris.target) plt.xlabel(&quot;cp1&quot;) plt.ylabel(&quot;cp2&quot;) . Text(0, 0.5, &#39;cp2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[MachineLearning] Dimensionality Reduction - Singular Value Decomposition(SVD)",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Overview . SVD&#46976;? . Singular Value Decompositiond ì˜ ì•½ìë¡œ &quot;íŠ¹ì´ê°’ ë¶„í•´&quot;ë¥¼ ëœ»í•¨ . | ìˆ˜ì‹ . | . $A=U&#931;V^T$ . $A$ : m x n í–‰ë ¬$U$ : m x m ì§êµí–‰ë ¬$Î£$ : m x n ëŒ€ê°í–‰ë ¬$V$ : n x n ì§êµí–‰ë ¬SVD &#51032;&#51032; . ì§êµí•˜ëŠ” ë²¡í„° ì§‘í•©ì—ì„œ, ì„ í˜• ë³€í™˜ í›„ì— í¬ê¸°ëŠ” ë³€í•˜ì§€ë§Œ ì—¬ì „íˆ ì§êµí•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ì§êµ ì§‘í•©ì„ êµ¬í•˜ëŠ” ê²ƒ, ê·¸ë¦¬ê³  ì„ í˜• ë³€í™˜ í›„ ê²°ê³¼ë¥¼ êµ¬í•˜ëŠ” ê²ƒ | . SVD &#54596;&#50836;&#49457; . EVD(ê³ ìœ ê°’ ë¶„í•´)ì˜ ê²½ìš° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì •ë°©í–‰ë ¬(n x n)ì— ëŒ€í•´ì„œë§Œ ì ìš© ê°€ëŠ¥í•¨ | í•˜ì§€ë§Œ í–‰ë ¬ì´ ì •ë°© í–‰ë ¬ì´ë“  ì•„ë‹ˆë“  ê´€ê³„ ì—†ì´ ëª¨ë“  m x n í–‰ë ¬ì— ëŒ€í•´ ì ìš© ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ìœ ìš©í•¨ | . Truncated SVD . Î£ì˜ ëŒ€ê° ì›ì†Œ ì¤‘ ìƒìœ„ ëª‡ê°œë§Œ ì¶”ì¶œí•˜ê³  ì—¬ê¸° ëŒ€ì‘í•˜ëŠ” Uì™€ Vì˜ ì›ì†Œë„ í•¨ê»˜ ì¤„ì´ëŠ” ë°©ë²• | . . 2. Example . SVD &#50696;&#49884; . import numpy as np from numpy.linalg import svd . 4 by 4 &#47004;&#45924; &#54665;&#47148; &#49373;&#49457; . np.random.seed(10) a= np.random.randn(4, 4) a . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . SVD &#48516;&#54644; . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&quot;U matrix: n&quot;, np.round(U, 3)) print(&quot;Sigma Value: n&quot;, np.round(Sigma, 3)) print(&quot;V transpose: n&quot;, np.round(Vt, 3)) . (4, 4) (4,) (4, 4) U matrix: [[-0.909 0.292 -0.29 0.07 ] [-0.055 -0.568 -0.208 0.794] [ 0.241 -0.094 -0.921 -0.292] [ 0.336 0.763 -0.157 0.529]] Sigma Value: [2.284 1.659 1.255 0.146] V transpose: [[-0.687 -0.134 0.688 0.193] [-0.422 0.856 -0.282 0.098] [-0.293 -0.046 -0.033 -0.954] [ 0.514 0.498 0.668 -0.205]] . &#44160;&#51613; . Sigma_mt = np.diag(Sigma) verify_mt = np.dot(np.dot(U, Sigma_mt), Vt) verify_mt . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . . Truncated SVD &#50696;&#49884; . from scipy.sparse.linalg import svds from scipy.linalg import svd . &#53945;&#51060;&#44050; 3&#44060;&#47196; Truncated SVD &#49688;&#54665; . U_tr, Sigma_tr, Vt_tr = svds(a, k=3) print(U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print(&quot;U_tr matrix: n&quot;, np.round(U_tr, 3)) print(&quot;Sigma_tr Value: n&quot;, np.round(Sigma_tr, 3)) print(&quot;V_tr transpose: n&quot;, np.round(Vt_tr, 3)) . (4, 3) (3,) (3, 4) U_tr matrix: [[ 0.29 0.292 0.909] [ 0.208 -0.568 0.055] [ 0.921 -0.094 -0.241] [ 0.157 0.763 -0.336]] Sigma_tr Value: [1.255 1.659 2.284] V_tr transpose: [[ 0.293 0.046 0.033 0.954] [-0.422 0.856 -0.282 0.098] [ 0.687 0.134 -0.688 -0.193]] . &#48373;&#50896; . Sigma_tr_mt = np.diag(Sigma_tr) restore = np.dot(np.dot(U_tr, Sigma_tr_mt), Vt_tr) restore . array([[ 1.32634718, 0.71020432, -1.55220761, -0.00629071], [ 0.56177478, -0.77777473, 0.18812528, 0.13234356], [ 0.02616807, -0.15341116, 0.46144994, 1.19429753], [-1.00471538, 0.98987057, 0.1771143 , 0.46097789]]) . . SVD &#54876;&#50857; . from sklearn.decomposition import TruncatedSVD from sklearn.datasets import load_iris import matplotlib.pyplot as plt import pandas as pd . &#45936;&#51060;&#53552; &#47196;&#46300; . iris = load_iris() iris_data = iris.data . Truncated SVD &#51652;&#54665; . tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_data) iris_tsvd = tsvd.transform(iris_data) result = pd.DataFrame(data = iris_tsvd, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) . &#49884;&#44033;&#54868; . plt.scatter(x=result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&quot;TruncsatedSVD Component 1&quot;) plt.ylabel(&quot;TruncsatedSVD Component 1&quot;) . Text(0, 0.5, &#39;TruncsatedSVD Component 1&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[MachineLearning] Dimensionality Reduction - Principal Component Analysis(PCA)",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Overview . PCA&#46976;? . Principal Component Analysis ì˜ ì•½ìë¡œ &quot;ì£¼ì„±ë¶„ ë¶„ì„&quot;ì„ ëœ»í•¨ | . PCA &#51032;&#51032; . nì°¨ì› ë°ì´í„°ë¥¼ ì •ì‚¬ì˜ ì‹œì¼œ k ì°¨ì›ìœ¼ë¡œ ë‚®ì¶œ ë•Œ, (n &gt; k) ì–´ë–¤ ë²¡í„°ì— ë°ì´í„°ë¥¼ ì •ì‚¬ì˜ ì‹œì¼œì•¼ ì›ë˜ ë°ì´í„°ì˜ êµ¬ì¡°ì„ ì œì¼ ì˜ ìœ ì§€ì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë‚´ëŠ” ê²ƒ | . PCA &#54596;&#50836;&#49457; . ì‹¤ì œ ë°ì´í„°ë“¤ì€ ë§¤ìš° ë§ì€ featureë¥¼ ê°€ì§€ê³  ìˆìŒ(= ì°¨ì›ì´ ë†’ìŒ). ë”°ë¼ì„œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ìˆì–´ì„œ ì•„ë˜ì™€ ì–´ë ¤ì›€ì´ ìˆìŒ ì „ì²´ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ | ì˜ë¯¸ ì—†ëŠ” faetureë“¤ì— ì˜í•´ ê³¼ì í•©ë˜ê±°ë‚˜ ì›í™œí•œ í•™ìŠµì´ ë˜ì§€ ì•ŠìŒ | | ê·¸ë˜ì„œ ì°¨ì›ì¶•ì†Œë¥¼ í†µí•´ ì „ì²´ì ì¸ ë°ì´í„° ì–‘ì„ ì¤„ì¼ í•„ìš”ê°€ ìˆìŒ | . 2. Example . import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.datasets import load_iris . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() x = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) x.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . &#45936;&#51060;&#53552; scale . x = StandardScaler().fit_transform(x) pd.DataFrame(data=x, columns=iris[&#39;feature_names&#39;]).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 -0.900681 | 1.019004 | -1.340227 | -1.315444 | . 1 -1.143017 | -0.131979 | -1.340227 | -1.315444 | . 2 -1.385353 | 0.328414 | -1.397064 | -1.315444 | . 3 -1.506521 | 0.098217 | -1.283389 | -1.315444 | . 4 -1.021849 | 1.249201 | -1.340227 | -1.315444 | . PCA &#49892;&#54665; . pca = PCA(n_components=2) result = pca.fit_transform(x) result_df = pd.DataFrame(data=result, columns = [&#39;pc1&#39;, &#39;pc2&#39;]) result_df[&#39;species&#39;] = iris[&#39;target&#39;] result_df[&#39;species&#39;] = result_df[&#39;species&#39;].map({0: &#39;setosa&#39;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) result_df.head() . pc1 pc2 species . 0 -2.264703 | 0.480027 | setosa | . 1 -2.080961 | -0.674134 | setosa | . 2 -2.364229 | -0.341908 | setosa | . 3 -2.299384 | -0.597395 | setosa | . 4 -2.389842 | 0.646835 | setosa | . PCA &#44208;&#44284;&#44032; &#50896;&#48376; &#45936;&#51060;&#53552; &#48516;&#49328;&#51032; 96%&#47484; &#49444;&#47749;&#54620;&#45796;&#44256; &#48380; &#49688; &#51080;&#51020; . sum(pca.explained_variance_ratio_) . 0.9581320720000164 . &#49884;&#44033;&#54868; . figure = plt.figure(figsize = (8, 8)) axis = figure.add_subplot(1, 1, 1) axis.set_xlabel(&#39;Principal Component 1&#39;) axis.set_ylabel(&#39;Principal Component 2&#39;) axis.set_title(&#39;PCA result&#39;) targets = iris[&#39;target_names&#39;] colors = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;] for target, color in zip(targets, colors): indices = result_df[&#39;species&#39;] == target axis.scatter(result_df.loc[indices, &#39;pc1&#39;], result_df.loc[indices, &#39;pc2&#39;], c = color, s = 50) axis.legend(targets) axis.grid() .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[MachineLearning] Dimensionality Reduction - Linear Discriminant Analysis(LDA)",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Overview . LDA&#46976;? . Linear Discriminant Analysis ì˜ ì•½ìë¡œ &quot;ì„ í˜• íŒë³„ë²•&quot;ì„ ëœ»í•¨ | . LDA &#51032;&#51032; . PCAì™€ ë§¤ìš° ìœ ì‚¬í•˜ê²Œ datasetì„ ì €ì°¨ì› ê³µê°„ì— íˆ¬ì˜í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•ì´ì§€ë§Œ, ê°œë³„ í´ë˜ìŠ¤ë¥¼ ë¶„ë³„í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ì°¨ì›ì„ ì¶•ì†Œí•¨ | ê·¸ë ‡ê¸° ë•Œë¬¸ì— Classificationì— ì ìš©í•˜ê¸° ì „ ë°ì´í„° ì „ì²˜ë¦¬ë¡œ ì‚¬ìš©í•˜ê¸° ì í•©í•¨ | . LDA &#44396;&#54788; &#50896;&#47532; . í´ë˜ìŠ¤ê°„ ë¶„ì‚°ê³¼ í´ë˜ìŠ¤ ë‚´ë¶€ ë¶„ì‚°ì˜ ë¹„ìœ¨ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œë¥¼ ì§„í–‰í•¨ | . 2. Example . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; scale &#51652;&#54665; . iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) . LDA &#51652;&#54665; . lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) result = pd.DataFrame(data = iris_lda) . &#49884;&#44033;&#54868; . plt.scatter(x = result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&#39;LDA component 1&#39;) plt.ylabel(&#39;LDA component 2&#39;) . Text(0, 0.5, &#39;LDA component 2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[MachineLearning] Dimensionality Reduction - Latent Semantic Analysis(LCA)",
            "content": "Content creators: ì¡°ë™í˜„ . Content reviewers: . 1. Overview . LCA&#46976;? . Latent Semantic Analysis ì˜ ì•½ìë¡œ &quot;ì ì¬ ì˜ë¯¸ ë¶„ì„&quot;ì„ ëœ»í•¨ | . LCA &#51032;&#51032; . í† í”½ ëª¨ë¸ë§ì´ë¼ëŠ” ë¶„ì•¼ì— ì•„ì´ë””ì–´ë¥¼ ì œê³µí•œ ì•Œê³ ë¦¬ì¦˜ | DTMì´ë‚˜ TF-IDFëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‹¨ì–´ì˜ ë¹ˆë„ ìˆ˜ë¥¼ ì´ìš©í•œ ìˆ˜ì¹˜í™” ë°©ë²•ì´ê¸° ë–„ë¬¸ì— ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬ ì´ë¥¼ ìœ„í•œ ëŒ€ì•ˆìœ¼ë¡œ ë“±ì¥í•¨ | . . 2. Example . import pandas as pd from sklearn.datasets import fetch_20newsgroups import nltk from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) documents = dataset.data print(&#39;ìƒ˜í”Œì˜ ìˆ˜ :&#39;,len(documents)) . ìƒ˜í”Œì˜ ìˆ˜ : 11314 . documents[1] . &#34; n n n n n n nYeah, do you expect people to read the FAQ, etc. and actually accept hard natheism? No, you need a little leap of faith, Jimmy. Your logic runs out nof steam! n n n n n n n nJim, n nSorry I can&#39;t pity you, Jim. And I&#39;m sorry that you have these feelings of ndenial about the faith you need to get by. Oh well, just pretend that it will nall end happily ever after anyway. Maybe if you start a new newsgroup, nalt.atheist.hard, you won&#39;t be bummin&#39; so much? n n n n n n nBye-Bye, Big Jim. Don&#39;t forget your Flintstone&#39;s Chewables! :) n-- nBake Timmons, III&#34; . print(dataset.target_names) . [&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;] . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . news_df = pd.DataFrame({&#39;document&#39;:documents}) news_df[&#39;clean_doc&#39;] = news_df[&#39;document&#39;].str.replace(&quot;[^a-zA-Z]&quot;, &quot; &quot;, regex=True) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: &#39; &#39;.join([w for w in x.split() if len(w)&gt;3])) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: x.lower()) . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons&#39; . &#48520;&#50857;&#50612; &#51228;&#44144; . stop_words = stopwords.words(&#39;english&#39;) tokenized_doc = news_df[&#39;clean_doc&#39;].apply(lambda x: x.split()) # í† í°í™” tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) . print(tokenized_doc[1]) . [&#39;yeah&#39;, &#39;expect&#39;, &#39;people&#39;, &#39;read&#39;, &#39;actually&#39;, &#39;accept&#39;, &#39;hard&#39;, &#39;atheism&#39;, &#39;need&#39;, &#39;little&#39;, &#39;leap&#39;, &#39;faith&#39;, &#39;jimmy&#39;, &#39;logic&#39;, &#39;runs&#39;, &#39;steam&#39;, &#39;sorry&#39;, &#39;pity&#39;, &#39;sorry&#39;, &#39;feelings&#39;, &#39;denial&#39;, &#39;faith&#39;, &#39;need&#39;, &#39;well&#39;, &#39;pretend&#39;, &#39;happily&#39;, &#39;ever&#39;, &#39;anyway&#39;, &#39;maybe&#39;, &#39;start&#39;, &#39;newsgroup&#39;, &#39;atheist&#39;, &#39;hard&#39;, &#39;bummin&#39;, &#39;much&#39;, &#39;forget&#39;, &#39;flintstone&#39;, &#39;chewables&#39;, &#39;bake&#39;, &#39;timmons&#39;] . TF-IDF &#54665;&#47148; &#47564;&#46308;&#44592; . detokenized_doc = [] for i in range(len(news_df)): t = &#39; &#39;.join(tokenized_doc[i]) detokenized_doc.append(t) news_df[&#39;clean_doc&#39;] = detokenized_doc . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons&#39; . vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_features= 1000, max_df = 0.5, smooth_idf=True) X = vectorizer.fit_transform(news_df[&#39;clean_doc&#39;]) print(&#39;TF-IDF í–‰ë ¬ì˜ í¬ê¸° :&#39;,X.shape) . TF-IDF í–‰ë ¬ì˜ í¬ê¸° : (11314, 1000) . &#53664;&#54589; &#47784;&#45944;&#47553; . svd_model = TruncatedSVD(n_components=20, algorithm=&#39;randomized&#39;, n_iter=100, random_state=122) svd_model.fit(X) len(svd_model.components_) . 20 . terms = vectorizer.get_feature_names() # ë‹¨ì–´ ì§‘í•©. 1,000ê°œì˜ ë‹¨ì–´ê°€ ì €ì¥ë¨. def get_topics(components, feature_names, n=5): for idx, topic in enumerate(components): print(&quot;Topic %d:&quot; % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]]) get_topics(svd_model.components_,terms) . Topic 1: [(&#39;like&#39;, 0.21386), (&#39;know&#39;, 0.20046), (&#39;people&#39;, 0.19293), (&#39;think&#39;, 0.17805), (&#39;good&#39;, 0.15128)] Topic 2: [(&#39;thanks&#39;, 0.32888), (&#39;windows&#39;, 0.29088), (&#39;card&#39;, 0.18069), (&#39;drive&#39;, 0.17455), (&#39;mail&#39;, 0.15111)] Topic 3: [(&#39;game&#39;, 0.37064), (&#39;team&#39;, 0.32443), (&#39;year&#39;, 0.28154), (&#39;games&#39;, 0.2537), (&#39;season&#39;, 0.18419)] Topic 4: [(&#39;drive&#39;, 0.53324), (&#39;scsi&#39;, 0.20165), (&#39;hard&#39;, 0.15628), (&#39;disk&#39;, 0.15578), (&#39;card&#39;, 0.13994)] Topic 5: [(&#39;windows&#39;, 0.40399), (&#39;file&#39;, 0.25436), (&#39;window&#39;, 0.18044), (&#39;files&#39;, 0.16078), (&#39;program&#39;, 0.13894)] Topic 6: [(&#39;chip&#39;, 0.16114), (&#39;government&#39;, 0.16009), (&#39;mail&#39;, 0.15625), (&#39;space&#39;, 0.1507), (&#39;information&#39;, 0.13562)] Topic 7: [(&#39;like&#39;, 0.67086), (&#39;bike&#39;, 0.14236), (&#39;chip&#39;, 0.11169), (&#39;know&#39;, 0.11139), (&#39;sounds&#39;, 0.10371)] Topic 8: [(&#39;card&#39;, 0.46633), (&#39;video&#39;, 0.22137), (&#39;sale&#39;, 0.21266), (&#39;monitor&#39;, 0.15463), (&#39;offer&#39;, 0.14643)] Topic 9: [(&#39;know&#39;, 0.46047), (&#39;card&#39;, 0.33605), (&#39;chip&#39;, 0.17558), (&#39;government&#39;, 0.1522), (&#39;video&#39;, 0.14356)] Topic 10: [(&#39;good&#39;, 0.42756), (&#39;know&#39;, 0.23039), (&#39;time&#39;, 0.1882), (&#39;bike&#39;, 0.11406), (&#39;jesus&#39;, 0.09027)] Topic 11: [(&#39;think&#39;, 0.78469), (&#39;chip&#39;, 0.10899), (&#39;good&#39;, 0.10635), (&#39;thanks&#39;, 0.09123), (&#39;clipper&#39;, 0.07946)] Topic 12: [(&#39;thanks&#39;, 0.36824), (&#39;good&#39;, 0.22729), (&#39;right&#39;, 0.21559), (&#39;bike&#39;, 0.21037), (&#39;problem&#39;, 0.20894)] Topic 13: [(&#39;good&#39;, 0.36212), (&#39;people&#39;, 0.33985), (&#39;windows&#39;, 0.28385), (&#39;know&#39;, 0.26232), (&#39;file&#39;, 0.18422)] Topic 14: [(&#39;space&#39;, 0.39946), (&#39;think&#39;, 0.23258), (&#39;know&#39;, 0.18074), (&#39;nasa&#39;, 0.15174), (&#39;problem&#39;, 0.12957)] Topic 15: [(&#39;space&#39;, 0.31613), (&#39;good&#39;, 0.3094), (&#39;card&#39;, 0.22603), (&#39;people&#39;, 0.17476), (&#39;time&#39;, 0.14496)] Topic 16: [(&#39;people&#39;, 0.48156), (&#39;problem&#39;, 0.19961), (&#39;window&#39;, 0.15281), (&#39;time&#39;, 0.14664), (&#39;game&#39;, 0.12871)] Topic 17: [(&#39;time&#39;, 0.34465), (&#39;bike&#39;, 0.27303), (&#39;right&#39;, 0.25557), (&#39;windows&#39;, 0.1997), (&#39;file&#39;, 0.19118)] Topic 18: [(&#39;time&#39;, 0.5973), (&#39;problem&#39;, 0.15504), (&#39;file&#39;, 0.14956), (&#39;think&#39;, 0.12847), (&#39;israel&#39;, 0.10903)] Topic 19: [(&#39;file&#39;, 0.44163), (&#39;need&#39;, 0.26633), (&#39;card&#39;, 0.18388), (&#39;files&#39;, 0.17453), (&#39;right&#39;, 0.15448)] Topic 20: [(&#39;problem&#39;, 0.33006), (&#39;file&#39;, 0.27651), (&#39;thanks&#39;, 0.23578), (&#39;used&#39;, 0.19206), (&#39;space&#39;, 0.13185)] .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[MachineLearning] Cluster Analysis",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Definition . Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters) . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "[MachineLearning] Cluster Analysis - Mean Shift",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Overview . Aims to discover blobs in a smooth density of samples | Works by updating candidates for centroids to be the mean of the points within a given region | Candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids | Parameters : Non-parametric(bandwidth) | . Algorithm . Determine initial centroids | While moved distance is smaller than threshold For each centroids, move points towards a region of the maximum increase in the density of points. | . | Eliminate near-duplicates to form the final set of centroids | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import MeanShift mean_shift = MeanShift(bandwidth=1.4).fit(X) pred = mean_shift.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "[MachineLearning] Cluster Analysis - K-Means",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Overview . Clusters data by trying to separate samples in n groups of equal variance | Highly dependent on the initialization of the centroids | Object : Minimize inertia(=within-cluster sum-of-squares) Inertia makes the assumption that clusters are convex and isotropic | Responds poorly to elongated clusters, or manifolds with irregular shapes | . | Parameters : number of clusters | . Algorithm . Choose n initial centroids | While the centroids do not move significantly assigns each sample to its nearest centroid | creates new centroids by taking the mean value of all of the samples assigned to each previous centroid | | Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=4, random_state=SEED).fit(X) pred = kmeans.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], label=&quot;centers&quot;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "[MachineLearning] Cluster Analysis - Fuzzy C Means",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Overview . A form of clustering in which each data point can belong to more than one cluster | In fuzzy clustering, data points can potentially belong to multiple clusters. | The fuzzy c-menas algorithm is very similar to the k-means algorithm. | Parameters : number of clusters | . Algorithm . Choose a number of clusters. | Assign coefficients randomly to each data point for being in the clusters. | While the coefficients&#39; change between iterations is no more than the given threshold: Compute the centroid for each cluster. | For each data point, compute its coefficients of being in the clusters. | | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . import skfuzzy as fuzz # scikit-fuzzy c_means = fuzz.cmeans(X.T, c=4, m=2, error=1e-3, maxiter=300) centers, coefs = c_means[0], c_means[1] pred = np.argmax(coefs, axis=0) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(centers[:, 0], centers[:, 1], label=&#39;centers&#39;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "[MachineLearning] Cluster Analysis - Agglomerative Clustering",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Overview . Performs a hierarchical clustering using a bottom up approach | Each observation starts in its own cluster, and clusters are successively merged together Ward minimizes the sum of squared differences within all clusters. | Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters. | Average linkage minimizes the average of the distances between all observations of pairs of clusters. | Single linkage minimizes the distance between the closest observations of pairs of clusters. | . | . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import AgglomerativeClustering clustering = AgglomerativeClustering() pred = clustering.fit_predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "[MachineLearning] Cluster Analysis - DBSCAN",
            "content": "Content creators: ì´ì£¼í˜• . Content reviewers: . 1. Overview . Views clusters as areas of high density separated by areas of low density | clusters found by DBSCAN can be any shape | central component to the DBSCAN is the concept of core samples core samples are in areas of high density | . | Parameters : min_samples, eps . min_samples : controls how tolerant the algorithm is towards noise | eps : controls the local neighborhood of the points | . | Core sample : a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample . | Cluster : a set of core samples that can be built by recursively taking a core sample = finding all of its neighbors that are core samples | = finding all of their neighbors that are core samples | . | non-core samepls : neighbors of a core sample in the cluster but are not themselves core samples | Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm. | . Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) n_noise_ = list(labels).count(-1) unique_labels = set(labels) colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = [0, 0, 0, 1] class_member_mask = labels == k xy = X[class_member_mask &amp; core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=14, ) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=6, ) .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "[MachineLearning] Classification",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Definition . Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. Classification is the grouping of related facts into classes. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "[MachineLearning] Classification - Support Vector Machine",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . SVM(Support Vector Machine)&#46976;? . ê²°ì • ê²½ê³„(Decision Boundary), ì¦‰ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê¸°ì¤€ ì„ ì„ ì •ì˜í•˜ëŠ” ëª¨ë¸ . | ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì–´ëŠìª½ ê²°ì •ê²½ê³„ì— í¬í•¨í•˜ëŠ”ì§€ì— ë”°ë¼ ë¶„ë¥˜ . | . &#51339;&#51008; &#44208;&#51221;&#44221;&#44228;&#46976;? . ë°ì´í„° êµ°ìœ¼ë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì ¸ìˆëŠ” ê²°ì • ê²½ê²Œ . | ì„œí¬íŠ¸ ë²¡í„°: ê²°ì • ê²½ê³„ì™€ ê°€ê¹Œì´ ìˆëŠ” ë°ì´í„°ë“¤ . | Margin: ê²°ì • ê²½ê³„ì™€ ì„œí¬íŠ¸ ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ . | ìµœì ì˜ ê²°ì •ê²½ê²ŒëŠ” Marginì„ ìµœëŒ€í™” í•œë‹¤ . | nê°œì˜ ì†ì„±ì„ ê°€ì§„ ë°ì´í„°ì—ëŠ” ìµœì†Œ n+1ê°œì˜ ì„œí¬íŠ¸ ë²¡í„°ê°€ ì¡´ì¬ . | . SVM &#51109;&#51216; . SVMì—ì„œ ê²°ì • ê²½ê³„ëŠ” ì„œí¬íŠ¸ ë²¡í„°ì— ì˜í•´ ì •ì˜ë˜ë¯€ë¡œ, ë°ì´í„° ì¤‘ì—ì„œ ì„œí¬íŠ¸ ë²¡í„°ë§Œì„ ì˜ ì„ ë³„í•˜ë©´ í•„ìš”ì—†ëŠ” ë°ì´í„°ë“¤ì„ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤. . | ì´ë¡œì¸í•´ ë§¤ìš° ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.svm import SVC classifier = SVC(kernel = &#39;linear&#39;) classifier.fit(X_train, y_train) . SVC(kernel=&#39;linear&#39;) . &#50696;&#52769; . classifier.predict(X_valid) . array([2, 1, 2, 1, 0, 1, 2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 0, 0, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 2, 0, 0]) . classifier.score(X_valid, y_valid) . 0.9555555555555556 . classifier.score(X_valid, y_valid) . 0.9555555555555556 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "relUrl": "/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "[MachineLearning] Classification - Naive Bayes Classification",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . Naive Bayes&#46976;? . ë² ì´ì¦ˆ ì •ë¦¬ì— ê¸°ë°˜í•œ í†µê³„ì  ë¶„ë¥˜ ê¸°ë²• . | ê³µí†µì ìœ¼ë¡œ ëª¨ë“  íŠ¹ì„±ë“¤ì´ ì„œë¡œ ë…ë¦½ì„ì„ ê°€ì • . | ë³µì¡í•œ ë°˜ë³µ ë§¤ê°œë³€ìˆ˜ ì¶”ì • ì—†ì–´ ë§¤ìš° í° ë°ì´í„°ì…‹ì— ìœ ìš©í•¨ . | ì •í™•ì„±ì´ ë†’ìŒ . | ìŠ¤íŒ¸ ë©”ì¼ í•„í„°, í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê°ì • ë¶„ì„, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì— í™œìš© . | . &#50508;&#44256;&#47532;&#51608; . ë² ì´ì¦ˆ ì •ë¦¬ëŠ” P(c), P(x) ë° P(x|c)ë¡œë¶€í„° í›„ë°© í™•ë¥  P(c|x)ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. . | Naigive Bayes ë¶„ë¥˜ìëŠ” ì£¼ì–´ì§„ í´ë˜ìŠ¤(c)ì— ëŒ€í•œ ì˜ˆì¸¡ ë³€ìˆ˜(x) ê°’ì˜ íš¨ê³¼ê°€ ë‹¤ë¥¸ ì˜ˆì¸¡ ë³€ìˆ˜ì˜ ê°’ê³¼ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤. . | . P(c|x)ëŠ” ì£¼ì–´ì§„ ì˜ˆì¸¡ ë³€ìˆ˜(ì†ì„±)ì—ì„œ í´ë˜ìŠ¤(ëª©í‘œê°’)ì˜ í›„ë°© í™•ë¥  . | P(c)ëŠ” í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥  . | P(x|c)ëŠ” ì£¼ì–´ì§„ í´ë˜ìŠ¤ì˜ í™•ë¥ ì¸ ìš°ë„ . | P(x)ëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ì˜ ì‚¬ì „ í™•ë¥  . | $$ Bayes Rule: {P(c|x) = }{{P(x|c)P(c)} over P(x)} $$ . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.naive_bayes import GaussianNB #Create a Gaussian Classifier classifier = GaussianNB() # Train the model using the training sets classifier.fit(X_train, y_train) #Predict Output predicted= classifier.predict(X_valid) # 0:Overcast, 2:Mild print(&quot;Predicted Value:&quot;, predicted) # 1: Yes . Predicted Value: [2 1 1 1 0 1 2 0 2 2 2 0 1 2 0 1 0 0 1 2 2 0 1 1 2 1 1 1 1 1 0 0 1 2 2 0 2 1 0 2 0 0 2 0 0] . &#50696;&#52769; . classifier.score(X_valid, y_valid) . 0.9333333333333333 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "[MachineLearning] Classification - K-Nearest Neighbor",
            "content": "Content creators: ì´ì¤‘ì› . Content reviewers: . 1. Overview . K-Nearest Neighbor&#46976;? . ë¹„ëª¨ìˆ˜ ë°€ë„ì¶”ì • ë°©ë²•ì´ë‹¤. (í™•ë¥ ë¶„í¬ ëª¨ë¸ì„ ë¯¸ë¦¬ ê°€ì •í•˜ì§€ ì•Šê³  ë°ì´í„° ì§‘í•©ì„ ì´ìš©) . | ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì—¬ ì˜ˆì¸¡ì— ì‚¬ìš©í•œë‹¤. . | ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ì›ƒí•œ Kê°œì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ëŠ”ë‹¤. . | ì°¾ì•„ì§„ ì´ì›ƒ ë°ì´í„°ë“¤ì´ ë§ì´ ì†í•œ í´ë˜ìŠ¤ì— í• ë‹¹í•œë‹¤. . | . &#54617;&#49845; &#50508;&#44256;&#47532;&#51608; . ì£¼ì–´ì§„ ë°ì´í„° xì™€ ëª¨ë“  í•™ìŠµ ë°ì´í„° {x1,x2, ..., xN} ê³¼ì˜ ê±°ë¦¬ë¥¼ ê³„ ì‚°í•œë‹¤. . | ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ Kê°œì˜ ë°ì´í„°ë¥¼ ì°¾ì•„ í›„ë³´ ì§‘ í•© N(x)={x1,x2,..., xK}ì„ ë§Œë“ ë‹¤. . | í›„ë³´ ì§‘í•©ì˜ ê° ì›ì†Œê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ê·¸ ë¼ë²¨ê°’ c(x1), c(x2),...,c(xK)ì„ ì°¾ëŠ”ë‹¤. . | ì°¾ì•„ì§„ ë¼ë²¨ ê°’ ì¤‘ ê°€ì¥ ë§ì€ ë¹ˆë„ìˆ˜ë¥¼ ì°¨ì§€í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì°¾ì•„ xë¥¼ ê·¸ í´ë˜ìŠ¤ì— í• ë‹¹í•œë‹¤. . | &#44256;&#47140;&#49324;&#54637; . K&#51032; &#44050; . Kê°€ ì‘ë‹¤ë©´, ëª‡ê°œì˜ ì´ì›ƒí•œ ë°ì´í„°ì—ë§Œ ì˜ì¡´í•˜ì—¬ í´ë˜ìŠ¤ê°€ ê²°ì •ë¨ | ì´ëŠ” ë…¸ì´ì¦ˆì— ë¯¼ê°, ì˜¤ë²„í”¼íŒ…ì— ë°œìƒ | ë°ì´í„° íŠ¹ì •ì— ë”°ë¼ ì ì ˆí•œ Kë¥¼ ì„ íƒí•´ì•¼í•¨ | . &#44144;&#47532;&#54632;&#49688;&#47484; &#50612;&#46523;&#44172; &#49444;&#51221;&#54624; &#44163;&#51064;&#44032;? . ë‹¤ì–‘í•œ ê±°ë¦¬ í•¨ìˆ˜ê°€ ì¡´ì¬í•¨ | ê±°ë¦¬í•¨ìˆ˜ì— ë”°ë¼, ì˜ˆì¸¡ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ | ex) 1ì°¨ ë…¸ë¦„, 2ì°¨ ë…¸ë¦„, ë‚´ì , cosine distance ë“± .. | . &#51109;&#51216; . ë³µì¡í•œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë¹„êµì  ì˜ ì‘ë™í•¨ | í•™ìŠµì— ì‹œê°„ì´ ê±¸ë¦¬ì§€ ì•ŠìŒ | . &#45800;&#51216; . ìƒˆ ë°ì´í„°ê°€ ì£¼ì–´ì§ˆ ë•Œ, ëª¨ë“  í•™ìŠµ ë°ì´í„°ì™€ì˜ ê±°ë¦¬ë¥¼ êµ¬í•´ì£¼ì–´ì•¼í•¨ (ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¦) | í•™ìŠµ ë°ì´í„°ë¥¼ ëª¨ë‘ ì €ì¥í•˜ê³  ìˆì–´ì•¼í•¨ (ë©”ëª¨ë¦¬ ë¬¸ì œ) | . 2. Example . from sklearn.datasets import load_iris import pandas as pd from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . iris = load_iris() . df = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) df[&#39;target&#39;] = iris[&#39;target&#39;] df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, :4], df[&#39;target&#39;], stratify=df[&#39;target&#39;], test_size=0.2, random_state=30) x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) knn.fit(x_train, y_train) . KNeighborsClassifier(n_jobs=-1) . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . prediction = knn.predict(x_valid) (prediction == y_valid).mean() knn.score(x_valid, y_valid) . 0.9333333333333333 . &#52572;&#51201;&#51032; K&#44050; &#52286;&#44592; . test_scores = [] train_scores = [] for i in range(1,10): knn = KNeighborsClassifier(i) knn.fit(x_train,y_train) train_scores.append(knn.score(x_train,y_train)) test_scores.append(knn.score(x_valid,y_valid)) . max_train_score = max(train_scores) train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score] print(&#39;Max train score {} % and k = {}&#39;.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind)))) . Max train score 100.0 % and k = [1] . max_test_score = max(test_scores) test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score] print(&#39;Max test score {} % and k = {}&#39;.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind)))) . Max test score 96.66666666666667 % and k = [1, 3, 4, 7, 8, 9] . plt.figure(figsize=(12,5)) p = sns.lineplot(range(1,10),train_scores,marker=&#39;*&#39;,label=&#39;Train Score&#39;) p = sns.lineplot(range(1,10),test_scores,marker=&#39;o&#39;,label=&#39;Test Score&#39;) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . knn = KNeighborsClassifier(7) knn.fit(x_train,y_train) knn.score(x_valid,y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "relUrl": "/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "date": " â€¢ Mar 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://knu-ai-researcher.github.io/reports/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://knu-ai-researcher.github.io/reports/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}