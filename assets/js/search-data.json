{
  
    
        "post0": {
            "title": "[NeuralNetwork] Convolutional Neural Network - VGG",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Image(&#39;fig01.png&#39;, width=500) . Pooling 이전에 Conv layer를 2~3개까지 두어 깊게 쌓은 CNN 모델 | Conv layer(filter size = 3)을 3개로 연속하게 두면, receptive field가 7 X 7까지 확장되어 좋은 효과를 얻을 수 있음이 특징. | . . 2. Example . import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision.datasets import CIFAR10 from torchvision import transforms from torchvision.models import vgg19 . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cuda&#39;, index=0) . transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])]) train_dataset = CIFAR10(root=&#39;dataset/&#39;, download=True, train=True, transform=transform) test_dataset = CIFAR10(root=&#39;dataset/&#39;, download=True, train=False, transform=transform) train_dataloader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=8) test_dataloader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True, num_workers=8) . Files already downloaded and verified Files already downloaded and verified . model = vgg19(pretrained=True).to(device) . # Layers freeze model.train() for param in model.features.parameters(): param.requires_grad = False for param in model.classifier[:2].parameters(): param.requires_grad = False model.classifier[6].out_features = 10 . optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001) loss_fn = torch.nn.CrossEntropyLoss() . Train . epoch = 10 for epoch in range(epoch): running_loss = 0.0 for i, data in enumerate(train_dataloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;[EPOCH {epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}&#39;) running_loss = 0.0 print(&#39;Finished Training&#39;) . [EPOCH 1, 196] loss: 0.114 [EPOCH 2, 196] loss: 0.085 [EPOCH 3, 196] loss: 0.082 [EPOCH 4, 196] loss: 0.079 [EPOCH 5, 196] loss: 0.077 [EPOCH 6, 196] loss: 0.076 [EPOCH 7, 196] loss: 0.074 [EPOCH 8, 196] loss: 0.074 [EPOCH 9, 196] loss: 0.073 [EPOCH 10, 196] loss: 0.072 Finished Training . Test . model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_dataloader: data, target = data.to(device), target.to(device) output = model(data) test_loss += loss_fn(output, target).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() print(&#39; nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) n&#39;.format(test_loss, correct, len(test_dataloader.dataset), 100. * correct / len(test_dataloader.dataset))) . Test set: Average loss: 26.5306, Accuracy: 7675/10000 (77%) .",
            "url": "https://knu-ai-researcher.github.io/reports/convolutional%20neural%20network/2022/04/02/nn-vgg.html",
            "relUrl": "/convolutional%20neural%20network/2022/04/02/nn-vgg.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[NeuralNetwork] Convolutional Neural Network - ResNet",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Image(&#39;fig01.png&#39;, width=500) . Image(&#39;fig02.png&#39;, width=800) . Stacked residual blocks | 모든 residual block은 두개의 33 conv layer를 둠.(55 receptive field) | Skip connection을 두어 깊게 쌓을 수 있으며, vanishing gradient 현상이 발생하지 않음. | FC layer를 여러개 두어 발생하는 translation sensitive 문제를 한 개의 fc layer만 두어 해결 | . . 2. Example . import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision.datasets import CIFAR10 from torchvision import transforms from torchvision.models import resnet34 . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cuda&#39;, index=0) . transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])]) train_dataset = CIFAR10(root=&#39;dataset/&#39;, download=True, train=True, transform=transform) test_dataset = CIFAR10(root=&#39;dataset/&#39;, download=True, train=False, transform=transform) train_dataloader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=8) test_dataloader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True, num_workers=8) . Files already downloaded and verified Files already downloaded and verified . model = resnet34(pretrained=True).to(device) . # Layers freeze model.train() for param in model.parameters(): param.requires_grad = False for param in model.fc.parameters(): param.requires_grad = True model.fc.out_features = 10 . optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001) loss_fn = torch.nn.CrossEntropyLoss() . Train . epoch = 10 for epoch in range(epoch): running_loss = 0.0 for i, data in enumerate(train_dataloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;[EPOCH {epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}&#39;) running_loss = 0.0 print(&#39;Finished Training&#39;) . [EPOCH 1, 196] loss: 0.186 [EPOCH 2, 196] loss: 0.074 [EPOCH 3, 196] loss: 0.062 [EPOCH 4, 196] loss: 0.058 [EPOCH 5, 196] loss: 0.055 [EPOCH 6, 196] loss: 0.053 [EPOCH 7, 196] loss: 0.053 [EPOCH 8, 196] loss: 0.052 [EPOCH 9, 196] loss: 0.051 [EPOCH 10, 196] loss: 0.050 Finished Training . Test . model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_dataloader: data, target = data.to(device), target.to(device) output = model(data) test_loss += loss_fn(output, target).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() print(&#39; nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) n&#39;.format(test_loss, correct, len(test_dataloader.dataset), 100. * correct / len(test_dataloader.dataset))) . Test set: Average loss: 22.0000, Accuracy: 8171/10000 (82%) .",
            "url": "https://knu-ai-researcher.github.io/reports/convolutional%20neural%20network/2022/04/02/nn-resnet.html",
            "relUrl": "/convolutional%20neural%20network/2022/04/02/nn-resnet.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[NeuralNetwork] Convolutional Neural Network",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Convolutional Neural Network are neural network with local connectivity, shared weights. | Conv layer $ rightarrow$ Pooling $ rightarrow$ Conv layer $ rightarrow$ Pooling $ rightarrow$ Flatten $ rightarrow$ Fully connected layer | Flatten 될 때, translation sensitive 문제 발생 | . . 2. Example . import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision.datasets import MNIST from torchvision import transforms . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=0, std=1)]) train_dataset = MNIST(root=&#39;dataset/&#39;, download=True, train=True, transform=transform) test_dataset = MNIST(root=&#39;dataset/&#39;, download=True, train=False, transform=transform) train_dataloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=8) test_dataloader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True, num_workers=8) . class CNNModel(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=2)) self.conv2 = nn.Sequential( nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=2)) self.conv3 = nn.Sequential( nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=2)) self.fc = nn.Sequential( nn.Flatten(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 10), nn.Softmax(dim=1) ) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = self.conv3(x) x = self.fc(x) return x # model model = CNNModel().to(device) . optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001) loss_fn = torch.nn.CrossEntropyLoss() . Train . epoch = 10 model.train() for epoch in range(epoch): running_loss = 0.0 for i, data in enumerate(train_dataloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;[EPOCH {epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}&#39;) running_loss = 0.0 print(&#39;Finished Training&#39;) . [EPOCH 1, 469] loss: 0.354 [EPOCH 2, 469] loss: 0.353 [EPOCH 3, 469] loss: 0.353 [EPOCH 4, 469] loss: 0.353 [EPOCH 5, 469] loss: 0.352 [EPOCH 6, 469] loss: 0.351 [EPOCH 7, 469] loss: 0.351 [EPOCH 8, 469] loss: 0.351 [EPOCH 9, 469] loss: 0.352 [EPOCH 10, 469] loss: 0.350 Finished Training . Test . model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_dataloader: data, target = data.to(device), target.to(device) output = model(data) test_loss += loss_fn(output, target).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() print(&#39; nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) n&#39;.format(test_loss, correct, len(test_dataloader.dataset), 100. * correct / len(test_dataloader.dataset))) . Test set: Average loss: 117.9541, Accuracy: 9691/10000 (97%) .",
            "url": "https://knu-ai-researcher.github.io/reports/convolutional%20neural%20network/2022/04/02/nn-convolutional-neural-network.html",
            "relUrl": "/convolutional%20neural%20network/2022/04/02/nn-convolutional-neural-network.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[NeuralNetwork] Natural Language Processing",
            "content": "Content creators: 이주형 . Content reviewers: . Advanced Topics of Natural Language Processing . The Encoder-decoder Model . Overview . Encoder-decoder networks, or sequence-to-sequence networks, are models to generate contextually appropriate output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue. . Encoder-decoder networks consist of three components: . An encoder ahtat accepts an input sequence, $ x_1^n$, and generates a corresponding sequence of contextualized representations, $ h_1^n $. | A *context vector, $ c $, which is a function of $ h_1^n $, and conveys the essence of the input to the decoder. | A *decoder, which accepts $ c $ as input and generates an arbitrary length sequence of hidden states $ h_1^m $, from which a corresponding sequence of output states $ y_1^m $, can be obtained. | . Code . from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator from torchtext.datasets import Multi30k import torch from torch import nn from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader import torch.nn.functional as F from typing import Iterable, List import random . # !python -m spacy download de_core_news_sm . SRC_LANGUAGE = &#39;de&#39; TGT_LANGUAGE = &#39;en&#39; token_transform = {} vocab_transform = {} token_transform[SRC_LANGUAGE] = get_tokenizer(&#39;spacy&#39;, language=&#39;de_core_news_sm&#39;) token_transform[TGT_LANGUAGE] = get_tokenizer(&#39;spacy&#39;, language=&#39;en_core_web_sm&#39;) # helper function to yield list of tokens def yield_tokens(data_iter: Iterable, language: str) -&gt; List[str]: language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} for data_sample in data_iter: yield token_transform[language](data_sample[language_index[language]]) # Define special symbols and indices UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # Make sure the tokens are in order of their indices to properly insert them in vocab special_symbols = [&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;bos&gt;&#39;, &#39;&lt;eos&gt;&#39;] for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: # Training data Iterator train_iter = Multi30k(split=&#39;train&#39;, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) # Create torchtext&#39;s Vocab object vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln), min_freq=1, specials=special_symbols, special_first=True) # Set UNK_IDX as the default index. This index is returned when the token is not found. # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: vocab_transform[ln].set_default_index(UNK_IDX) . /NasData/home/ljh/.pyenv/versions/miniconda3-4.7.12/envs/nlp/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again. warnings.warn(&#34;Some child DataPipes are not exhausted when __iter__ is called. We are resetting &#34; . DEVICE = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # helper function to club together sequential operations def sequential_transforms(*transforms): def func(txt_input): for transform in transforms: txt_input = transform(txt_input) return txt_input return func # function to add BOS/EOS and create tensor for input sequence indices def tensor_transform(token_ids: List[int]): return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))) # src and tgt language text transforms to convert raw strings into tensors indices text_transform = {} for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization vocab_transform[ln], #Numericalization tensor_transform) # Add BOS/EOS and create tensor # function to collate data samples into batch tesors def collate_fn(batch): src_batch, tgt_batch = [], [] for src_sample, tgt_sample in batch: src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(&quot; n&quot;))) tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(&quot; n&quot;))) src_batch = pad_sequence(src_batch, padding_value=PAD_IDX) tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX) return src_batch, tgt_batch . torch.manual_seed(0) SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE]) TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE]) EMB_SIZE = 256 HID_DIM = 512 BATCH_SIZE = 64 NUM_LAYERS = 2 ENC_DROPOUT = 0.5 DEC_DROPOUT = 0.5 loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) . from torch.utils.data import DataLoader def train_epoch(model, optimizer): model.train() losses = 0 train_iter = Multi30k(split=&#39;train&#39;, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn) count = 0 for src, tgt in train_dataloader: count += src.size()[0] src = src.to(DEVICE) tgt = tgt.to(DEVICE) tgt_input = tgt[:-1, :] logits = model(src, tgt_input, ) optimizer.zero_grad() tgt_out = tgt[1:, :] loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)) loss.backward() optimizer.step() losses += loss.item() return losses / count def evaluate(model): model.eval() losses = 0 val_iter = Multi30k(split=&#39;valid&#39;, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn) count = 0 for src, tgt in val_dataloader: count += src.size()[0] src = src.to(DEVICE) tgt = tgt.to(DEVICE) tgt_input = tgt[:-1, :] logits = model(src, tgt_input) tgt_out = tgt[1:, :] loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)) losses += loss.item() return losses / count . class Encoder(nn.Module): def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout): super().__init__() self.hid_dim = hid_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, emb_dim) self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout) self.dropout = nn.Dropout(dropout) def forward(self, src): #src = [src len, batch size] embedded = self.dropout(self.embedding(src)) #embedded = [src len, batch size, emb dim] outputs, (hidden, cell) = self.rnn(embedded) #outputs = [src len, batch size, hid dim * n directions] #hidden = [n layers * n directions, batch size, hid dim] #cell = [n layers * n directions, batch size, hid dim] #outputs are always from the top hidden layer return hidden, cell class Decoder(nn.Module): def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout): super().__init__() self.output_dim = output_dim self.hid_dim = hid_dim self.n_layers = n_layers self.embedding = nn.Embedding(output_dim, emb_dim) self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout) self.fc_out = nn.Linear(hid_dim, output_dim) self.dropout = nn.Dropout(dropout) def forward(self, input, hidden, cell): #input = [batch size] #hidden = [n layers * n directions, batch size, hid dim] #cell = [n layers * n directions, batch size, hid dim] #n directions in the decoder will both always be 1, therefore: #hidden = [n layers, batch size, hid dim] #context = [n layers, batch size, hid dim] input = input.unsqueeze(0) #input = [1, batch size] embedded = self.dropout(self.embedding(input)) #embedded = [1, batch size, emb dim] output, (hidden, cell) = self.rnn(embedded, (hidden, cell)) #output = [seq len, batch size, hid dim * n directions] #hidden = [n layers * n directions, batch size, hid dim] #cell = [n layers * n directions, batch size, hid dim] #seq len and n directions will always be 1 in the decoder, therefore: #output = [1, batch size, hid dim] #hidden = [n layers, batch size, hid dim] #cell = [n layers, batch size, hid dim] prediction = self.fc_out(output.squeeze(0)) #prediction = [batch size, output dim] return prediction, hidden, cell class Seq2Seq(nn.Module): def __init__(self, encoder, decoder, device): super().__init__() self.encoder = encoder self.decoder = decoder self.device = device assert encoder.hid_dim == decoder.hid_dim, &quot;Hidden dimensions of encoder and decoder must be equal!&quot; assert encoder.n_layers == decoder.n_layers, &quot;Encoder and decoder must have equal number of layers!&quot; def forward(self, src, trg, teacher_forcing_ratio = 0.5): #src = [src len, batch size] #trg = [trg len, batch size] #teacher_forcing_ratio is probability to use teacher forcing #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time batch_size = trg.shape[1] trg_len = trg.shape[0] trg_vocab_size = self.decoder.output_dim #tensor to store decoder outputs outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device) #last hidden state of the encoder is used as the initial hidden state of the decoder hidden, cell = self.encoder(src) #first input to the decoder is the &lt;sos&gt; tokens input = trg[0,:] for t in range(1, trg_len): #insert input token embedding, previous hidden and previous cell states #receive output tensor (predictions) and new hidden and cell states output, hidden, cell = self.decoder(input, hidden, cell) #place predictions in a tensor holding predictions for each token outputs[t] = output #decide if we are going to use teacher forcing or not teacher_force = random.random() &lt; teacher_forcing_ratio #get the highest predicted token from our predictions top1 = output.argmax(1) #if teacher forcing, use actual next token as next input #if not, use predicted token input = trg[t] if teacher_force else top1 return outputs . enc = Encoder(SRC_VOCAB_SIZE, EMB_SIZE, HID_DIM, NUM_LAYERS, ENC_DROPOUT) dec = Decoder(TGT_VOCAB_SIZE, EMB_SIZE, HID_DIM, NUM_LAYERS, DEC_DROPOUT) model = Seq2Seq(enc, dec, DEVICE) for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) moel = model.to(DEVICE) loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) . from timeit import default_timer as timer NUM_EPOCHS = 18 for epoch in range(1, NUM_EPOCHS+1): start_time = timer() train_loss = train_epoch(model, optimizer) end_time = timer() val_loss = evaluate(model) print((f&quot;Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, &quot;f&quot;Epoch time = {(end_time - start_time):.3f}s&quot;)) . Epoch: 1, Train loss: 0.221, Val loss: 0.203, Epoch time = 45.336s Epoch: 2, Train loss: 0.202, Val loss: 0.199, Epoch time = 45.441s Epoch: 3, Train loss: 0.197, Val loss: 0.197, Epoch time = 45.388s Epoch: 4, Train loss: 0.194, Val loss: 0.194, Epoch time = 45.424s Epoch: 5, Train loss: 0.191, Val loss: 0.191, Epoch time = 45.771s Epoch: 6, Train loss: 0.188, Val loss: 0.189, Epoch time = 45.990s Epoch: 7, Train loss: 0.186, Val loss: 0.188, Epoch time = 45.483s Epoch: 8, Train loss: 0.184, Val loss: 0.186, Epoch time = 45.484s Epoch: 9, Train loss: 0.182, Val loss: 0.185, Epoch time = 45.589s Epoch: 10, Train loss: 0.181, Val loss: 0.184, Epoch time = 45.523s Epoch: 11, Train loss: 0.179, Val loss: 0.183, Epoch time = 45.555s Epoch: 12, Train loss: 0.178, Val loss: 0.182, Epoch time = 45.587s Epoch: 13, Train loss: 0.176, Val loss: 0.181, Epoch time = 45.530s Epoch: 14, Train loss: 0.175, Val loss: 0.180, Epoch time = 46.451s Epoch: 15, Train loss: 0.174, Val loss: 0.180, Epoch time = 47.121s Epoch: 16, Train loss: 0.173, Val loss: 0.180, Epoch time = 46.128s Epoch: 17, Train loss: 0.172, Val loss: 0.179, Epoch time = 48.337s Epoch: 18, Train loss: 0.170, Val loss: 0.179, Epoch time = 49.316s . Attention Mechanism . Overview . The attention mechanism is a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. In the attention mechanism, as in the vanilla encoder-decoder model, the context vector $ c $ is a single vector that is a function of the hidden states of the encoder. . The idea of attention is to create the single fixed-length vector $ c $ by taking a weighted sum of all the encoder hidden states. The weights focus on a particular part of the source text that is relevant for the token the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, dirrerent for each token in decoding. . This context vector, $ c_i $, is generated anew with each decoding step $ i $ and takes all of the encoder hidden states into account in its derivation. We then make this context available during decoding by conditioning the computation of the current decoder hidden state on it (along with the prior hidden state and the previous output generated by the decoder). . Code . class AttentionEncoder(nn.Module): def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout): super().__init__() self.embedding = nn.Embedding(input_dim, emb_dim) self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim) self.dropout = nn.Dropout(dropout) def forward(self, src): #src = [src len, batch size] embedded = self.dropout(self.embedding(src)) #embedded = [src len, batch size, emb dim] outputs, hidden = self.rnn(embedded) #outputs = [src len, batch size, hid dim * num directions] #hidden = [n layers * num directions, batch size, hid dim] #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...] #outputs are always from the last layer #hidden [-2, :, : ] is the last of the forwards RNN #hidden [-1, :, : ] is the last of the backwards RNN #initial decoder hidden is final hidden state of the forwards and backwards # encoder RNNs fed through a linear layer hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))) #outputs = [src len, batch size, enc hid dim * 2] #hidden = [batch size, dec hid dim] return outputs, hidden . class Attention(nn.Module): def __init__(self, enc_hid_dim, dec_hid_dim): super().__init__() self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim) self.v = nn.Linear(dec_hid_dim, 1, bias = False) def forward(self, hidden, encoder_outputs): #hidden = [batch size, dec hid dim] #encoder_outputs = [src len, batch size, enc hid dim * 2] batch_size = encoder_outputs.shape[1] src_len = encoder_outputs.shape[0] #repeat decoder hidden state src_len times hidden = hidden.unsqueeze(1).repeat(1, src_len, 1) encoder_outputs = encoder_outputs.permute(1, 0, 2) #hidden = [batch size, src len, dec hid dim] #encoder_outputs = [batch size, src len, enc hid dim * 2] energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) #energy = [batch size, src len, dec hid dim] attention = self.v(energy).squeeze(2) #attention= [batch size, src len] return F.softmax(attention, dim=1) . class AttentionDecoder(nn.Module): def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention): super().__init__() self.output_dim = output_dim self.attention = attention self.embedding = nn.Embedding(output_dim, emb_dim) self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim) self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim) self.dropout = nn.Dropout(dropout) def forward(self, input, hidden, encoder_outputs): #input = [batch size] #hidden = [batch size, dec hid dim] #encoder_outputs = [src len, batch size, enc hid dim * 2] input = input.unsqueeze(0) #input = [1, batch size] embedded = self.dropout(self.embedding(input)) #embedded = [1, batch size, emb dim] a = self.attention(hidden, encoder_outputs) #a = [batch size, src len] a = a.unsqueeze(1) #a = [batch size, 1, src len] encoder_outputs = encoder_outputs.permute(1, 0, 2) #encoder_outputs = [batch size, src len, enc hid dim * 2] weighted = torch.bmm(a, encoder_outputs) #weighted = [batch size, 1, enc hid dim * 2] weighted = weighted.permute(1, 0, 2) #weighted = [1, batch size, enc hid dim * 2] rnn_input = torch.cat((embedded, weighted), dim = 2) #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim] output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0)) #output = [seq len, batch size, dec hid dim * n directions] #hidden = [n layers * n directions, batch size, dec hid dim] #seq len, n layers and n directions will always be 1 in this decoder, therefore: #output = [1, batch size, dec hid dim] #hidden = [1, batch size, dec hid dim] #this also means that output == hidden assert (output == hidden).all() embedded = embedded.squeeze(0) output = output.squeeze(0) weighted = weighted.squeeze(0) prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) #prediction = [batch size, output dim] return prediction, hidden.squeeze(0) . class AttentionMechanism(nn.Module): def __init__(self, encoder, decoder, device): super().__init__() self.encoder = encoder self.decoder = decoder self.device = device def forward(self, src, trg, teacher_forcing_ratio = 0.5): #src = [src len, batch size] #trg = [trg len, batch size] #teacher_forcing_ratio is probability to use teacher forcing #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time batch_size = src.shape[1] trg_len = trg.shape[0] trg_vocab_size = self.decoder.output_dim #tensor to store decoder outputs outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device) #encoder_outputs is all hidden states of the input sequence, back and forwards #hidden is the final forward and backward hidden states, passed through a linear layer encoder_outputs, hidden = self.encoder(src) #first input to the decoder is the &lt;sos&gt; tokens input = trg[0,:] for t in range(1, trg_len): #insert input token embedding, previous hidden state and all encoder hidden states #receive output tensor (predictions) and new hidden state output, hidden = self.decoder(input, hidden, encoder_outputs) #place predictions in a tensor holding predictions for each token outputs[t] = output #decide if we are going to use teacher forcing or not teacher_force = random.random() &lt; teacher_forcing_ratio #get the highest predicted token from our predictions top1 = output.argmax(1) #if teacher forcing, use actual next token as next input #if not, use predicted token input = trg[t] if teacher_force else top1 return outputs . attn = Attention(HID_DIM, HID_DIM) enc = AttentionEncoder(SRC_VOCAB_SIZE, EMB_SIZE, HID_DIM, HID_DIM, ENC_DROPOUT) dec = AttentionDecoder(TGT_VOCAB_SIZE, EMB_SIZE, HID_DIM, HID_DIM, DEC_DROPOUT, attn) model = AttentionMechanism(enc, dec, DEVICE) for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) moel = model.to(DEVICE) loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) . from timeit import default_timer as timer NUM_EPOCHS = 5 for epoch in range(1, NUM_EPOCHS+1): start_time = timer() train_loss = train_epoch(model, optimizer) end_time = timer() val_loss = evaluate(model) print((f&quot;Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, &quot;f&quot;Epoch time = {(end_time - start_time):.3f}s&quot;)) . /NasData/home/ljh/.pyenv/versions/miniconda3-4.7.12/envs/nlp/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again. warnings.warn(&#34;Some child DataPipes are not exhausted when __iter__ is called. We are resetting &#34; . Epoch: 1, Train loss: 0.216, Val loss: 0.199, Epoch time = 61.528s Epoch: 2, Train loss: 0.198, Val loss: 0.196, Epoch time = 62.244s Epoch: 3, Train loss: 0.193, Val loss: 0.191, Epoch time = 62.492s Epoch: 4, Train loss: 0.188, Val loss: 0.188, Epoch time = 62.553s Epoch: 5, Train loss: 0.184, Val loss: 0.184, Epoch time = 62.161s .",
            "url": "https://knu-ai-researcher.github.io/reports/natural%20language%20processing/2022/04/02/nlp.html",
            "relUrl": "/natural%20language%20processing/2022/04/02/nlp.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[NeuralNetwork] Reinforcement learning - Q-Learning",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . Q-Learning &#51060;&#46976;? . 유한 마르코프 결정에서 최적의 정책을 찾기 위해 사용 | 주어진 상태에서 주어진 행동을 수행하는 것에 대한 기대값을 예측하는 함수인 Q 함수를 학습 | . . 2.Example . import gym import numpy as np import matplotlib.pyplot as plt from gym.envs.registration import register register( id=&#39;FrozenLake-v3&#39;, entry_point = &#39;gym.envs.toy_text:FrozenLakeEnv&#39;, kwargs={&#39;map_name&#39;:&#39;4x4&#39;, &#39;is_slippery&#39;:False} ) env = gym.make(&#39;FrozenLake-v3&#39;) . /home/yang/.local/lib/python3.8/site-packages/gym/envs/registration.py:595: UserWarning: WARN: Overriding environment FrozenLake-v3 logger.warn(f&#34;Overriding environment {id}&#34;) . Q = np.zeros([env.observation_space.n, env.action_space.n]) dis = 0.99 num_episodes = 2000 rList = [] . for i in range(num_episodes) : state = env.reset() rAll = 0 done = False while not done : action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i+1)) new_state, reward, done, _ = env.step(action) Q[state, action] = reward + dis * np.max(Q[new_state, :]) rAll += reward state = new_state rList.append(rAll) . for i in range(num_episodes) : state = env.reset() rAll = 0 done = False e = 1./((i / 100) + 1) while not done : if np.random.rand(1) &lt; e : action = env.action_space.sample() else : action = np.argmax(Q[state, :]) new_state, reward, done, _ = env.step(action) Q[state, action] = reward + dis * np.max(Q[new_state, :]) rAll += reward state = new_state rList.append(rAll) . print(&quot;Success rate : &quot;+str(sum(rList) / num_episodes)) print(&quot;Final Q-Table Values&quot;) print(Q) plt.bar(range(len(rList)), rList, color=&quot;blue&quot;) plt.show() . Success rate : 1.741 Final Q-Table Values [[0.94148015 0.95099005 0.93206535 0.94148015] [0.94148015 0. 0.92274469 0.93206535] [0.93206535 0. 0.91351725 0.92274469] [0.92274469 0. 0. 0.91351725] [0.95099005 0.96059601 0. 0.94148015] [0. 0. 0. 0. ] [0. 0.9801 0. 0.92274469] [0. 0. 0. 0. ] [0.96059601 0. 0.970299 0.95099005] [0.96059601 0.9801 0.9801 0. ] [0.970299 0.99 0. 0.970299 ] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0.9801 0.99 0.970299 ] [0.9801 0.99 1. 0.9801 ] [0. 0. 0. 0. ]] .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20learining/2022/03/26/nn-reinforcement-learning-q-network.html",
            "relUrl": "/reinforcement%20learining/2022/03/26/nn-reinforcement-learning-q-network.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[NeuralNetwork] Computational Unit - Rosenblatt's Perceptron",
            "content": "Content creators: HEESUNG YANG . Content reviewers: . 1. Overview . First model for supervised neural network, in 1957 | Single-layer single-output neural network for binary classification of linearly separable dataset | Model : | . $ text{For} , mathbf{x} = [+1, , x_1, , cdots, , x_m]^T , text{and} , mathbf{w} = [b, , w_1, , cdots, , w_m]^T $ . $$ hat{y} = sgn( mathbf{w}^T mathbf{x}) quad text{where} , {+1} , text{is positive, and} , text{-1} , text{is negative predicts.} $$ Learning : $$ mathbf{w}^* = underset{ mathbf{w}}{ arg min} sum_{ mathbf{x} in mathcal{H}} | mathbf{w}^T mathbf{x} | quad text{where} , mathcal{H} , text{is set of misclassified samples.} $$ | . Weights update : | . $$ Delta mathbf{w} = eta (y - hat{y}) mathbf{x} $$ . 2. Example . XOR Problem . import numpy as np # XOR truth table X = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]) # first column is +1 (bias multiplicated) yy = np.array([-1, 1, 1, -1]).T . %matplotlib inline import matplotlib.pyplot as plt def init_parameters(): ww = np.random.rand(3) # first column is bias return ww def model(ww, xx): return 1 if ww.T@xx &gt;= 0 else -1 def train(ww, X, y, learning_rate, start_epoch, num_epoch): num_data = X.shape[0] ax = plt.subplots() for epoch in range(num_epoch): # SGD context for i in range(num_data): y_pred = model(ww, X[i, :]) ww = ww + learning_rate*(y[i] - y_pred)*X[i, :] print(f&#39;[EPOCH {start_epoch + epoch}] weights &#39;, ww) return ww def visualize(ww, X, y, title): ax = plt.subplot() # set plot ax.set_title(title) ax.set_xlim((-0.5, 1.5)) ax.set_ylim((-0.5, 1.5)) ax.set_xticks([0, 1]) ax.set_yticks([0, 1]) # Scatter [ax.scatter(X[i, 1], X[i, 2], c=&#39;red&#39;) for i in range(X.shape[0]) if y[i] == 1] [ax.scatter(X[i, 1], X[i, 2], c=&#39;blue&#39;) for i in range(X.shape[0]) if y[i] == -1] # Dicision boundary # ww[0] + ww[1]*x + ww[2]*y = 0 xlins = np.linspace(-2, 2, 2) ylins = -ww[0]/ww[2] - ww[1]*xlins/ww[2] ax.plot(xlins, ylins, c=&#39;black&#39;) . Result . ww = init_parameters() visualize(ww, X, yy, &quot;Before training&quot;) . ww = train(ww, X, yy, 0.01, 0, 3) visualize(ww, X, yy, &quot;Epoch 3&quot;) . [EPOCH 0] weights [ 0.04599641 0.63590948 -0.01149363] [EPOCH 1] weights [ 0.00599641 0.61590948 -0.03149363] [EPOCH 2] weights [-0.01400359 0.59590948 -0.03149363] . ww = train(ww, X, yy, 0.01, 3, 7) visualize(ww, X, yy, &quot;Epoch 10&quot;) . [EPOCH 3] weights [-0.01400359 0.57590948 -0.03149363] [EPOCH 4] weights [-0.01400359 0.55590948 -0.03149363] [EPOCH 5] weights [-0.01400359 0.53590948 -0.03149363] [EPOCH 6] weights [-0.01400359 0.51590948 -0.03149363] [EPOCH 7] weights [-0.01400359 0.49590948 -0.03149363] [EPOCH 8] weights [-0.01400359 0.47590948 -0.03149363] [EPOCH 9] weights [-0.01400359 0.45590948 -0.03149363] . ww = train(ww, X, yy, 0.01, 10, 40) visualize(ww, X, yy, &quot;Epoch 50&quot;) . [EPOCH 10] weights [-0.01400359 0.43590948 -0.03149363] [EPOCH 11] weights [-0.01400359 0.41590948 -0.03149363] [EPOCH 12] weights [-0.01400359 0.39590948 -0.03149363] [EPOCH 13] weights [-0.01400359 0.37590948 -0.03149363] [EPOCH 14] weights [-0.01400359 0.35590948 -0.03149363] [EPOCH 15] weights [-0.01400359 0.33590948 -0.03149363] [EPOCH 16] weights [-0.01400359 0.31590948 -0.03149363] [EPOCH 17] weights [-0.01400359 0.29590948 -0.03149363] [EPOCH 18] weights [-0.01400359 0.27590948 -0.03149363] [EPOCH 19] weights [-0.01400359 0.25590948 -0.03149363] [EPOCH 20] weights [-0.01400359 0.23590948 -0.03149363] [EPOCH 21] weights [-0.01400359 0.21590948 -0.03149363] [EPOCH 22] weights [-0.01400359 0.19590948 -0.03149363] [EPOCH 23] weights [-0.01400359 0.17590948 -0.03149363] [EPOCH 24] weights [-0.01400359 0.15590948 -0.03149363] [EPOCH 25] weights [-0.01400359 0.13590948 -0.03149363] [EPOCH 26] weights [-0.01400359 0.11590948 -0.03149363] [EPOCH 27] weights [-0.01400359 0.09590948 -0.03149363] [EPOCH 28] weights [-0.01400359 0.07590948 -0.03149363] [EPOCH 29] weights [-0.01400359 0.05590948 -0.03149363] [EPOCH 30] weights [-0.01400359 0.03590948 -0.03149363] [EPOCH 31] weights [-0.01400359 0.01590948 -0.03149363] [EPOCH 32] weights [-0.01400359 -0.00409052 -0.03149363] [EPOCH 33] weights [ 0.00599641 -0.00409052 -0.01149363] [EPOCH 34] weights [-0.01400359 -0.02409052 -0.01149363] [EPOCH 35] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 36] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 37] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 38] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 39] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 40] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 41] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 42] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 43] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 44] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 45] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 46] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 47] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 48] weights [ 0.00599641 -0.02409052 -0.01149363] [EPOCH 49] weights [ 0.00599641 -0.02409052 -0.01149363] .",
            "url": "https://knu-ai-researcher.github.io/reports/computational%20unit/2022/03/26/nn-computational-unit-rosenblatt-perceptron.html",
            "relUrl": "/computational%20unit/2022/03/26/nn-computational-unit-rosenblatt-perceptron.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[NeuralNetwork] Computational Unit - Multi Layer Perceptron",
            "content": "Content creators: HEESUNG YANG . Content reviewers: . 1. Overview . First model for supervised neural network, in 1957 | Single-layer single-output neural network for binary classification of linearly separable dataset | Model : | . $ text{For} , mathbf{x} = [x_1, , cdots, , x_m]^T , text{and} , W_1 = begin{bmatrix} w_{1, 1}^{(1)} &amp; cdots &amp; w_{1, m}^{(1)} w_{2, 1}^{(1)} &amp; cdots &amp; w_{2, m}^{(1)} vdots &amp; ddots &amp; vdots w_{n, 1}^{(1)} &amp; cdots &amp; w_{n, m}^{(1)} end{bmatrix}, , W_2 = begin{bmatrix} w_{1, 1}^{(2)} &amp; cdots &amp; w_{1, n}^{(2)} w_{2, 1}^{(2)} &amp; cdots &amp; w_{2, n}^{(2)} vdots &amp; ddots &amp; vdots w_{o, 1}^{(2)} &amp; cdots &amp; w_{o, n}^{(2)} end{bmatrix}, , $ . $ mathbf{b}_1 = [b_1^{(1)}, , b_2^{(1)}, cdots , , b_n^{(1)}]^T, , mathbf{b}_2 = [b_1^{(2)}, , b_2^{(2)}, , , cdots , , b_o^{(2)}]^T $, . $$ hat{ mathbf{y}} = text{softmax}(W_2 sigma(W_1 mathbf{x} + mathbf{b}_1) + mathbf{b}_2). $$ Learning : Error back-propagation | . . 2. Example . Dataset . import torch import torchvision.datasets as dsets import torchvision.transforms as transforms import matplotlib.pyplot as plt import random device = &#39;cpu&#39; . training_epochs = 10 batch_size = 16 . mnist_train = dsets.MNIST(root=&#39;dataset/&#39;, train=True, transform=transforms.ToTensor(), download=True) mnist_test = dsets.MNIST(root=&#39;dataset/&#39;, train=False, transform=transforms.ToTensor(), download=True) . train_data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True) test_data_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=True) . import torch.nn as nn class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.flatten = nn.Flatten() self.fc1 = nn.Linear(28 * 28, 512) self.fc2 = nn.Linear(512, 10) self.softmax = nn.Softmax(dim=1) def forward(self, x): x = self.flatten(x) x = self.fc1(x) x = torch.sigmoid(x) x = self.fc2(x) x = self.softmax(x) return x def train(model, train_loader, optimizer): model.train() for batch_idx, (data, label) in enumerate(train_loader): data = data.to(device) label = label.to(device) optimizer.zero_grad() output = model(data) loss = loss_fn(output, label) loss.backward() optimizer.step() def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(device) label = label.to(device) output = model(image) test_loss += loss_fn(output, label).item() prediction = output.max(1, keepdim=True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= len(test_loader.dataset) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracy . model = MLP() loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model.parameters(), lr=0.03) . for Epoch in range(1, training_epochs + 1): train(model, train_data_loader, optimizer) test_loss, test_accuracy = evaluate(model, test_data_loader) print(&quot;[EPOCH: {}], tTest Loss: {:.4f}, tTest Accuracy: {:.2f} %&quot;.format( Epoch, test_loss, test_accuracy )) # 0.0362 loss -&gt; log(10) . [EPOCH: 1], Test Loss: 0.1266, Test Accuracy: 48.11 % [EPOCH: 2], Test Loss: 0.1172, Test Accuracy: 64.19 % [EPOCH: 3], Test Loss: 0.1099, Test Accuracy: 74.16 % [EPOCH: 4], Test Loss: 0.1080, Test Accuracy: 75.35 % [EPOCH: 5], Test Loss: 0.1073, Test Accuracy: 75.81 % [EPOCH: 6], Test Loss: 0.1069, Test Accuracy: 75.96 % [EPOCH: 7], Test Loss: 0.1067, Test Accuracy: 76.21 % [EPOCH: 8], Test Loss: 0.1066, Test Accuracy: 76.47 % [EPOCH: 9], Test Loss: 0.1064, Test Accuracy: 76.55 % [EPOCH: 10], Test Loss: 0.1063, Test Accuracy: 76.58 % .",
            "url": "https://knu-ai-researcher.github.io/reports/computational%20unit/2022/03/26/nn-computational-unit-multi-layer-perceptron.html",
            "relUrl": "/computational%20unit/2022/03/26/nn-computational-unit-multi-layer-perceptron.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[NeuralNetwork] Computational Unit - Hopfield Network",
            "content": "Content creators: HEESUNG YANG . Content reviewers: . 1. Overview . Associative memory model, in 1982 | The weights are symmetric | A parallel computer model that operates asynchronously | Model : | . Image(&#39;fig01.png&#39;, width=500) . Activation : $$ f(x) = begin{cases} {+1} quad text{if} , x &gt; 0 -1 quad text{if} , x &lt; 0 0 quad text{else} end{cases} $$ | . . 2. Example . import numpy as np def hebbian_weights(a): weights=np.zeros((input_pattern.shape[1],input_pattern.shape[1])) for i in a: weights=weights+np.outer(i,i) np.fill_diagonal(weights, 0) return weights def transfer_function(newy,theta): if newy&gt;theta: return 1 if newy==theta: return newy else: return 0 def activation_update(y,w,x,order): newy=x+np.sum((y*(w[:,order]))) return newy . order=[0,3,1,2] att=[] input_pattern=np.array([[1,-1,1,1],[1,1,1,-1]]) . x = np.array([[0,0,0,0], [0,0,0,1], [0,0,1,0], [0,0,1,1], [0,1,0,0], [0,1,0,1], [0,1,1,0], [0,1,1,1], [1,0,0,0], [1,0,0,1], [1,0,1,0], [1,0,1,1], [1,1,0,0], [1,1,0,1], [1,1,1,0], [1,1,1,1]]) w = hebbian_weights(input_pattern) . for test in x: y=np.copy(test) flag=0 count=0 while flag==0: for i in order: old=np.copy(y) newy=activation_update(y,w,y[i],i) y[i]=transfer_function(newy,0) if np.array_equal(old,y): count=count+1 if count==input_pattern.shape[1]: flag=1 att.append([test,y]) . w=hebbian_weights(input_pattern) w . array([[ 0., 0., 2., 0.], [ 0., 0., 0., -2.], [ 2., 0., 0., 0.], [ 0., -2., 0., 0.]]) . import pandas as pd df=pd.DataFrame(att,columns=[&#39;Test Patttern&#39;,&#39;Converged to&#39;]) df . Test Patttern Converged to . 0 [0, 0, 0, 0] | [0, 0, 0, 0] | . 1 [0, 0, 0, 1] | [0, 0, 0, 1] | . 2 [0, 0, 1, 0] | [1, 0, 1, 0] | . 3 [0, 0, 1, 1] | [1, 0, 1, 1] | . 4 [0, 1, 0, 0] | [0, 1, 0, 0] | . 5 [0, 1, 0, 1] | [0, 1, 0, 0] | . 6 [0, 1, 1, 0] | [1, 1, 1, 0] | . 7 [0, 1, 1, 1] | [1, 1, 1, 0] | . 8 [1, 0, 0, 0] | [1, 0, 1, 0] | . 9 [1, 0, 0, 1] | [1, 0, 1, 1] | . 10 [1, 0, 1, 0] | [1, 0, 1, 0] | . 11 [1, 0, 1, 1] | [1, 0, 1, 1] | . 12 [1, 1, 0, 0] | [1, 1, 1, 0] | . 13 [1, 1, 0, 1] | [1, 1, 1, 0] | . 14 [1, 1, 1, 0] | [1, 1, 1, 0] | . 15 [1, 1, 1, 1] | [1, 1, 1, 0] | .",
            "url": "https://knu-ai-researcher.github.io/reports/computational%20unit/2022/03/26/nn-computational-unit-hopfield-network.html",
            "relUrl": "/computational%20unit/2022/03/26/nn-computational-unit-hopfield-network.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[NeuralNetwork] Computational Unit - Boltzmann Machine",
            "content": "Content creators: HEESUNG YANG . Content reviewers: . 1. Overview . Associative memory model, in 1982 | The weights are symmetric | A parallel computer model that operates asynchronously | Although the operation rule of the hopfield network changes the state of the network only in the direction of decreasing energy, the Boltzmann machine uses an operation rule that allows even the transition of the state of increasing energy with a small probability. | Model : | . Image(&#39;fig01.png&#39;, width=500) . Activation : $$ f(x) = begin{cases} {+1} quad text{if} , x &gt; 0 -1 quad text{if} , x &lt; 0 0 quad text{else} end{cases} $$ | . . 2. Example . import numpy as np import torch import torch.utils.data import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.autograd import Variable from torchvision import datasets, transforms from torchvision.utils import make_grid , save_image %matplotlib inline import matplotlib.pyplot as plt . batch_size = 16 train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#39;dataset/&#39;, train=True, download = True, transform = transforms.Compose( [transforms.ToTensor()]) ), batch_size=batch_size ) test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#39;dataset/&#39;, train=False, transform=transforms.Compose( [transforms.ToTensor()]) ), batch_size=batch_size) . class RBM(nn.Module): def __init__(self, n_vis=784, n_hin=128, k=5): super(RBM, self).__init__() self.W = nn.Parameter(torch.randn(n_hin,n_vis)*1e-2) self.v_bias = nn.Parameter(torch.zeros(n_vis)) self.h_bias = nn.Parameter(torch.zeros(n_hin)) self.k = k def sample_from_p(self,p): return F.relu(torch.sign(p - Variable(torch.rand(p.size())))) def v_to_h(self,v): p_h = F.sigmoid(F.linear(v,self.W,self.h_bias)) sample_h = self.sample_from_p(p_h) return p_h,sample_h def h_to_v(self,h): p_v = F.sigmoid(F.linear(h,self.W.t(),self.v_bias)) sample_v = self.sample_from_p(p_v) return p_v,sample_v def forward(self,v): pre_h1,h1 = self.v_to_h(v) h_ = h1 for _ in range(self.k): pre_v_,v_ = self.h_to_v(h_) pre_h_,h_ = self.v_to_h(v_) return v,v_ def free_energy(self,v): vbias_term = v.mv(self.v_bias) wx_b = F.linear(v,self.W,self.h_bias) hidden_term = wx_b.exp().add(1).log().sum(1) return (-hidden_term - vbias_term).mean() . rbm = RBM(k=1) train_op = optim.SGD(rbm.parameters(),0.1) for epoch in range(10): loss_ = [] for _, (data,target) in enumerate(train_loader): data = Variable(data.view(-1, 784)) sample_data = data.bernoulli() v,v1 = rbm(sample_data) loss = rbm.free_energy(v) - rbm.free_energy(v1) loss_.append(loss.data) train_op.zero_grad() loss.backward() train_op.step() print(&quot;Training loss for {} epoch: {}&quot;.format(epoch, np.mean(loss_))) . /home/yang/.local/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&#34;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&#34;) . Training loss for 0 epoch: -6.216996669769287 Training loss for 1 epoch: -1.4275646209716797 Training loss for 2 epoch: 0.5669228434562683 Training loss for 3 epoch: 1.7080472707748413 Training loss for 4 epoch: 2.213977098464966 Training loss for 5 epoch: 2.5781733989715576 Training loss for 6 epoch: 2.691725015640259 Training loss for 7 epoch: 2.83552622795105 Training loss for 8 epoch: 2.977477550506592 Training loss for 9 epoch: 2.9726319313049316 . def show_adn_save(file_name,img): npimg = np.transpose(img.numpy(),(1,2,0)) f = &quot;./%s.png&quot; % file_name plt.imshow(npimg) plt.imsave(f,npimg) . show_adn_save(&quot;generate&quot;, make_grid(v1.view(16,1,28,28).data)) .",
            "url": "https://knu-ai-researcher.github.io/reports/computational%20unit/2022/03/26/nn-computational-unit-boltzmann-machine.html",
            "relUrl": "/computational%20unit/2022/03/26/nn-computational-unit-boltzmann-machine.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[MachineLearning] Ensemble Learning - Random Forest",
            "content": "Content creators: 이주형, 이중원 . Content reviewers: . 1. Overview . A specialized bagging for decision tree algorithms | Sampling w/ replacement + Selecting variables randomly | . Generalization Error . If the population size is large enough, then the generalization error of random forests bounded by $$ textrm{Generalization Error} le frac{ bar{p} left( 1-s^2 right)}{s^2}$$ where $ bar{p} $ is mean value of correlation coefficients between individual trees, | $ s^2 $ is margin function. | . | The more accurate the individual classifiers, the larger the $ s^2 $ | The less correlated among the classifiers, the lower the generalization error. | . Computing Variable importance . Compute the OOB error for the original dataset $ e_i $ | Compute the OOB error for the dataset in which the variable $ x_i $ is permuted $ p_i $. | Compute the variable importance based on the mean and stdev of $ d_i = {p_i}-{e_i} $ over all trees in the population. | The more important variable $ x_i $, the larger the $ d_i $. | . Random Forest : Example . %matplotlib inline from sklearn.metrics import r2_score from sklearn.datasets import fetch_california_housing import numpy as np from matplotlib import pyplot as plt from sklearn.ensemble import RandomForestRegressor . X, y = fetch_california_housing(data_home=&#39;./&#39;, return_X_y=True) . regr = RandomForestRegressor( oob_score=True, ).fit(X, y) regr.oob_score_ . 0.8118371967651296 .",
            "url": "https://knu-ai-researcher.github.io/reports/ensemble%20learning/2022/03/26/ml-ensemble-random-forest.html",
            "relUrl": "/ensemble%20learning/2022/03/26/ml-ensemble-random-forest.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[MachineLearning] Ensemble Learning - Overview",
            "content": "Content creators: 이주형, 이중원 . Content reviewers: . 1. Overview . No Free Lunch Theorem . There is no classification method to be superior or inferior overall. | Obtaining good generalization performance, there is no context-independent or usage-independent reasons to favor one algorithm over others. | If one algorithm seems to outperform another in a particular situation, it is a consequence of its fit to a particular pattern recognition problem. | In practice, experience with a broad range of techniques is the best insurance for solving arbitrary new classification problems. | . Bias and Variance . Bias : the amount by which the average estimator differs from the truth Low Bias : on average, we will accurately estimate the function from the dataset | High Bias : implies a poor match | . | Variance : spread of the individual estimations around their mean Low Variance : estimated function does not change much with different datasets | High Variance : implies a weak match | . | Bias and variance are not independent of each other | . Model Complexity . Lower model complexity : high bias &amp; low variance -&gt; Boosting Logistic regression, LDA, k-NN with large k | . | Higher model complexity : low bias &amp; high variance -&gt; Bagging DT, ANN, SVM, k-NN with small k | . | . Purpose of Ensemble . Goal : Reduce the error through constructing multiple learners to reduce bias and variance | To construct good ensemble systems, each base classifier component should achieve sufficient degree of diversity, and properly combine the outputs of individual classifiers. | . Reference .",
            "url": "https://knu-ai-researcher.github.io/reports/ensemble%20learning/2022/03/26/ml-ensemble-overview.html",
            "relUrl": "/ensemble%20learning/2022/03/26/ml-ensemble-overview.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[MachineLearning] Ensemble Learning - Bagging",
            "content": "Content creators: 이주형, 이중원 . Content reviewers: . 1. Overview . Bagging - Bootstrap Aggregating . Each member of the ensemble is constructed from a different training dataset. | Each dataset is generated by sampling from the total N data examples, choosing N items uniformly at random with replacement. | Probability that an instance is not included in a bootstrap(→Out of Bag, OOB): $p= left( 1- frac{1}{N} right)^N rightarrow lim_{N to infty} left( 1- frac{1}{N} right)^N = e^{-1} approx 0.368$ | . Result Aggregating for Classification task . Voting Strategy Majority Voting | Weighting Voting, and the weight could be validation acc. of individual models | predicted prob. for each class | can use both for weight | . | . | Stacking : Result aggregation Use another prediction model, i.e. meta-classifier, to aggregate the results | Source : Predictions made by ensemble members | Target : Actual true label | . | . Reference . Bagging : Examples . %matplotlib inline from sklearn.ensemble import BaggingRegressor from sklearn.datasets import fetch_california_housing from sklearn.neighbors import KNeighborsRegressor as NN from sklearn.metrics import r2_score import numpy as np from matplotlib import pyplot as plt . SEED = 42 X, y = fetch_california_housing(data_home=&#39;./&#39;, return_X_y=True) . clf = BaggingRegressor( # experiment group base_estimator=NN(), n_estimators=10, max_samples=0.3, oob_score=True, random_state=SEED ).fit(X, y) . clf.oob_score_ . 0.1475162707581028 . X_train, y_train = X[np.unique(clf.estimators_samples_),:], y[np.unique(clf.estimators_samples_)] X_test, y_test = np.delete(X, np.unique(clf.estimators_samples_), axis=0), np.delete(y, np.unique(clf.estimators_samples_)) . component_res = [] for estimator in clf.estimators_: component_pred = estimator.predict(X_test) component_res.append(r2_score(y_test, component_pred)) . component_res . [-0.03174907758299472, -0.021045219553941896, 0.003535488064889125, -0.03675586372593598, -0.005254866062958996, 0.02225931720479135, -0.00932398344070906, -0.02436691937209745, -0.005065799458619846, -0.02227930930171418] . nn_clf = NN().fit(X_train, y_train) pred = nn_clf.predict(X_test) nn_clf_score = r2_score(y_test, pred) . plt.bar([r&#39;$E$&#39;], clf.oob_score_, label=&#39;bagging(ensembled)&#39;) plt.bar([f&#39;c{i}&#39; for i in range(10)], component_res, label=&#39;bagging(component)&#39;) plt.bar([r&#39;$S$&#39;], nn_clf_score, label=&#39;single model&#39;) plt.legend() plt.grid(axis=&#39;y&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/ensemble%20learning/2022/03/26/ml-ensemble-bagging.html",
            "relUrl": "/ensemble%20learning/2022/03/26/ml-ensemble-bagging.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[MachineLearning] Reinforcement learning",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Definition . Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20learning/2022/03/22/ml-reinforcement-learning.html",
            "relUrl": "/reinforcement%20learning/2022/03/22/ml-reinforcement-learning.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[MachineLearning] Reinforcement learning - Genetic Algorithm",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . Genetic Algorithm&#51060;&#46976;? . &quot;유전자 알고리즘&quot;을 뜻함 | 자연계의 진화 연산을 컴퓨팅의 최적화 분야에 적용한 것 | . Step . 1. &#51665;&#45800; &#52488;&#44592;&#54868; . 문제를 정의하고, 문제를 염색체 형태로 표현한 후 N개의 집단으로 이루어진 초기 염색체 집단을 생성 . 2. &#51201;&#54633;&#46020; &#44228;&#49328; . 염색체의 적합도를 측정하는 적합도 함수를 정의하고 계산 . 3. &#51333;&#47308; &#51312;&#44148; &#54869;&#51064; . 도출된 적합도가 종료 조건을 만족하면 알고리즘을 종료하고, 만족하지 못하면 다음 단계로 넘어감 . 4. &#49440;&#53469; . 현재의 해당 집단에서 부모 염색체를 한 쌍 선택함. 단, 적합도가 높은 염색체가 선택될 확률이 높아야 함 . 5. &#44368;&#52264; . 부모 염색체의 일부를 교차시켜서 자식 염색체를 한 쌍 생성함 . 6. &#46028;&#50672;&#48320;&#51060; . 만들어진 자식 염색체의 일부를 랜덤하게 선택하여 변경함 부모 염색체와 동일한 수의 자식 염색체가 생성되었으면 이것으로 모두 부모 염색체를 교체하고 다시 2번으로 돌아감 . . &#50672;&#49328;&#51088; . &#49440;&#53469; &#50672;&#49328;&#51088; (select) . 선택 연산자란 좋은 적합도 점수를 가진 염색체에게 우선 순위를 부여하고 좋은 유전자를 다음 세대에 전달할 수 있도록 하는 연산자 보통 룰렛 휠 선택 알고리즘(roulette wheel selection)이 많이 쓰이며 염색체 후보들이 차지하는 룰렛의 비율이 적합도 함수 값에 비례하도록 한 알고리즘임 . &#44368;&#52264; &#50672;&#49328;&#51088; (crossover) . 교차 연산자는 염색체 간의 교배를 나타내며, 선택 사용자를 사용하여 두 개의 염색체를 선택하고 교차 위치를 임의로 선택함. 그 후 교차 지점을 중심으로 유전자를 서로 교환하여 새로운 자식을 생성 . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328;&#51088; (mutate) . Local minimum을 피하고 개체군의 다양성을 유지하기 위한 연산자로써, 자손에 무작위 유전자를 삽입(or 변경)하는 연산자임. . Pseudo Code . Genetic Algorithm(population, FitnessFunc) { repeat new_population = [] for i = 1 to size(population) do father = select(population, FitnessFunc) mother = select(population, FitnessFunc) child = crossover(father, mother) if (난수 &lt; 변이 확률) then child = mutate(child) new_population.append(child) population = new_population until 종료 조건 만족할 때 까지 return 가장 적합한 개체 } . . 2.Example . 고전적인 예제인 0 ~ 31 범위 안에서 x^2의 값을 최대화 하는 x 값을 유전자 알고리즘을 이용해 찾아내보자 | . import random POPULATION_SIZE = 4 MUTATION_RATE = 0.1 SIZE = 5 . &#50684;&#49353;&#52404; &#53364;&#47000;&#49828; &#44396;&#54788; . class Chromosome: def __init__(self, g=[]): self.genes = g.copy() # 유전자는 리스트로 구현 self.fitness = 0 # 적합도 if self.genes.__len__()==0: # 염색체가 초기 상태이면 초기화 i = 0 while i&lt;SIZE: if random.random() &gt;= 0.5: self.genes.append(1) else: self.genes.append(0) i += 1 def cal_fitness(self): # 적합도를 계산 self.fitness = 0 value = 0 for i in range(SIZE): value += self.genes[i] * pow(2, SIZE-1-i) self.fitness = value return self.fitness def __str__(self): return self.genes.__str__() . &#50684;&#49353;&#52404;&#50752; &#51201;&#54633;&#46020; &#52636;&#47141; &#54632;&#49688; . def print_p(pop): i = 0 for x in pop: print(&quot;염색체 #&quot;, i, &quot;=&quot;, x, &quot;적합도=&quot;, x.cal_fitness()) i += 1 print(&quot;&quot;) . &#49440;&#53469; &#50672;&#49328; . def select(pop): max_value = sum([c.cal_fitness() for c in population]) pick = random.uniform(0, max_value) current = 0 for c in pop: current += c.cal_fitness() if current &gt; pick: return c . &#44368;&#52264; &#50672;&#49328; . def crossover(pop): father = select(pop) mother = select(pop) index = random.randint(1, SIZE - 2) child1 = father.genes[:index] + mother.genes[index:] child2 = mother.genes[:index] + father.genes[index:] return (child1, child2) . &#46028;&#50672;&#48320;&#51060; &#50672;&#49328; . def mutate(c): for i in range(SIZE): if random.random() &lt; MUTATION_RATE: if random.random() &lt; 0.5: c.genes[i] = 1 else: c.genes[i] = 0 . &#47700;&#51064; &#54532;&#47196;&#44536;&#47016; . population = [] i=0 # 초기 염색체를 생성하여 객체 집단에 추가 while i&lt;POPULATION_SIZE: population.append(Chromosome()) i += 1 count=0 population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;세대 번호=&quot;, count) print_p(population) count=1 while population[0].cal_fitness() &lt; 31: new_pop = [] # 선택과 교차 연산 for _ in range(POPULATION_SIZE//2): c1, c2 = crossover(population); new_pop.append(Chromosome(c1)); new_pop.append(Chromosome(c2)); # 자식 세대가 부모 세대를 대체 # 깊은 복사를 수행 population = new_pop.copy(); # 돌연변이 연산 for c in population: mutate(c) # 출력을 위한 정렬 population.sort(key=lambda x: x.cal_fitness(), reverse=True) print(&quot;세대 번호=&quot;, count) print_p(population) count += 1 if count &gt; 100 : break; . 세대 번호= 0 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [0, 1, 1, 0, 1] 적합도= 13 염색체 # 2 = [0, 0, 1, 0, 0] 적합도= 4 염색체 # 3 = [0, 0, 0, 1, 1] 적합도= 3 세대 번호= 1 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 0, 1] 적합도= 17 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 2 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 0, 1] 적합도= 17 염색체 # 3 = [0, 0, 0, 0, 1] 적합도= 1 세대 번호= 3 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 1] 적합도= 17 세대 번호= 4 염색체 # 0 = [1, 1, 1, 0, 1] 적합도= 29 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 5 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 6 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 7 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 8 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 9 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 10 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 11 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 12 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 13 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 14 염색체 # 0 = [1, 1, 0, 1, 1] 적합도= 27 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 15 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 3 = [0, 1, 0, 1, 0] 적합도= 10 세대 번호= 16 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 17 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 1, 1, 0] 적합도= 22 세대 번호= 18 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 19 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 20 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 21 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 0, 1, 0, 0] 적합도= 20 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 22 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 1, 0, 1] 적합도= 21 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [0, 0, 0, 1, 0] 적합도= 2 세대 번호= 23 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 1, 0, 1] 적합도= 21 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 1, 0] 적합도= 18 세대 번호= 24 염색체 # 0 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 25 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 2 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 26 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 2 = [1, 0, 1, 1, 0] 적합도= 22 염색체 # 3 = [1, 0, 0, 1, 1] 적합도= 19 세대 번호= 27 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [1, 0, 0, 0, 0] 적합도= 16 세대 번호= 28 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 2 = [1, 0, 0, 0, 0] 적합도= 16 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 29 염색체 # 0 = [1, 0, 0, 1, 0] 적합도= 18 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 0, 0] 적합도= 8 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 30 염색체 # 0 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 31 염색체 # 0 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 1 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 2 = [0, 1, 0, 1, 0] 적합도= 10 염색체 # 3 = [0, 1, 0, 0, 0] 적합도= 8 세대 번호= 32 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 0, 1, 0] 적합도= 26 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [0, 1, 0, 1, 0] 적합도= 10 세대 번호= 33 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [1, 1, 0, 1, 0] 적합도= 26 세대 번호= 34 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 0, 0, 0] 적합도= 24 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 35 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 36 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 3 = [1, 1, 1, 1, 0] 적합도= 30 세대 번호= 37 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 1, 1, 0, 0] 적합도= 12 세대 번호= 38 염색체 # 0 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 0] 적합도= 14 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 세대 번호= 39 염색체 # 0 = [1, 1, 1, 1, 1] 적합도= 31 염색체 # 1 = [1, 1, 1, 1, 0] 적합도= 30 염색체 # 2 = [0, 1, 1, 1, 1] 적합도= 15 염색체 # 3 = [0, 1, 1, 1, 0] 적합도= 14 .",
            "url": "https://knu-ai-researcher.github.io/reports/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "relUrl": "/reinforcement%20learining/2022/03/22/ml-reinforcement-learning-genetic-algorithm.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "[MachineLearning] Regression Analysis",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Definition . Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the &#39;outcome&#39; or &#39;response&#39; variable) and one or more independent variables (often called &#39;predictors&#39;, &#39;covariates&#39;, &#39;explanatory variables&#39; or &#39;features&#39;). . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "[MachineLearning] Regression Analysis - Ridge/Lasso Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Ridge &amp; Lasso Regression . Ridge, Lasso Regression은 다중공선성 문제와 over-fitting 과적합 문제 방지를 위해 정규화 방식이 적용 . | Ridge Regression은 L2 정규화 . | Lasso Regression은 L1 정규화 | . &#45796;&#51473;&#44277;&#49440;&#49457;(Multicollinearity) . 두개 이상의 에측 변수 x들간의 강한 상관관계가 나타나, 독립변수들이 독립적이지 않는 문제가 발생하게 되는 현상을 말함. . | 이 경우 coefficient 추정치가 부정확해지고 standard error 값이 높아지게 된다 . | . Ridge Regression . Ridge 릿지 회귀는 선형 회귀모델의 Cost Function에 페널티를 적용한 것 . | 여기서 페널티는 Lambda * 계수 coefficient 제곱의 합이다 . | 이때 Lambda 값이 0에 가까워지면 Ridge는 본래 선형 회귀모델의 Cost Function에 가까워지게 된다 . | 반면에 Lambda의 값이 어느 정도 크다면, coefficient의 크기가 줄어서(0에 가까워져서) 모델의 복잡도가 줄어들고 multicollinearity 문제의 영향을 줄어든다. . | 왜냐면 서로 영향을 미치는 독립변수들의 weight가 줄어드는 것이기 때문이다. | . Lasso Regression . Ridge Regression은 Linear Regression 선형 회귀모델의 Cost Function 비용함수에 페널티를 적용한 것이다 . | Ridge와 수식은 비슷하지만 한 가지 차이점은, 페널티의 계산이 Lambda coefficient 제곱합이 아니라 Lambda coefficient 절대값의 합이라는 것이다. . | 이 계산은 L1 정규화 방식이고, zero coefficient 를 만드는 것이 가능해진다 . | 즉, 어떤 독립변수의 경우 아예 사라지게 될 수도 있게 되면서 feature selection, 상대적으로 더 중요한 독립변수를 선택할 수 있다 . | Lasso는 특정 독립변수의 coefficient 값을 0으로 만들 수 있고 Ridge는 0에 가까워지게 하지만 Lasso처럼 0으로 만들 수는 없다는 차이점이 있다 . Reference:https://nurilee.com/2020/01/26/data-science-model-summary-linear-ridge-lasso-elasticnet/&gt; https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/ . | . 2. Example . import matplotlib.pyplot as plt import numpy as np import pandas as pd import matplotlib matplotlib.rcParams.update({&#39;font.size&#39;: 12}) from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge . &#45936;&#51060;&#53552; &#51456;&#48708; . boston=load_boston() boston_df=pd.DataFrame(boston.data,columns=boston.feature_names) # add another column that contains the house prices which in scikit learn datasets are considered as target boston_df[&#39;Price&#39;]=boston.target newX=boston_df.drop(&#39;Price&#39;,axis=1) newX.head() newY=boston_df[&#39;Price&#39;] #print type(newY)# pandas core frame X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3) . Ridge Regression . lr = LinearRegression() lr.fit(X_train, y_train) rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization, # in this case linear and ridge regression resembles rr.fit(X_train, y_train) rr100 = Ridge(alpha=100) # comparison with alpha value rr100.fit(X_train, y_train) . Ridge(alpha=100) . train_score=lr.score(X_train, y_train) test_score=lr.score(X_test, y_test) Ridge_train_score = rr.score(X_train,y_train) Ridge_test_score = rr.score(X_test, y_test) Ridge_train_score100 = rr100.score(X_train,y_train) Ridge_test_score100 = rr100.score(X_test, y_test) . plt.plot(rr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Ridge; $ alpha = 0.01$&#39;,zorder=7) plt.plot(rr100.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Ridge; $ alpha = 100$&#39;) plt.plot(lr.coef_,alpha=0.4,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=7,color=&#39;green&#39;,label=&#39;Linear Regression&#39;) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.show() . Lasso Regression . from sklearn.linear_model import Lasso lasso = Lasso() lasso.fit(X_train,y_train) train_score=lasso.score(X_train,y_train) test_score=lasso.score(X_test,y_test) coeff_used = np.sum(lasso.coef_!=0) print(&quot;training score:&quot;, train_score) print(&quot;test score: &quot;, test_score) print(&quot;number of features used: &quot;, coeff_used) . training score: 0.6832133784853487 test score: 0.6364462662362061 number of features used: 11 . lasso001 = Lasso(alpha=0.01, max_iter=10e5) lasso001.fit(X_train,y_train) train_score001=lasso001.score(X_train,y_train) test_score001=lasso001.score(X_test,y_test) coeff_used001 = np.sum(lasso001.coef_!=0) print(&quot;training score for alpha=0.01:&quot;, train_score001) print(&quot;test score for alpha =0.01: &quot;, test_score001) print(&quot;number of features used: for alpha =0.01:&quot;, coeff_used001) . training score for alpha=0.01: 0.7414845253242521 test score for alpha =0.01: 0.7096270988778384 number of features used: for alpha =0.01: 13 . lasso00001 = Lasso(alpha=0.0001, max_iter=10e5) lasso00001.fit(X_train,y_train) train_score00001=lasso00001.score(X_train,y_train) test_score00001=lasso00001.score(X_test,y_test) coeff_used00001 = np.sum(lasso00001.coef_!=0) print(&quot;training score for alpha=0.0001:&quot;, train_score00001) print(&quot;test score for alpha =0.0001: &quot;, test_score00001) print(&quot;number of features used: for alpha =0.0001:&quot;, coeff_used00001) . training score for alpha=0.0001: 0.7419034541315459 test score for alpha =0.0001: 0.7147428283500775 number of features used: for alpha =0.0001: 13 . lr = LinearRegression() lr.fit(X_train,y_train) lr_train_score=lr.score(X_train,y_train) lr_test_score=lr.score(X_test,y_test) print(&quot;LR training score:&quot;, lr_train_score) print(&quot;LR test score: &quot;, lr_test_score) . LR training score: 0.7419034960343789 LR test score: 0.7147895265576851 . plt.plot(lasso.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;*&#39;,markersize=5,color=&#39;red&#39;,label=r&#39;Lasso; $ alpha = 1$&#39;,zorder=7) # alpha here is for transparency plt.plot(lasso001.coef_,alpha=0.5,linestyle=&#39;none&#39;,marker=&#39;d&#39;,markersize=6,color=&#39;blue&#39;,label=r&#39;Lasso; $ alpha = 0.01$&#39;) # alpha here is for transparency plt.plot(lasso00001.coef_,alpha=0.8,linestyle=&#39;none&#39;,marker=&#39;v&#39;,markersize=6,color=&#39;black&#39;,label=r&#39;Lasso; $ alpha = 0.00001$&#39;) # alpha here is for transparency plt.plot(lr.coef_,alpha=0.7,linestyle=&#39;none&#39;,marker=&#39;o&#39;,markersize=5,color=&#39;green&#39;,label=&#39;Linear Regression&#39;,zorder=2) plt.xlabel(&#39;Coefficient Index&#39;,fontsize=16) plt.ylabel(&#39;Coefficient Magnitude&#39;,fontsize=16) plt.legend(fontsize=13,loc=4) plt.tight_layout() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-ridge_lasso-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "[MachineLearning] Regression Analysis - Polynomial Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Polynomial Regression&#46976;? . 데이터의 분포가 선형이 아닌 곡선으로 나타나는 경우에 사용하는 회귀 . | 1차항이 아닌 2차, 3차항 등으로 확장하여 구성된 다항 회귀식 . | $ bar y = b_0 + {b_1}{x_i} + {b_2}{x_i^2} cdot cdot cdot b_p x_i^p$ . | . &#50508;&#44256;&#47532;&#51608; . 입력 데이터셋을 X라 할 때, . | 입력 데이터셋에 새로운 변수로 추가하고, 이 확장된 변수를 포함한 데이터셋을 선형모델로 훈련시키는 방법 . | 즉, X 를 통해 X^2, X^3 을 생성한 뒤, y = 1 + aX + bX^2 + cX^3 형식을 구성하고 . | a, b, c 를 선형 회귀 모델을 통해 학습하는 방법입니다. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np m = 100 X_train = 6 * np.random.rand(m, 1) - 3 y_train = 0.7 * X_train **2 + X_train + 2 + np.random.randn(m, 1) plt.scatter(X, y) plt.show() . &#54617;&#49845; . poly = PolynomialFeatures(degree=2, include_bias=True) X_train_poly = poly.fit_transform(X_train) . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X_train_poly, y_train) . LinearRegression() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#49373;&#49457; . m = 20 X_test = 6 * np.random.rand(m, 1) - 3 y_test = 0.7 * X_test **2 + X_test + 2 + np.random.randn(m, 1) . &#48320;&#54872; . X_test_poly = poly.transform(X_test) . &#50696;&#52769; . y_pred = lin_reg.predict(X_test_poly) . lin_reg.score(X_test_poly, y_test) . 0.7699902117540998 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-polynomial-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "[MachineLearning] Regression Analysis - Logistic Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Logistic Regression&#46976;? . Regression기법을 분류 문제로 확장한 것 . | 이항 형태의 범주형 데이터인 경우, 사용가능 ex) 양성/음성, 합격/불합격 . | 입력 값이 각 클래스에 속하는 확률값을 회귀분석으로 예측 . | . Odds . 임의의 사건 X가 발생하지 않을 확률 대비 일어날 확률의 비율 . | $Odds = {P(y=1|x) over {1-P(y=1|x)}} = exp(mx + b)$ . | . Logistic Function (Sigmoid Function) . 𝑥 ∈ (−∞,∞)를 −1,1 범위로 매핑하는 S자형 함수 | $f(x) = {1 over {1 + exp(-x)}}$ = $exp(x) over {1 + exp(x)}$ | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=30) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . classifier = LogisticRegression(random_state=0) classifier.fit(X_train, y_train) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . LogisticRegression(random_state=0) . &#50696;&#52769; . y_pred = classifier.predict(X_valid) y_pred . array([[6.16121922e-07, 2.15069658e-02, 9.78492418e-01], [2.24813541e-04, 1.44106469e-01, 8.55668717e-01], [9.86872166e-01, 1.31278254e-02, 8.45196341e-09], [7.42905537e-02, 9.15190175e-01, 1.05192710e-02], [6.17130899e-02, 9.34883255e-01, 3.40365509e-03], [9.85451789e-01, 1.45481989e-02, 1.22669080e-08], [9.83349822e-01, 1.66501582e-02, 2.00523225e-08], [9.74561843e-01, 2.54381352e-02, 2.13873436e-08], [1.05323138e-06, 2.92859163e-02, 9.70713030e-01], [4.60507545e-03, 8.28495273e-01, 1.66899652e-01], [2.90757993e-02, 9.57199786e-01, 1.37244147e-02], [5.07575743e-05, 5.38960827e-02, 9.46053160e-01], [2.38861710e-02, 9.59412646e-01, 1.67011834e-02], [3.74400885e-06, 1.20910117e-02, 9.87905244e-01], [9.76403956e-01, 2.35960160e-02, 2.74985641e-08], [9.76299518e-01, 2.37004423e-02, 3.95390327e-08], [9.52336822e-01, 4.76629417e-02, 2.36333757e-07], [5.73850109e-04, 4.81120782e-01, 5.18305368e-01], [9.81557938e-01, 1.84420425e-02, 1.96011616e-08], [1.01003329e-02, 7.51043736e-01, 2.38855931e-01], [1.67803358e-05, 1.42899971e-01, 8.57083248e-01], [9.73880934e-01, 2.61190258e-02, 3.98647797e-08], [8.45573758e-03, 9.35096103e-01, 5.64481595e-02], [6.91704414e-03, 8.60176111e-01, 1.32906844e-01], [3.84852525e-04, 4.50294871e-01, 5.49320276e-01], [1.11451716e-03, 8.01459697e-01, 1.97425786e-01], [6.74370685e-05, 4.30887311e-02, 9.56843832e-01], [6.18950666e-05, 1.89381057e-01, 8.10557048e-01], [9.78683149e-01, 2.13168316e-02, 1.91861458e-08], [4.82177753e-06, 5.19363140e-02, 9.48058864e-01]]) . classifier.score(X_valid, y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-logistic-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "[MachineLearning] Regression Analysis - Linear Regression",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Linear Regression&#46976;? . 가지고 있는 데이터를 가장 잘 설명할 수 있는 선(y = ax + b)을 찾는 방법을 선형 회귀(Linear Regression)이라 부른다. . | 예를 들어 키와 몸무게 데이터를 가장 잘 설명할 수 있는 선을 하나 잘 찾는다면, 새로운 사람의 키 정보만을 가지고 몸무게를 예측하는 것이 가능하다 . | . &#45936;&#51060;&#53552;&#47484; &#44032;&#51109; &#51096; &#49444;&#47749;&#54616;&#45716; &#49440;&#51060;&#46976; &#47924;&#50631;&#51064;&#44032;? . 우리가 예측한 선(ax + b)과 실제 가지고 있는 데이터 사이에는 차이가 존재한다. 우리는 이 차이를 오차라고 부른다. . | 우리가 찾고자 하는 데이터를 잘 설명하는 선이라는 것은 이 오차를 최소화화는 선을 찾는 것이다. . | 오차는 Mean Square Error 또는 Mean Absolute Error 등 다양하게 오차를 정의하여 사용할 수 있다. . | . Mean Square Error . 일반적으로 오차는 양수, 음수 모두로 표현이 되므로, 우리는 이 모든 오차를 양수, 음수 관계없이 동일하게 반영하도록 이 오차를 제곱하여 사용한다. . | 우리는 이 오차를 제곱하여 평균을 해준 것을 Mean Square Error(평균 제곱 오차)라고 부른다. . | $MSE = {1 over N} sum_{i=1}^N ({y - bar{y}})^2$ . | . &#50612;&#46523;&#44172; &#50724;&#52264;&#47484; &#51460;&#51068; &#44163;&#51064;&#44032;? . Gradient Descent(경사하강법)을 이용한다. | https://m.blog.naver.com/jevida/221855713144 | . &#54620;&#44228; . x와 y가 선형 관계가 아닌 경우, 에측이 잘 되지 않음 | 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . import numpy as np import matplotlib.pyplot as plt m = 100 X = 6 * np.random.rand(m,1) - 3 y = 3.331 * X + 23 + 4 * np.random.randn(m,1) # 약간의 노이즈 포함 X_train, X_test, y_train, y_test = X[20:], X[:20], y[20:], y[:20] plt.plot(X,y,&quot;b.&quot;) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#54617;&#49845; . import matplotlib.pyplot as plt import numpy as np from sklearn import linear_model from sklearn.metrics import mean_squared_error, r2_score # Create linear regression object regr = linear_model.LinearRegression() # Train the model using the training sets regr.fit(X_train, y_train) # Make predictions using the testing set y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, y_pred)) . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 . &#54617;&#49845;&#45936;&#51060;&#53552;&#50640; &#45824;&#54644;&#49436; &#50696;&#52769;&#54620; &#49440; &#54869;&#51064; . 오차가 최소한인 선이 구해진 것을 볼 수 있다. . diabetes_y_pred = regr.predict(X_train) # Plot outputs plt.scatter(X_train, y_train, color=&quot;r&quot;) plt.plot(X_train, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . diabetes_y_pred = regr.predict(X_test) # The coefficients print(&quot;Coefficients: n&quot;, regr.coef_) # The mean squared error print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, diabetes_y_pred)) # The coefficient of determination: 1 is perfect prediction print(&quot;Coefficient of determination: %.2f&quot; % r2_score(y_test, diabetes_y_pred)) # Plot outputs plt.scatter(X_test, y_test, color=&quot;r&quot;) plt.plot(X_test, diabetes_y_pred, color=&quot;b&quot;, linewidth=3) plt.ylabel(&quot;Y&quot;, fontsize=15,rotation=0) plt.xlabel(&quot;X&quot;, fontsize=15) plt.show() . Coefficients: [[3.55590157]] Mean squared error: 14.31 Coefficient of determination: 0.58 .",
            "url": "https://knu-ai-researcher.github.io/reports/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "relUrl": "/regression%20analysis/2022/03/22/ml-regression-linear-regression.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "[MachineLearning] Dimensionality Reduction",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Definition . Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "[MachineLearning] Dimensionality Reduction - t-SNE",
            "content": "Content creators: 조동현 . Content reviewers: . 1.Overview . t-SNE&#46976;? . &quot;t - distributed stochastic neighbor embedding&quot; 의 약자로 &quot;t 분포 확률적 임베딩&quot;을 뜻함 | . t-SNE &#51032;&#51032; . 매니폴드 학습의 하나로 복잡한 데이터의 시각화가 목적, 높은 차원의 데이터를 2차원 또는 3차원으로 축소시켜 시각화를 함 | 높은 차원 공간에서 비슷한 데이터 구조는 낮은 차언 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 대응함 | . 2. Example . from sklearn.datasets import load_iris from sklearn.manifold import TSNE import matplotlib.pyplot as plt import pandas as pd import numpy as np . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() df = pd.DataFrame(data = iris.data, columns = iris.feature_names) df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . 2&#52264;&#50896; t-SNE &#51076;&#48288;&#46377; . tsne_np = TSNE(n_components=2).fit_transform(df) tsne_df = pd.DataFrame(tsne_np, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) tsne_df[&#39;target&#39;] = iris.target tsne_df . cp1 cp2 target . 0 -24.555080 | 7.379160 | 0 | . 1 -21.977486 | 6.074713 | 0 | . 2 -22.032751 | 7.286000 | 0 | . 3 -21.595722 | 6.841762 | 0 | . 4 -24.597988 | 7.200622 | 0 | . ... ... | ... | ... | . 145 17.367140 | -5.027658 | 2 | . 146 14.227304 | -5.973306 | 2 | . 147 16.485725 | -5.267701 | 2 | . 148 18.021391 | -4.299052 | 2 | . 149 12.856414 | -6.667024 | 2 | . 150 rows × 3 columns . plt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=iris.target) plt.xlabel(&quot;cp1&quot;) plt.ylabel(&quot;cp2&quot;) . Text(0, 0.5, &#39;cp2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-t-SNE.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "[MachineLearning] Dimensionality Reduction - Singular Value Decomposition(SVD)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . SVD&#46976;? . Singular Value Decompositiond 의 약자로 &quot;특이값 분해&quot;를 뜻함 . | 수식 . | . $A=U&#931;V^T$ . $A$ : m x n 행렬$U$ : m x m 직교행렬$Σ$ : m x n 대각행렬$V$ : n x n 직교행렬SVD &#51032;&#51032; . 직교하는 벡터 집합에서, 선형 변환 후에 크기는 변하지만 여전히 직교할 수 있게 되는 직교 집합을 구하는 것, 그리고 선형 변환 후 결과를 구하는 것 | . SVD &#54596;&#50836;&#49457; . EVD(고유값 분해)의 경우 조건을 만족하는 정방행렬(n x n)에 대해서만 적용 가능함 | 하지만 행렬이 정방 행렬이든 아니든 관계 없이 모든 m x n 행렬에 대해 적용 가능하기 때문에 유용함 | . Truncated SVD . Σ의 대각 원소 중 상위 몇개만 추출하고 여기 대응하는 U와 V의 원소도 함께 줄이는 방법 | . . 2. Example . SVD &#50696;&#49884; . import numpy as np from numpy.linalg import svd . 4 by 4 &#47004;&#45924; &#54665;&#47148; &#49373;&#49457; . np.random.seed(10) a= np.random.randn(4, 4) a . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . SVD &#48516;&#54644; . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&quot;U matrix: n&quot;, np.round(U, 3)) print(&quot;Sigma Value: n&quot;, np.round(Sigma, 3)) print(&quot;V transpose: n&quot;, np.round(Vt, 3)) . (4, 4) (4,) (4, 4) U matrix: [[-0.909 0.292 -0.29 0.07 ] [-0.055 -0.568 -0.208 0.794] [ 0.241 -0.094 -0.921 -0.292] [ 0.336 0.763 -0.157 0.529]] Sigma Value: [2.284 1.659 1.255 0.146] V transpose: [[-0.687 -0.134 0.688 0.193] [-0.422 0.856 -0.282 0.098] [-0.293 -0.046 -0.033 -0.954] [ 0.514 0.498 0.668 -0.205]] . &#44160;&#51613; . Sigma_mt = np.diag(Sigma) verify_mt = np.dot(np.dot(U, Sigma_mt), Vt) verify_mt . array([[ 1.3315865 , 0.71527897, -1.54540029, -0.00838385], [ 0.62133597, -0.72008556, 0.26551159, 0.10854853], [ 0.00429143, -0.17460021, 0.43302619, 1.20303737], [-0.96506567, 1.02827408, 0.22863013, 0.44513761]]) . . Truncated SVD &#50696;&#49884; . from scipy.sparse.linalg import svds from scipy.linalg import svd . &#53945;&#51060;&#44050; 3&#44060;&#47196; Truncated SVD &#49688;&#54665; . U_tr, Sigma_tr, Vt_tr = svds(a, k=3) print(U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print(&quot;U_tr matrix: n&quot;, np.round(U_tr, 3)) print(&quot;Sigma_tr Value: n&quot;, np.round(Sigma_tr, 3)) print(&quot;V_tr transpose: n&quot;, np.round(Vt_tr, 3)) . (4, 3) (3,) (3, 4) U_tr matrix: [[ 0.29 0.292 0.909] [ 0.208 -0.568 0.055] [ 0.921 -0.094 -0.241] [ 0.157 0.763 -0.336]] Sigma_tr Value: [1.255 1.659 2.284] V_tr transpose: [[ 0.293 0.046 0.033 0.954] [-0.422 0.856 -0.282 0.098] [ 0.687 0.134 -0.688 -0.193]] . &#48373;&#50896; . Sigma_tr_mt = np.diag(Sigma_tr) restore = np.dot(np.dot(U_tr, Sigma_tr_mt), Vt_tr) restore . array([[ 1.32634718, 0.71020432, -1.55220761, -0.00629071], [ 0.56177478, -0.77777473, 0.18812528, 0.13234356], [ 0.02616807, -0.15341116, 0.46144994, 1.19429753], [-1.00471538, 0.98987057, 0.1771143 , 0.46097789]]) . . SVD &#54876;&#50857; . from sklearn.decomposition import TruncatedSVD from sklearn.datasets import load_iris import matplotlib.pyplot as plt import pandas as pd . &#45936;&#51060;&#53552; &#47196;&#46300; . iris = load_iris() iris_data = iris.data . Truncated SVD &#51652;&#54665; . tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_data) iris_tsvd = tsvd.transform(iris_data) result = pd.DataFrame(data = iris_tsvd, columns = [&quot;cp1&quot;, &quot;cp2&quot;]) . &#49884;&#44033;&#54868; . plt.scatter(x=result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&quot;TruncsatedSVD Component 1&quot;) plt.ylabel(&quot;TruncsatedSVD Component 1&quot;) . Text(0, 0.5, &#39;TruncsatedSVD Component 1&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-SVD.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "[MachineLearning] Dimensionality Reduction - Principal Component Analysis(PCA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . PCA&#46976;? . Principal Component Analysis 의 약자로 &quot;주성분 분석&quot;을 뜻함 | . PCA &#51032;&#51032; . n차원 데이터를 정사영 시켜 k 차원으로 낮출 때, (n &gt; k) 어떤 벡터에 데이터를 정사영 시켜야 원래 데이터의 구조을 제일 잘 유지시킬 수 있는지 알아내는 것 | . PCA &#54596;&#50836;&#49457; . 실제 데이터들은 매우 많은 feature를 가지고 있음(= 차원이 높음). 따라서 머신러닝을 적용하여 문제를 해결하는데 있어서 아래와 어려움이 있음 전체 데이터의 양이 많아 학습 속도가 느려짐 | 의미 없는 faeture들에 의해 과적합되거나 원활한 학습이 되지 않음 | | 그래서 차원축소를 통해 전체적인 데이터 양을 줄일 필요가 있음 | . 2. Example . import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.datasets import load_iris . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . iris = load_iris() x = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) x.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . &#45936;&#51060;&#53552; scale . x = StandardScaler().fit_transform(x) pd.DataFrame(data=x, columns=iris[&#39;feature_names&#39;]).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 -0.900681 | 1.019004 | -1.340227 | -1.315444 | . 1 -1.143017 | -0.131979 | -1.340227 | -1.315444 | . 2 -1.385353 | 0.328414 | -1.397064 | -1.315444 | . 3 -1.506521 | 0.098217 | -1.283389 | -1.315444 | . 4 -1.021849 | 1.249201 | -1.340227 | -1.315444 | . PCA &#49892;&#54665; . pca = PCA(n_components=2) result = pca.fit_transform(x) result_df = pd.DataFrame(data=result, columns = [&#39;pc1&#39;, &#39;pc2&#39;]) result_df[&#39;species&#39;] = iris[&#39;target&#39;] result_df[&#39;species&#39;] = result_df[&#39;species&#39;].map({0: &#39;setosa&#39;, 1: &quot;versicolor&quot;, 2: &quot;virginica&quot;}) result_df.head() . pc1 pc2 species . 0 -2.264703 | 0.480027 | setosa | . 1 -2.080961 | -0.674134 | setosa | . 2 -2.364229 | -0.341908 | setosa | . 3 -2.299384 | -0.597395 | setosa | . 4 -2.389842 | 0.646835 | setosa | . PCA &#44208;&#44284;&#44032; &#50896;&#48376; &#45936;&#51060;&#53552; &#48516;&#49328;&#51032; 96%&#47484; &#49444;&#47749;&#54620;&#45796;&#44256; &#48380; &#49688; &#51080;&#51020; . sum(pca.explained_variance_ratio_) . 0.9581320720000164 . &#49884;&#44033;&#54868; . figure = plt.figure(figsize = (8, 8)) axis = figure.add_subplot(1, 1, 1) axis.set_xlabel(&#39;Principal Component 1&#39;) axis.set_ylabel(&#39;Principal Component 2&#39;) axis.set_title(&#39;PCA result&#39;) targets = iris[&#39;target_names&#39;] colors = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;] for target, color in zip(targets, colors): indices = result_df[&#39;species&#39;] == target axis.scatter(result_df.loc[indices, &#39;pc1&#39;], result_df.loc[indices, &#39;pc2&#39;], c = color, s = 50) axis.legend(targets) axis.grid() .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-PCA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "[MachineLearning] Dimensionality Reduction - Linear Discriminant Analysis(LDA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . LDA&#46976;? . Linear Discriminant Analysis 의 약자로 &quot;선형 판별법&quot;을 뜻함 | . LDA &#51032;&#51032; . PCA와 매우 유사하게 dataset을 저차원 공간에 투영해 차원을 축소하는 기법이지만, 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소함 | 그렇기 때문에 Classification에 적용하기 전 데이터 전처리로 사용하기 적합함 | . LDA &#44396;&#54788; &#50896;&#47532; . 클래스간 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 차원 축소를 진행함 | . 2. Example . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; scale &#51652;&#54665; . iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) . LDA &#51652;&#54665; . lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) result = pd.DataFrame(data = iris_lda) . &#49884;&#44033;&#54868; . plt.scatter(x = result.iloc[:,0], y=result.iloc[:,1], c=iris.target) plt.xlabel(&#39;LDA component 1&#39;) plt.ylabel(&#39;LDA component 2&#39;) . Text(0, 0.5, &#39;LDA component 2&#39;) .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LDA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "[MachineLearning] Dimensionality Reduction - Latent Semantic Analysis(LCA)",
            "content": "Content creators: 조동현 . Content reviewers: . 1. Overview . LCA&#46976;? . Latent Semantic Analysis 의 약자로 &quot;잠재 의미 분석&quot;을 뜻함 | . LCA &#51032;&#51032; . 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘 | DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 떄문에 단어의 의미를 고려하지 못한다는 단점이 존재 이를 위한 대안으로 등장함 | . . 2. Example . import pandas as pd from sklearn.datasets import fetch_20newsgroups import nltk from nltk.corpus import stopwords from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD . &#45936;&#51060;&#53552; &#47196;&#46300; &#48143; &#54869;&#51064; . dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) documents = dataset.data print(&#39;샘플의 수 :&#39;,len(documents)) . 샘플의 수 : 11314 . documents[1] . &#34; n n n n n n nYeah, do you expect people to read the FAQ, etc. and actually accept hard natheism? No, you need a little leap of faith, Jimmy. Your logic runs out nof steam! n n n n n n n nJim, n nSorry I can&#39;t pity you, Jim. And I&#39;m sorry that you have these feelings of ndenial about the faith you need to get by. Oh well, just pretend that it will nall end happily ever after anyway. Maybe if you start a new newsgroup, nalt.atheist.hard, you won&#39;t be bummin&#39; so much? n n n n n n nBye-Bye, Big Jim. Don&#39;t forget your Flintstone&#39;s Chewables! :) n-- nBake Timmons, III&#34; . print(dataset.target_names) . [&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;] . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . news_df = pd.DataFrame({&#39;document&#39;:documents}) news_df[&#39;clean_doc&#39;] = news_df[&#39;document&#39;].str.replace(&quot;[^a-zA-Z]&quot;, &quot; &quot;, regex=True) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: &#39; &#39;.join([w for w in x.split() if len(w)&gt;3])) news_df[&#39;clean_doc&#39;] = news_df[&#39;clean_doc&#39;].apply(lambda x: x.lower()) . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons&#39; . &#48520;&#50857;&#50612; &#51228;&#44144; . stop_words = stopwords.words(&#39;english&#39;) tokenized_doc = news_df[&#39;clean_doc&#39;].apply(lambda x: x.split()) # 토큰화 tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) . print(tokenized_doc[1]) . [&#39;yeah&#39;, &#39;expect&#39;, &#39;people&#39;, &#39;read&#39;, &#39;actually&#39;, &#39;accept&#39;, &#39;hard&#39;, &#39;atheism&#39;, &#39;need&#39;, &#39;little&#39;, &#39;leap&#39;, &#39;faith&#39;, &#39;jimmy&#39;, &#39;logic&#39;, &#39;runs&#39;, &#39;steam&#39;, &#39;sorry&#39;, &#39;pity&#39;, &#39;sorry&#39;, &#39;feelings&#39;, &#39;denial&#39;, &#39;faith&#39;, &#39;need&#39;, &#39;well&#39;, &#39;pretend&#39;, &#39;happily&#39;, &#39;ever&#39;, &#39;anyway&#39;, &#39;maybe&#39;, &#39;start&#39;, &#39;newsgroup&#39;, &#39;atheist&#39;, &#39;hard&#39;, &#39;bummin&#39;, &#39;much&#39;, &#39;forget&#39;, &#39;flintstone&#39;, &#39;chewables&#39;, &#39;bake&#39;, &#39;timmons&#39;] . TF-IDF &#54665;&#47148; &#47564;&#46308;&#44592; . detokenized_doc = [] for i in range(len(news_df)): t = &#39; &#39;.join(tokenized_doc[i]) detokenized_doc.append(t) news_df[&#39;clean_doc&#39;] = detokenized_doc . news_df[&#39;clean_doc&#39;][1] . &#39;yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons&#39; . vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_features= 1000, max_df = 0.5, smooth_idf=True) X = vectorizer.fit_transform(news_df[&#39;clean_doc&#39;]) print(&#39;TF-IDF 행렬의 크기 :&#39;,X.shape) . TF-IDF 행렬의 크기 : (11314, 1000) . &#53664;&#54589; &#47784;&#45944;&#47553; . svd_model = TruncatedSVD(n_components=20, algorithm=&#39;randomized&#39;, n_iter=100, random_state=122) svd_model.fit(X) len(svd_model.components_) . 20 . terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨. def get_topics(components, feature_names, n=5): for idx, topic in enumerate(components): print(&quot;Topic %d:&quot; % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]]) get_topics(svd_model.components_,terms) . Topic 1: [(&#39;like&#39;, 0.21386), (&#39;know&#39;, 0.20046), (&#39;people&#39;, 0.19293), (&#39;think&#39;, 0.17805), (&#39;good&#39;, 0.15128)] Topic 2: [(&#39;thanks&#39;, 0.32888), (&#39;windows&#39;, 0.29088), (&#39;card&#39;, 0.18069), (&#39;drive&#39;, 0.17455), (&#39;mail&#39;, 0.15111)] Topic 3: [(&#39;game&#39;, 0.37064), (&#39;team&#39;, 0.32443), (&#39;year&#39;, 0.28154), (&#39;games&#39;, 0.2537), (&#39;season&#39;, 0.18419)] Topic 4: [(&#39;drive&#39;, 0.53324), (&#39;scsi&#39;, 0.20165), (&#39;hard&#39;, 0.15628), (&#39;disk&#39;, 0.15578), (&#39;card&#39;, 0.13994)] Topic 5: [(&#39;windows&#39;, 0.40399), (&#39;file&#39;, 0.25436), (&#39;window&#39;, 0.18044), (&#39;files&#39;, 0.16078), (&#39;program&#39;, 0.13894)] Topic 6: [(&#39;chip&#39;, 0.16114), (&#39;government&#39;, 0.16009), (&#39;mail&#39;, 0.15625), (&#39;space&#39;, 0.1507), (&#39;information&#39;, 0.13562)] Topic 7: [(&#39;like&#39;, 0.67086), (&#39;bike&#39;, 0.14236), (&#39;chip&#39;, 0.11169), (&#39;know&#39;, 0.11139), (&#39;sounds&#39;, 0.10371)] Topic 8: [(&#39;card&#39;, 0.46633), (&#39;video&#39;, 0.22137), (&#39;sale&#39;, 0.21266), (&#39;monitor&#39;, 0.15463), (&#39;offer&#39;, 0.14643)] Topic 9: [(&#39;know&#39;, 0.46047), (&#39;card&#39;, 0.33605), (&#39;chip&#39;, 0.17558), (&#39;government&#39;, 0.1522), (&#39;video&#39;, 0.14356)] Topic 10: [(&#39;good&#39;, 0.42756), (&#39;know&#39;, 0.23039), (&#39;time&#39;, 0.1882), (&#39;bike&#39;, 0.11406), (&#39;jesus&#39;, 0.09027)] Topic 11: [(&#39;think&#39;, 0.78469), (&#39;chip&#39;, 0.10899), (&#39;good&#39;, 0.10635), (&#39;thanks&#39;, 0.09123), (&#39;clipper&#39;, 0.07946)] Topic 12: [(&#39;thanks&#39;, 0.36824), (&#39;good&#39;, 0.22729), (&#39;right&#39;, 0.21559), (&#39;bike&#39;, 0.21037), (&#39;problem&#39;, 0.20894)] Topic 13: [(&#39;good&#39;, 0.36212), (&#39;people&#39;, 0.33985), (&#39;windows&#39;, 0.28385), (&#39;know&#39;, 0.26232), (&#39;file&#39;, 0.18422)] Topic 14: [(&#39;space&#39;, 0.39946), (&#39;think&#39;, 0.23258), (&#39;know&#39;, 0.18074), (&#39;nasa&#39;, 0.15174), (&#39;problem&#39;, 0.12957)] Topic 15: [(&#39;space&#39;, 0.31613), (&#39;good&#39;, 0.3094), (&#39;card&#39;, 0.22603), (&#39;people&#39;, 0.17476), (&#39;time&#39;, 0.14496)] Topic 16: [(&#39;people&#39;, 0.48156), (&#39;problem&#39;, 0.19961), (&#39;window&#39;, 0.15281), (&#39;time&#39;, 0.14664), (&#39;game&#39;, 0.12871)] Topic 17: [(&#39;time&#39;, 0.34465), (&#39;bike&#39;, 0.27303), (&#39;right&#39;, 0.25557), (&#39;windows&#39;, 0.1997), (&#39;file&#39;, 0.19118)] Topic 18: [(&#39;time&#39;, 0.5973), (&#39;problem&#39;, 0.15504), (&#39;file&#39;, 0.14956), (&#39;think&#39;, 0.12847), (&#39;israel&#39;, 0.10903)] Topic 19: [(&#39;file&#39;, 0.44163), (&#39;need&#39;, 0.26633), (&#39;card&#39;, 0.18388), (&#39;files&#39;, 0.17453), (&#39;right&#39;, 0.15448)] Topic 20: [(&#39;problem&#39;, 0.33006), (&#39;file&#39;, 0.27651), (&#39;thanks&#39;, 0.23578), (&#39;used&#39;, 0.19206), (&#39;space&#39;, 0.13185)] .",
            "url": "https://knu-ai-researcher.github.io/reports/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "relUrl": "/dimensionality%20reduction/2022/03/22/ml-dimensionality-reduction-LCA.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "[MachineLearning] Cluster Analysis",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Definition . Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters) . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "[MachineLearning] Cluster Analysis - Mean Shift",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Aims to discover blobs in a smooth density of samples | Works by updating candidates for centroids to be the mean of the points within a given region | Candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids | Parameters : Non-parametric(bandwidth) | . Algorithm . Determine initial centroids | While moved distance is smaller than threshold For each centroids, move points towards a region of the maximum increase in the density of points. | . | Eliminate near-duplicates to form the final set of centroids | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import MeanShift mean_shift = MeanShift(bandwidth=1.4).fit(X) pred = mean_shift.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-mean-shift.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "[MachineLearning] Cluster Analysis - K-Means",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Clusters data by trying to separate samples in n groups of equal variance | Highly dependent on the initialization of the centroids | Object : Minimize inertia(=within-cluster sum-of-squares) Inertia makes the assumption that clusters are convex and isotropic | Responds poorly to elongated clusters, or manifolds with irregular shapes | . | Parameters : number of clusters | . Algorithm . Choose n initial centroids | While the centroids do not move significantly assigns each sample to its nearest centroid | creates new centroids by taking the mean value of all of the samples assigned to each previous centroid | | Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=4, random_state=SEED).fit(X) pred = kmeans.predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], label=&quot;centers&quot;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-k-means.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "[MachineLearning] Cluster Analysis - Fuzzy C Means",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . A form of clustering in which each data point can belong to more than one cluster | In fuzzy clustering, data points can potentially belong to multiple clusters. | The fuzzy c-menas algorithm is very similar to the k-means algorithm. | Parameters : number of clusters | . Algorithm . Choose a number of clusters. | Assign coefficients randomly to each data point for being in the clusters. | While the coefficients&#39; change between iterations is no more than the given threshold: Compute the centroid for each cluster. | For each data point, compute its coefficients of being in the clusters. | | 2. Example . import numpy as np from sklearn.datasets import make_blobs from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . import skfuzzy as fuzz # scikit-fuzzy c_means = fuzz.cmeans(X.T, c=4, m=2, error=1e-3, maxiter=300) centers, coefs = c_means[0], c_means[1] pred = np.argmax(coefs, axis=0) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.scatter(centers[:, 0], centers[:, 1], label=&#39;centers&#39;, c=&#39;cyan&#39;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-fuzzy-c-means.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "[MachineLearning] Cluster Analysis - Agglomerative Clustering",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Performs a hierarchical clustering using a bottom up approach | Each observation starts in its own cluster, and clusters are successively merged together Ward minimizes the sum of squared differences within all clusters. | Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters. | Average linkage minimizes the average of the distances between all observations of pairs of clusters. | Single linkage minimizes the distance between the closest observations of pairs of clusters. | . | . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import AgglomerativeClustering clustering = AgglomerativeClustering() pred = clustering.fit_predict(X) for _ in np.unique(pred): ix = np.where(pred == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-agglomerative-clustering.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "[MachineLearning] Cluster Analysis - DBSCAN",
            "content": "Content creators: 이주형 . Content reviewers: . 1. Overview . Views clusters as areas of high density separated by areas of low density | clusters found by DBSCAN can be any shape | central component to the DBSCAN is the concept of core samples core samples are in areas of high density | . | Parameters : min_samples, eps . min_samples : controls how tolerant the algorithm is towards noise | eps : controls the local neighborhood of the points | . | Core sample : a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample . | Cluster : a set of core samples that can be built by recursively taking a core sample = finding all of its neighbors that are core samples | = finding all of their neighbors that are core samples | . | non-core samepls : neighbors of a core sample in the cluster but are not themselves core samples | Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm. | . Reference . 2. Example . import numpy as np from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt SEED = 42 X, y = make_blobs( n_samples=2000, n_features=2, centers=4, shuffle=False, random_state=SEED ) for _ in np.unique(y): ix = np.where(y == _) plt.scatter(X[ix, 0], X[ix, 1], label=f&quot;{_} (n={len(ix[0])})&quot;) plt.legend() plt.show() . from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) n_noise_ = list(labels).count(-1) unique_labels = set(labels) colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = [0, 0, 0, 1] class_member_mask = labels == k xy = X[class_member_mask &amp; core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=14, ) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot( xy[:, 0], xy[:, 1], &quot;o&quot;, markerfacecolor=tuple(col), markeredgecolor=&quot;k&quot;, markersize=6, ) .",
            "url": "https://knu-ai-researcher.github.io/reports/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "relUrl": "/clustering%20analysis/2022/03/22/ml-clustering-DBSCAN.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "[MachineLearning] Classification",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Definition . Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. Classification is the grouping of related facts into classes. . wikipedia .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post32": {
            "title": "[MachineLearning] Classification - Support Vector Machine",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . SVM(Support Vector Machine)&#46976;? . 결정 경계(Decision Boundary), 즉 분류를 위한 기준 선을 정의하는 모델 . | 새로운 데이터가 주어졌을 때, 어느쪽 결정경계에 포함하는지에 따라 분류 . | . &#51339;&#51008; &#44208;&#51221;&#44221;&#44228;&#46976;? . 데이터 군으로부터 멀리 떨어져있는 결정 경게 . | 서포트 벡터: 결정 경계와 가까이 있는 데이터들 . | Margin: 결정 경계와 서포트 벡터 사이의 거리 . | 최적의 결정경게는 Margin을 최대화 한다 . | n개의 속성을 가진 데이터에는 최소 n+1개의 서포트 벡터가 존재 . | . SVM &#51109;&#51216; . SVM에서 결정 경계는 서포트 벡터에 의해 정의되므로, 데이터 중에서 서포트 벡터만을 잘 선별하면 필요없는 데이터들을 무시할 수 있다. . | 이로인해 매우 빠르다는 장점이 있다. . | . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.svm import SVC classifier = SVC(kernel = &#39;linear&#39;) classifier.fit(X_train, y_train) . SVC(kernel=&#39;linear&#39;) . &#50696;&#52769; . classifier.predict(X_valid) . array([2, 1, 2, 1, 0, 1, 2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 0, 0, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 2, 0, 0]) . classifier.score(X_valid, y_valid) . 0.9555555555555556 . classifier.score(X_valid, y_valid) . 0.9555555555555556 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "relUrl": "/classification/2022/03/22/ml-classification-support-vector-machine.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post33": {
            "title": "[MachineLearning] Classification - Naive Bayes Classification",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . Naive Bayes&#46976;? . 베이즈 정리에 기반한 통계적 분류 기법 . | 공통적으로 모든 특성들이 서로 독립임을 가정 . | 복잡한 반복 매개변수 추정 없어 매우 큰 데이터셋에 유용함 . | 정확성이 높음 . | 스팸 메일 필터, 텍스트 분류, 감정 분석, 추천 시스템 등에 활용 . | . &#50508;&#44256;&#47532;&#51608; . 베이즈 정리는 P(c), P(x) 및 P(x|c)로부터 후방 확률 P(c|x)를 계산하는 방법을 제공합니다. . | Naigive Bayes 분류자는 주어진 클래스(c)에 대한 예측 변수(x) 값의 효과가 다른 예측 변수의 값과 독립적이라고 가정합니다. . | . P(c|x)는 주어진 예측 변수(속성)에서 클래스(목표값)의 후방 확률 . | P(c)는 클래스의 사전 확률 . | P(x|c)는 주어진 클래스의 확률인 우도 . | P(x)는 예측 변수의 사전 확률 . | $$ Bayes Rule: {P(c|x) = }{{P(x|c)P(c)} over P(x)} $$ . 2. Example . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y=True) X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.3, random_state=34) X_train.shape, y_train.shape, X_valid.shape, y_valid.shape . ((105, 4), (105,), (45, 4), (45,)) . &#54617;&#49845; . from sklearn.naive_bayes import GaussianNB #Create a Gaussian Classifier classifier = GaussianNB() # Train the model using the training sets classifier.fit(X_train, y_train) #Predict Output predicted= classifier.predict(X_valid) # 0:Overcast, 2:Mild print(&quot;Predicted Value:&quot;, predicted) # 1: Yes . Predicted Value: [2 1 1 1 0 1 2 0 2 2 2 0 1 2 0 1 0 0 1 2 2 0 1 1 2 1 1 1 1 1 0 0 1 2 2 0 2 1 0 2 0 0 2 0 0] . &#50696;&#52769; . classifier.score(X_valid, y_valid) . 0.9333333333333333 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "relUrl": "/classification/2022/03/22/ml-classification-naive-bayes-classification.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post34": {
            "title": "[MachineLearning] Classification - K-Nearest Neighbor",
            "content": "Content creators: 이중원 . Content reviewers: . 1. Overview . K-Nearest Neighbor&#46976;? . 비모수 밀도추정 방법이다. (확률분포 모델을 미리 가정하지 않고 데이터 집합을 이용) . | 모든 학습 데이터를 저장하여 예측에 사용한다. . | 새로운 데이터가 주어졌을 때, 이웃한 K개의 학습 데이터를 찾는다. . | 찾아진 이웃 데이터들이 많이 속한 클래스에 할당한다. . | . &#54617;&#49845; &#50508;&#44256;&#47532;&#51608; . 주어진 데이터 x와 모든 학습 데이터 {x1,x2, ..., xN} 과의 거리를 계 산한다. . | 거리가 가장 가까운 것부터 순서대로 K개의 데이터를 찾아 후보 집 합 N(x)={x1,x2,..., xK}을 만든다. . | 후보 집합의 각 원소가 어떤 클래스에 속하는지 그 라벨값 c(x1), c(x2),...,c(xK)을 찾는다. . | 찾아진 라벨 값 중 가장 많은 빈도수를 차지하는 클래스를 찾아 x를 그 클래스에 할당한다. . | &#44256;&#47140;&#49324;&#54637; . K&#51032; &#44050; . K가 작다면, 몇개의 이웃한 데이터에만 의존하여 클래스가 결정됨 | 이는 노이즈에 민감, 오버피팅에 발생 | 데이터 특정에 따라 적절한 K를 선택해야함 | . &#44144;&#47532;&#54632;&#49688;&#47484; &#50612;&#46523;&#44172; &#49444;&#51221;&#54624; &#44163;&#51064;&#44032;? . 다양한 거리 함수가 존재함 | 거리함수에 따라, 예측 결과가 달라질 수 있음 | ex) 1차 노름, 2차 노름, 내적, cosine distance 등 .. | . &#51109;&#51216; . 복잡한 데이터에 대해서도 비교적 잘 작동함 | 학습에 시간이 걸리지 않음 | . &#45800;&#51216; . 새 데이터가 주어질 때, 모든 학습 데이터와의 거리를 구해주어야함 (계산 비용이 많이 듦) | 학습 데이터를 모두 저장하고 있어야함 (메모리 문제) | . 2. Example . from sklearn.datasets import load_iris import pandas as pd from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns . &#45936;&#51060;&#53552;&#49483; &#51456;&#48708; . iris = load_iris() . df = pd.DataFrame(iris[&#39;data&#39;], columns=iris[&#39;feature_names&#39;]) df[&#39;target&#39;] = iris[&#39;target&#39;] df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, :4], df[&#39;target&#39;], stratify=df[&#39;target&#39;], test_size=0.2, random_state=30) x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . ((120, 4), (120,), (30, 4), (30,)) . &#54617;&#49845; . from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) knn.fit(x_train, y_train) . KNeighborsClassifier(n_jobs=-1) . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#50696;&#52769; . prediction = knn.predict(x_valid) (prediction == y_valid).mean() knn.score(x_valid, y_valid) . 0.9333333333333333 . &#52572;&#51201;&#51032; K&#44050; &#52286;&#44592; . test_scores = [] train_scores = [] for i in range(1,10): knn = KNeighborsClassifier(i) knn.fit(x_train,y_train) train_scores.append(knn.score(x_train,y_train)) test_scores.append(knn.score(x_valid,y_valid)) . max_train_score = max(train_scores) train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score] print(&#39;Max train score {} % and k = {}&#39;.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind)))) . Max train score 100.0 % and k = [1] . max_test_score = max(test_scores) test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score] print(&#39;Max test score {} % and k = {}&#39;.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind)))) . Max test score 96.66666666666667 % and k = [1, 3, 4, 7, 8, 9] . plt.figure(figsize=(12,5)) p = sns.lineplot(range(1,10),train_scores,marker=&#39;*&#39;,label=&#39;Train Score&#39;) p = sns.lineplot(range(1,10),test_scores,marker=&#39;o&#39;,label=&#39;Test Score&#39;) . /Users/jwon/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . knn = KNeighborsClassifier(7) knn.fit(x_train,y_train) knn.score(x_valid,y_valid) . 0.9666666666666667 .",
            "url": "https://knu-ai-researcher.github.io/reports/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "relUrl": "/classification/2022/03/22/ml-classification-k-nearest-neighbor.html",
            "date": " • Mar 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://knu-ai-researcher.github.io/reports/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://knu-ai-researcher.github.io/reports/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}