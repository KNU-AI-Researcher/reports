---
keywords: fastai
description: KNU AIR week4
title: "[MachineLearning] Ensemble Learning - Overview"
toc: false
badges: false
comments: false
categories: [ensemble learning]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ml-ensemble-overview.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ml-ensemble-overview.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Content creators:</strong> 이주형, 이중원</p>
<p><strong>Content reviewers:</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Overview">1. Overview<a class="anchor-link" href="#1.-Overview"> </a></h1><h3 id="No-Free-Lunch-Theorem">No Free Lunch Theorem<a class="anchor-link" href="#No-Free-Lunch-Theorem"> </a></h3><ul>
<li>There is no classification method to be superior or inferior overall.</li>
<li>Obtaining good generalization performance, there is no context-independent or usage-independent reasons to favor one algorithm over others.</li>
<li>If one algorithm seems to outperform another in a particular situation, it is a consequence of its fit to a particular pattern recognition problem.</li>
<li>In practice, experience with a broad range of techniques is the best insurance for solving arbitrary new classification problems.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bias-and-Variance">Bias and Variance<a class="anchor-link" href="#Bias-and-Variance"> </a></h3><ul>
<li><strong>Bias</strong> : the amount by which the average estimator differs from the truth<ul>
<li>Low Bias : on average, we will accurately estimate the function from the dataset</li>
<li>High Bias : implies a poor match</li>
</ul>
</li>
<li><strong>Variance</strong> : spread of the individual estimations around their mean<ul>
<li>Low Variance : estimated function does not change much with different datasets</li>
<li>High Variance : implies a weak match</li>
</ul>
</li>
<li>Bias and variance are <em>not independent</em> of each other</li>
</ul>
<h3 id="Model-Complexity">Model Complexity<a class="anchor-link" href="#Model-Complexity"> </a></h3><ul>
<li>Lower model complexity : high bias &amp; low variance -&gt; <strong>Boosting</strong><ul>
<li>Logistic regression, LDA, k-NN with large k</li>
</ul>
</li>
<li>Higher model complexity : low bias &amp; high variance -&gt; <strong>Bagging</strong><ul>
<li>DT, ANN, SVM, k-NN with small k</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Purpose-of-Ensemble">Purpose of Ensemble<a class="anchor-link" href="#Purpose-of-Ensemble"> </a></h3><ul>
<li>Goal : Reduce the error through constructing multiple learners to reduce bias and variance</li>
<li>To construct good ensemble systems, each base classifier component should achieve <strong>sufficient degree of diversity</strong>, and properly combine the outputs of individual classifiers.</li>
</ul>
<p><a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">Reference</a></p>

</div>
</div>
</div>
</div>
 

